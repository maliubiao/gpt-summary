Response: The user wants a summary of the C++ code provided. This code seems to be part of an assembler for the RISC-V architecture within the V8 JavaScript engine.

Here's a breakdown of the code and its purpose:

1. **`emit` functions:** These functions are responsible for writing raw instruction bytes into the assembler's internal buffer. They take an instruction as input (`Instr` or `ShortInstr`) and append its binary representation to the buffer. The `DEBUG_PRINTF` line suggests logging for debugging purposes, showing the memory address and offset where the instruction is being placed. `CheckBuffer` likely handles buffer growth if necessary, and `disassembleInstr` might be for verification or debugging, showing the disassembled form of the emitted instruction. `CheckTrampolinePoolQuick` likely manages a pool of code used for long jumps.

2. **`instr_at_put` functions:** These functions allow writing instructions at a specific position within the buffer, either by providing an offset (`pos`) or a memory address (`pc`). They also handle writing to a `WritableJitAllocation`, which is a way to manage memory allocated for just-in-time (JIT) compiled code.

3. **`ConstantPool` class:** This class manages a pool of constant values that the generated code can access.
    *   **`EmitPrologue`:** Emits code at the beginning of the constant pool. The code seems to load zero into a register, likely as a marker or size indicator.
    *   **`PrologueSize`:**  Calculates the size of the prologue.
    *   **`SetLoadOffsetToConstPoolEntry`:**  Patches an instruction to load a constant from the pool. It modifies the `auipc` and load instructions to calculate the correct address of the constant pool entry.
    *   **`Check`:**  Determines if the constant pool needs to be emitted into the code stream. It considers factors like whether emission is forced, if the pool is close to its capacity, and whether emitting it would disrupt short instruction sequences.
    *   The constants `kMaxDistToPool`, `kCheckInterval`, etc., define parameters for how often and under what conditions the constant pool is checked and emitted.

4. **LLVM Code Snippet (within `#if defined(V8_TARGET_ARCH_RISCV64)`):** This section contains code related to efficiently materializing (generating instructions to represent) integer constants in RISC-V assembly. The functions `RecursiveLi`, `RecursiveLiCount`, and `RecursiveLiImpl` deal with breaking down large integer constants into sequences of smaller instructions (like `lui`, `addi`, `slli`, `addiw`) that can be executed on the RISC-V processor. The goal is to minimize the number of instructions needed to load a constant into a register.

5. **`DefaultTmpList` and `DefaultFPTmpList`:** These functions define lists of registers that the assembler can use as temporary registers.

**Relationship to JavaScript:**

This code is crucial for the V8 JavaScript engine's JIT compilation process. When JavaScript code is executed, V8 compiles frequently used parts of it into native machine code for better performance. This `Assembler` class is the tool used to generate that RISC-V machine code.

*   **`emit` and `instr_at_put`:** These functions are used to write the actual RISC-V instructions that implement the logic of the JavaScript code. For example, an addition in JavaScript might be translated into an `add` instruction using the `emit` function.
*   **`ConstantPool`:** When JavaScript code uses constant values (like strings, numbers, or object literals), these constants are often stored in the constant pool. The assembler generates instructions that load these constants from the pool into registers when they are needed.
*   **`RecursiveLi` (LLVM code):**  If JavaScript code uses large integer values, this code helps to generate the most efficient sequence of RISC-V instructions to load those values into registers.

**JavaScript Examples:**

1. **Direct Instruction Emission:** While not directly exposed to JavaScript, the `emit` functions are the fundamental building blocks of the JIT compiler. When compiling a JavaScript addition, V8 would use `emit` to generate the corresponding RISC-V `add` instruction.

    ```javascript
    // Internally, V8 might do something conceptually like this when compiling:
    // Assuming 'a' and 'b' are in registers r1 and r2, and we want to store the result in r3
    // assembler.emit(RISCVInstructions.add(r3, r1, r2));
    ```

2. **Constant Pool Usage:**

    ```javascript
    function greet(name) {
      return "Hello, " + name + "!";
    }
    ```

    The string `"Hello, "` and `"!"` would likely be stored in the constant pool. The generated RISC-V code would use instructions to load these strings from the constant pool when the `greet` function is executed.

    ```assembly
    // Hypothetical RISC-V assembly generated by V8
    // ...
    auipc t0, %hi(constant_pool)  // Load the high part of the constant pool address
    addi t0, t0, %lo(constant_pool) // Load the low part
    addi a0, t0, offset_of_hello_string // Load the address of "Hello, " into register a0
    // ... more instructions to handle the 'name' variable and the "!" string
    ```

3. **Materializing Large Integers:**

    ```javascript
    const veryLargeNumber = 9007199254740991; // Max safe integer in JavaScript
    console.log(veryLargeNumber);
    ```

    The `RecursiveLi` functions would be used to generate the sequence of RISC-V instructions needed to load this large number into a register.

    ```assembly
    // Hypothetical RISC-V assembly for loading the large number
    lui  t0, 0x2190  // Load upper bits
    addiw t0, t0, -0x7ff  // Add lower bits (32-bit part)
    slli t0, t0, 32
    addi t0, t0, -1     // Add the lower 32-bit part
    // ... t0 now holds the value of veryLargeNumber
    ```

**In summary, this C++ file defines the low-level machinery for generating RISC-V machine code within the V8 JavaScript engine. It provides functions to emit instructions, manage a constant pool, and efficiently load integer constants, all of which are essential for the JIT compilation process that makes JavaScript execution fast.**
This C++ code snippet is part of the RISC-V assembler implementation within the V8 JavaScript engine. It focuses on:

1. **Emitting Instructions and Raw Data:** It provides functions like `emit(Instr x)`, `emit(ShortInstr x)`, and `emit(uint64_t data)` to write machine code instructions and raw data directly into the assembler's internal buffer. These functions also include debugging output and checks for buffer overflow and the trampoline pool.

2. **Modifying Existing Instructions:** The `instr_at_put` functions allow modification of instructions that have already been emitted at a specific position (either by offset or memory address). This is useful for patching instructions during code generation.

3. **Managing the Constant Pool:** The `ConstantPool` class is responsible for managing a pool of constant values that the generated code needs to access.
    *   **`EmitPrologue`:**  Emits the initial code for the constant pool, which includes a marker and potentially alignment padding.
    *   **`PrologueSize`:** Calculates the size of the constant pool prologue.
    *   **`SetLoadOffsetToConstPoolEntry`:**  Patches instructions (specifically `auipc` and `ld/lw`) to correctly load data from a specific entry in the constant pool. It calculates the relative offset and updates the instruction operands.
    *   **`Check`:**  Determines whether the constant pool needs to be emitted into the generated code. It checks if the pool is non-empty and if it's time to emit based on distance and other factors.

4. **Efficiently Materializing Integer Constants (RISC-V64 Specific):** The code within the `#if defined(V8_TARGET_ARCH_RISCV64)` block implements strategies to generate efficient RISC-V instructions for loading potentially large 64-bit immediate values into registers. It uses techniques like decomposing the value into smaller parts and using instructions like `lui`, `addi`, `slli`, and `addiw` to construct the full value. The `RecursiveLi` and related functions handle this process recursively. The `GeneralLiCount` functions estimate the number of instructions required for loading a given immediate.

5. **Providing Default Temporary Registers:** The `DefaultTmpList` and `DefaultFPTmpList` functions specify which registers can be used as temporary registers during code generation.

**Relationship to JavaScript and Examples:**

This code plays a crucial role in V8's just-in-time (JIT) compilation of JavaScript code. When V8 compiles JavaScript functions to machine code, it uses this assembler to generate the actual RISC-V instructions.

Here's how it relates and some JavaScript examples:

1. **Emitting Instructions for JavaScript Operations:**

    ```javascript
    function add(a, b) {
      return a + b;
    }
    ```

    When V8 compiles this `add` function, the `emit` functions would be used to generate RISC-V instructions that perform the addition. For example, assuming `a` and `b` are in registers `r10` and `r11`, the assembler might emit an `add` instruction:

    ```c++
    // Hypothetical internal V8 code
    __ riscv_assembler()->emit(riscv::ADD, r12, r10, r11); // Store result in r12
    ```

2. **Using the Constant Pool for Literals:**

    ```javascript
    function greet(name) {
      return "Hello, " + name;
    }
    ```

    The string literal `"Hello, "` would likely be stored in the constant pool. The generated RISC-V code would use instructions to load the address of this string from the constant pool.

    ```c++
    // Hypothetical internal V8 code
    Label hello_string_label;
    // ... later when emitting the constant pool ...
    __ riscv_assembler()->place_label(&hello_string_label);
    __ riscv_assembler()->emit_const_string("Hello, ");

    // ... when generating code for the greet function ...
    Register string_ptr = a0; // Example register
    __ riscv_assembler()->LoadAddress(&hello_string_label, string_ptr);
    ```

    The `ConstantPool`'s `SetLoadOffsetToConstPoolEntry` would be used to patch the `LoadAddress` instruction to point to the correct location of the string in the pool.

3. **Materializing Large Numbers:**

    ```javascript
    const bigNumber = 123456789012345;
    console.log(bigNumber);
    ```

    For RISC-V64, the `RecursiveLi` functions would be used to generate the sequence of instructions necessary to load this large number into a register.

    ```c++
    // Hypothetical internal V8 code (RISC-V64)
    Register dest_reg = t0;
    __ riscv_assembler()->RecursiveLi(dest_reg, 123456789012345);
    ```

    This would result in a series of `lui`, `addi`, and `slli` instructions being emitted.

**In essence, this code provides the low-level building blocks that V8 uses to translate JavaScript code into executable RISC-V machine code, optimizing for efficiency in instruction encoding and constant handling.** The constant pool helps avoid embedding large constant values directly within the instruction stream, and the immediate materialization logic ensures that loading constants is done with the fewest possible instructions.

Prompt: 
```
这是目录为v8/src/codegen/riscv/assembler-riscv.cc的一个c++源代码文件， 请归纳一下它的功能, 如果它与javascript的功能有关系，请用javascript举例说明
这是第2部分，共2部分，请归纳一下它的功能

"""
) {
  if (!is_buffer_growth_blocked()) {
    CheckBuffer();
  }
  DEBUG_PRINTF("%p(%d): ", pc_, pc_offset());
  EmitHelper(x);
  disassembleInstr(pc_ - sizeof(x));
  CheckTrampolinePoolQuick();
}

void Assembler::emit(ShortInstr x) {
  if (!is_buffer_growth_blocked()) {
    CheckBuffer();
  }
  DEBUG_PRINTF("%p(%d): ", pc_, pc_offset());
  EmitHelper(x);
  disassembleInstr(pc_ - sizeof(x));
  CheckTrampolinePoolQuick();
}

void Assembler::emit(uint64_t data) {
  DEBUG_PRINTF("%p(%d): ", pc_, pc_offset());
  if (!is_buffer_growth_blocked()) CheckBuffer();
  EmitHelper(data);
}

void Assembler::instr_at_put(int pos, Instr instr,
                             WritableJitAllocation* jit_allocation) {
  if (jit_allocation) {
    jit_allocation->WriteUnalignedValue(
        reinterpret_cast<Address>(buffer_start_ + pos), instr);
  } else {
    *reinterpret_cast<Instr*>(buffer_start_ + pos) = instr;
  }
}

void Assembler::instr_at_put(int pos, ShortInstr instr,
                             WritableJitAllocation* jit_allocation) {
  if (jit_allocation) {
    jit_allocation->WriteUnalignedValue(
        reinterpret_cast<Address>(buffer_start_ + pos), instr);
  } else {
    *reinterpret_cast<ShortInstr*>(buffer_start_ + pos) = instr;
  }
}

void Assembler::instr_at_put(Address pc, Instr instr,
                             WritableJitAllocation* jit_allocation) {
  if (jit_allocation) {
    jit_allocation->WriteUnalignedValue(pc, instr);
  } else {
    *reinterpret_cast<Instr*>(pc) = instr;
  }
}

// Constant Pool

void ConstantPool::EmitPrologue(Alignment require_alignment) {
  // Recorded constant pool size is expressed in number of 32-bits words,
  // and includes prologue and alignment, but not the jump around the pool
  // and the size of the marker itself.
  const int marker_size = 1;
  int word_count =
      ComputeSize(Jump::kOmitted, require_alignment) / kInt32Size - marker_size;
#if V8_TARGET_ARCH_RISCV64
  assm_->ld(zero_reg, zero_reg, word_count);
#elif V8_TARGET_ARCH_RISCV32
  assm_->lw(zero_reg, zero_reg, word_count);
#endif
  assm_->EmitPoolGuard();
}

int ConstantPool::PrologueSize(Jump require_jump) const {
  // Prologue is:
  //   j over  ;; if require_jump
  //   ld x0, x0, #pool_size
  //   j 0x0
  int prologue_size = require_jump == Jump::kRequired ? kInstrSize : 0;
  prologue_size += 2 * kInstrSize;
  return prologue_size;
}

void ConstantPool::SetLoadOffsetToConstPoolEntry(int load_offset,
                                                 Instruction* entry_offset,
                                                 const ConstantPoolKey& key) {
  Instr instr_auipc = assm_->instr_at(load_offset);
  Instr instr_load = assm_->instr_at(load_offset + 4);
  // Instruction to patch must be 'ld/lw rd, offset(rd)' with 'offset == 0'.
  DCHECK(assm_->IsAuipc(instr_auipc));
#if V8_TARGET_ARCH_RISCV64
  DCHECK(assm_->IsLd(instr_load));
#elif V8_TARGET_ARCH_RISCV32
  DCHECK(assm_->IsLw(instr_load));
#endif
  DCHECK_EQ(assm_->LoadOffset(instr_load), 1);
  DCHECK_EQ(assm_->AuipcOffset(instr_auipc), 0);
  int32_t distance = static_cast<int32_t>(
      reinterpret_cast<Address>(entry_offset) -
      reinterpret_cast<Address>(assm_->toAddress(load_offset)));
  CHECK(is_int32(distance + 0x800));
  int32_t Hi20 = (((int32_t)distance + 0x800) >> 12);
  int32_t Lo12 = (int32_t)distance << 20 >> 20;
  assm_->instr_at_put(load_offset, SetAuipcOffset(Hi20, instr_auipc));
  assm_->instr_at_put(load_offset + 4, SetLoadOffset(Lo12, instr_load));
}

void ConstantPool::Check(Emission force_emit, Jump require_jump,
                         size_t margin) {
  // Some short sequence of instruction must not be broken up by constant pool
  // emission, such sequences are protected by a ConstPool::BlockScope.
  if (IsBlocked()) {
    // Something is wrong if emission is forced and blocked at the same time.
    DCHECK_EQ(force_emit, Emission::kIfNeeded);
    return;
  }

  // We emit a constant pool only if :
  //  * it is not empty
  //  * emission is forced by parameter force_emit (e.g. at function end).
  //  * emission is mandatory or opportune according to {ShouldEmitNow}.
  if (!IsEmpty() && (force_emit == Emission::kForced ||
                     ShouldEmitNow(require_jump, margin))) {
    // Emit veneers for branches that would go out of range during emission of
    // the constant pool.
    int worst_case_size = ComputeSize(Jump::kRequired, Alignment::kRequired);

    // Check that the code buffer is large enough before emitting the constant
    // pool (this includes the gap to the relocation information).
    int needed_space = worst_case_size + assm_->kGap;
    while (assm_->buffer_space() <= needed_space) {
      assm_->GrowBuffer();
    }

    EmitAndClear(require_jump);
  }
  // Since a constant pool is (now) empty, move the check offset forward by
  // the standard interval.
  SetNextCheckIn(ConstantPool::kCheckInterval);
}

// Pool entries are accessed with pc relative load therefore this cannot be more
// than 1 * MB. Since constant pool emission checks are interval based, and we
// want to keep entries close to the code, we try to emit every 64KB.
const size_t ConstantPool::kMaxDistToPool32 = 1 * MB;
const size_t ConstantPool::kMaxDistToPool64 = 1 * MB;
const size_t ConstantPool::kCheckInterval = 128 * kInstrSize;
const size_t ConstantPool::kApproxDistToPool32 = 64 * KB;
const size_t ConstantPool::kApproxDistToPool64 = kApproxDistToPool32;

const size_t ConstantPool::kOpportunityDistToPool32 = 64 * KB;
const size_t ConstantPool::kOpportunityDistToPool64 = 64 * KB;
const size_t ConstantPool::kApproxMaxEntryCount = 512;

#if defined(V8_TARGET_ARCH_RISCV64)
// LLVM Code
//===- RISCVMatInt.cpp - Immediate materialisation -------------*- C++
//-*--===//
//
//  Part of the LLVM Project, under the Apache License v2.0 with LLVM
//  Exceptions. See https://llvm.org/LICENSE.txt for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
void Assembler::RecursiveLi(Register rd, int64_t val) {
  if (val > 0 && RecursiveLiImplCount(val) > 2) {
    unsigned LeadingZeros = base::bits::CountLeadingZeros((uint64_t)val);
    uint64_t ShiftedVal = (uint64_t)val << LeadingZeros;
    int countFillZero = RecursiveLiImplCount(ShiftedVal) + 1;
    if (countFillZero < RecursiveLiImplCount(val)) {
      RecursiveLiImpl(rd, ShiftedVal);
      srli(rd, rd, LeadingZeros);
      return;
    }
  }
  RecursiveLiImpl(rd, val);
}

int Assembler::RecursiveLiCount(int64_t val) {
  if (val > 0 && RecursiveLiImplCount(val) > 2) {
    unsigned LeadingZeros = base::bits::CountLeadingZeros((uint64_t)val);
    uint64_t ShiftedVal = (uint64_t)val << LeadingZeros;
    // Fill in the bits that will be shifted out with 1s. An example where
    // this helps is trailing one masks with 32 or more ones. This will
    // generate ADDI -1 and an SRLI.
    int countFillZero = RecursiveLiImplCount(ShiftedVal) + 1;
    if (countFillZero < RecursiveLiImplCount(val)) {
      return countFillZero;
    }
  }
  return RecursiveLiImplCount(val);
}

void Assembler::RecursiveLiImpl(Register rd, int64_t Val) {
  if (is_int32(Val)) {
    // Depending on the active bits in the immediate Value v, the following
    // instruction sequences are emitted:
    //
    // v == 0                        : ADDI
    // v[0,12) != 0 && v[12,32) == 0 : ADDI
    // v[0,12) == 0 && v[12,32) != 0 : LUI
    // v[0,32) != 0                  : LUI+ADDI(W)
    int64_t Hi20 = ((Val + 0x800) >> 12) & 0xFFFFF;
    int64_t Lo12 = Val << 52 >> 52;

    if (Hi20) {
      lui(rd, (int32_t)Hi20);
    }

    if (Lo12 || Hi20 == 0) {
      if (Hi20) {
        addiw(rd, rd, Lo12);
      } else {
        addi(rd, zero_reg, Lo12);
      }
    }
    return;
  }

  // In the worst case, for a full 64-bit constant, a sequence of 8
  // instructions (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be
  // emitted. Note that the first two instructions (LUI+ADDIW) can contribute
  // up to 32 bits while the following ADDI instructions contribute up to 12
  // bits each.
  //
  // On the first glance, implementing this seems to be possible by simply
  // emitting the most significant 32 bits (LUI+ADDIW) followed by as many
  // left shift (SLLI) and immediate additions (ADDI) as needed. However, due
  // to the fact that ADDI performs a sign extended addition, doing it like
  // that would only be possible when at most 11 bits of the ADDI instructions
  // are used. Using all 12 bits of the ADDI instructions, like done by GAS,
  // actually requires that the constant is processed starting with the least
  // significant bit.
  //
  // In the following, constants are processed from LSB to MSB but instruction
  // emission is performed from MSB to LSB by recursively calling
  // generateInstSeq. In each recursion, first the lowest 12 bits are removed
  // from the constant and the optimal shift amount, which can be greater than
  // 12 bits if the constant is sparse, is determined. Then, the shifted
  // remaining constant is processed recursively and gets emitted as soon as
  // it fits into 32 bits. The emission of the shifts and additions is
  // subsequently performed when the recursion returns.

  int64_t Lo12 = Val << 52 >> 52;
  int64_t Hi52 = ((uint64_t)Val + 0x800ull) >> 12;
  int ShiftAmount = 12 + base::bits::CountTrailingZeros((uint64_t)Hi52);
  Hi52 = signExtend(Hi52 >> (ShiftAmount - 12), 64 - ShiftAmount);

  // If the remaining bits don't fit in 12 bits, we might be able to reduce
  // the shift amount in order to use LUI which will zero the lower 12 bits.
  bool Unsigned = false;
  if (ShiftAmount > 12 && !is_int12(Hi52)) {
    if (is_int32((uint64_t)Hi52 << 12)) {
      // Reduce the shift amount and add zeros to the LSBs so it will match
      // LUI.
      ShiftAmount -= 12;
      Hi52 = (uint64_t)Hi52 << 12;
    }
  }
  RecursiveLi(rd, Hi52);

  if (Unsigned) {
  } else {
    slli(rd, rd, ShiftAmount);
  }
  if (Lo12) {
    addi(rd, rd, Lo12);
  }
}

int Assembler::RecursiveLiImplCount(int64_t Val) {
  int count = 0;
  if (is_int32(Val)) {
    // Depending on the active bits in the immediate Value v, the following
    // instruction sequences are emitted:
    //
    // v == 0                        : ADDI
    // v[0,12) != 0 && v[12,32) == 0 : ADDI
    // v[0,12) == 0 && v[12,32) != 0 : LUI
    // v[0,32) != 0                  : LUI+ADDI(W)
    int64_t Hi20 = ((Val + 0x800) >> 12) & 0xFFFFF;
    int64_t Lo12 = Val << 52 >> 52;

    if (Hi20) {
      // lui(rd, (int32_t)Hi20);
      count++;
    }

    if (Lo12 || Hi20 == 0) {
      //   unsigned AddiOpc = (IsRV64 && Hi20) ? RISCV::ADDIW : RISCV::ADDI;
      //   Res.push_back(RISCVMatInt::Inst(AddiOpc, Lo12));
      count++;
    }
    return count;
  }

  // In the worst case, for a full 64-bit constant, a sequence of 8
  // instructions (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be
  // emitted. Note that the first two instructions (LUI+ADDIW) can contribute
  // up to 32 bits while the following ADDI instructions contribute up to 12
  // bits each.
  //
  // On the first glance, implementing this seems to be possible by simply
  // emitting the most significant 32 bits (LUI+ADDIW) followed by as many
  // left shift (SLLI) and immediate additions (ADDI) as needed. However, due
  // to the fact that ADDI performs a sign extended addition, doing it like
  // that would only be possible when at most 11 bits of the ADDI instructions
  // are used. Using all 12 bits of the ADDI instructions, like done by GAS,
  // actually requires that the constant is processed starting with the least
  // significant bit.
  //
  // In the following, constants are processed from LSB to MSB but instruction
  // emission is performed from MSB to LSB by recursively calling
  // generateInstSeq. In each recursion, first the lowest 12 bits are removed
  // from the constant and the optimal shift amount, which can be greater than
  // 12 bits if the constant is sparse, is determined. Then, the shifted
  // remaining constant is processed recursively and gets emitted as soon as
  // it fits into 32 bits. The emission of the shifts and additions is
  // subsequently performed when the recursion returns.

  int64_t Lo12 = Val << 52 >> 52;
  int64_t Hi52 = ((uint64_t)Val + 0x800ull) >> 12;
  int ShiftAmount = 12 + base::bits::CountTrailingZeros((uint64_t)Hi52);
  Hi52 = signExtend(Hi52 >> (ShiftAmount - 12), 64 - ShiftAmount);

  // If the remaining bits don't fit in 12 bits, we might be able to reduce
  // the shift amount in order to use LUI which will zero the lower 12 bits.
  bool Unsigned = false;
  if (ShiftAmount > 12 && !is_int12(Hi52)) {
    if (is_int32((uint64_t)Hi52 << 12)) {
      // Reduce the shift amount and add zeros to the LSBs so it will match
      // LUI.
      ShiftAmount -= 12;
      Hi52 = (uint64_t)Hi52 << 12;
    }
  }

  count += RecursiveLiImplCount(Hi52);

  if (Unsigned) {
  } else {
    // slli(rd, rd, ShiftAmount);
    count++;
  }
  if (Lo12) {
    // addi(rd, rd, Lo12);
    count++;
  }
  return count;
}

int Assembler::GeneralLiCount(int64_t imm, bool is_get_temp_reg) {
  int count = 0;
  // imitate Assembler::RV_li
  if (is_int32(imm + 0x800)) {
    // 32-bit case. Maximum of 2 instructions generated
    int64_t high_20 = ((imm + 0x800) >> 12);
    int64_t low_12 = imm << 52 >> 52;
    if (high_20) {
      count++;
      if (low_12) {
        count++;
      }
    } else {
      count++;
    }
    return count;
  } else {
    // 64-bit case: divide imm into two 32-bit parts, upper and lower
    int64_t up_32 = imm >> 32;
    int64_t low_32 = imm & 0xffffffffull;
    // Check if a temporary register is available
    if (is_get_temp_reg) {
      // keep track of hardware behavior for lower part in sim_low
      int64_t sim_low = 0;
      // Build lower part
      if (low_32 != 0) {
        int64_t high_20 = ((low_32 + 0x800) >> 12);
        int64_t low_12 = low_32 & 0xfff;
        if (high_20) {
          // Adjust to 20 bits for the case of overflow
          high_20 &= 0xfffff;
          sim_low = ((high_20 << 12) << 32) >> 32;
          count++;
          if (low_12) {
            sim_low += (low_12 << 52 >> 52) | low_12;
            count++;
          }
        } else {
          sim_low = low_12;
          count++;
        }
      }
      if (sim_low & 0x100000000) {
        // Bit 31 is 1. Either an overflow or a negative 64 bit
        if (up_32 == 0) {
          // Positive number, but overflow because of the add 0x800
          count++;
          count++;
          return count;
        }
        // low_32 is a negative 64 bit after the build
        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
      }
      if (up_32 == 0) {
        return count;
      }
      int64_t high_20 = (up_32 + 0x800) >> 12;
      int64_t low_12 = up_32 & 0xfff;
      if (high_20) {
        // Adjust to 20 bits for the case of overflow
        high_20 &= 0xfffff;
        count++;
        if (low_12) {
          count++;
        }
      } else {
        count++;
      }
      // Put it at the bgining of register
      count++;
      if (low_32 != 0) {
        count++;
      }
      return count;
    }
    // No temp register. Build imm in rd.
    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
    // parts to the upper part by doing shift and add.
    // First build upper part in rd.
    int64_t high_20 = (up_32 + 0x800) >> 12;
    int64_t low_12 = up_32 & 0xfff;
    if (high_20) {
      // Adjust to 20 bits for the case of overflow
      high_20 &= 0xfffff;
      count++;
      if (low_12) {
        count++;
      }
    } else {
      count++;
    }
    // upper part already in rd. Each part to be added to rd, has maximum of
    // 11 bits, and always starts with a 1. rd is shifted by the size of the
    // part plus the number of zeros between the parts. Each part is added
    // after the left shift.
    uint32_t mask = 0x80000000;
    int32_t i;
    for (i = 0; i < 32; i++) {
      if ((low_32 & mask) == 0) {
        mask >>= 1;
        if (i == 31) {
          // rest is zero
          count++;
        }
        continue;
      }
      // The first 1 seen
      if ((i + 11) < 32) {
        // Pick 11 bits
        count++;
        count++;
        i += 10;
        mask >>= 11;
      } else {
        count++;
        count++;
        break;
      }
    }
  }
  return count;
}
#endif

RegList Assembler::DefaultTmpList() { return {t3, t5}; }
DoubleRegList Assembler::DefaultFPTmpList() { return {kScratchDoubleReg}; }
}  // namespace internal
}  // namespace v8

"""


```