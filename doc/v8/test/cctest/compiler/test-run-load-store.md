Response:
Let's break down the thought process for analyzing the C++ code and fulfilling the request.

1. **Understand the Goal:** The primary goal is to analyze the provided C++ code (`test-run-load-store.cc`) and explain its functionality, especially its relevance to load and store operations in V8's compiler. The request also has specific constraints about `.tq` files, JavaScript relationships, code logic reasoning, and common programming errors.

2. **Initial Scan and Keyword Spotting:** Quickly scan the code for keywords and patterns:
    * `#include`:  Indicates dependencies. Look for compiler-related headers like `codegen-tester.h`.
    * `namespace v8::internal::compiler`:  Confirms this is part of V8's compiler.
    * `TEST(...)`:  These are Google Test macros, indicating this file contains unit tests.
    * `LoadFromPointer`, `UnalignedLoadFromPointer`, `StoreToPointer`, `UnalignedStoreToPointer`, `Load`, `UnalignedLoad`, `Store`, `UnalignedStore`: These are the core functions being tested, related to memory access.
    * `MachineType::Int32()`, `MachineType::Float32()`, etc.:  These define the data types being loaded and stored.
    * `TestAlignment::kAligned`, `TestAlignment::kUnaligned`:  Indicates testing of both aligned and unaligned memory access.
    * `FOR_INT32_INPUTS`, `FOR_FLOAT32_INPUTS`: Macros likely used to iterate through various input values for testing.
    * `CHECK_EQ`, `CHECK_DOUBLE_EQ`:  Assertion macros for verifying test results.

3. **Identify Core Functionality:**  The presence of `Load` and `Store` related functions, combined with the `TestAlignment` enum, strongly suggests the file is focused on testing the correctness of load and store instructions generated by the V8 compiler. The tests cover both aligned and unaligned access for various data types.

4. **Address the `.tq` Question:** The request specifically asks about `.tq` files. A quick scan reveals the filename ends in `.cc`, *not* `.tq`. Therefore, it's a C++ file, not a Torque file. This needs to be explicitly stated.

5. **JavaScript Relationship:**  Consider how load/store operations in the compiler relate to JavaScript. Every time JavaScript code accesses a variable or property, the V8 engine (including its compiler) performs load and store operations in memory. Provide simple JavaScript examples demonstrating these concepts.

6. **Code Logic Reasoning (with Hypotheses):** Choose a representative test function (e.g., `RunLoadInt32Offset`). Trace its execution with a hypothetical input:
    * **Hypothesis:** `p1` is initialized to 0. The loop iterates through `offsets`. Let's take `offset = 1`.
    * **Step-by-step:** `ComputeOffset` calculates the address to load from. `RawMachineAssemblerTester` sets up a test function. `LoadFromPointer` generates the load instruction.
    * **Input:** `j` takes on values from `FOR_INT32_INPUTS`. Let's say `j = 123`.
    * **Execution:** `p1` is set to 123. `m.Call()` executes the generated load instruction, which should read the value from the calculated address (which currently holds the value of `p1`).
    * **Output:** The loaded value should be equal to `p1`, which is 123. `CHECK_EQ` verifies this. Repeat for unaligned access.

7. **Common Programming Errors:**  Think about the implications of aligned vs. unaligned access. Unaligned access can lead to:
    * **Performance penalties:**  Some architectures handle unaligned access less efficiently.
    * **Hardware exceptions (segmentation faults):** On some architectures, unaligned access is forbidden and causes crashes.
    * **Data corruption (less common but possible):** In complex scenarios.

8. **Structure the Answer:** Organize the information logically, following the structure of the request:
    * **Functionality Summary:** Start with a concise description of the file's purpose.
    * **`.tq` Check:** Address the file extension question.
    * **JavaScript Relationship:** Explain the connection with JavaScript and provide examples.
    * **Code Logic Reasoning:** Choose a representative example and walk through it.
    * **Common Programming Errors:** Explain the pitfalls of unaligned access.

9. **Refine and Elaborate:** Review the generated answer. Ensure clarity, accuracy, and completeness. Add details where necessary (e.g., explaining the role of `RawMachineAssemblerTester`). Use clear and concise language.

**Self-Correction/Refinement during the process:**

* **Initial thought:** Maybe the file tests *specific optimizations* related to load/store. **Correction:** While optimizations might be a byproduct, the primary focus seems to be on the fundamental correctness of load/store instructions themselves, both aligned and unaligned.
* **Initial thought:**  Focus heavily on the macros like `FOR_INT32_INPUTS`. **Correction:**  While important for understanding the testing methodology, the core functionality revolves around the `Load` and `Store` functions. The macros are supporting infrastructure.
* **Realization:** The `CheckEq` template and its specializations for `Tagged` types are important. This highlights that the tests also consider V8's tagged pointer representation and potential compression. Make sure to include this detail.
* **Clarity:** Ensure the explanation of unaligned access errors is clear and provides concrete examples.

By following these steps, including self-correction, we arrive at a comprehensive and accurate explanation of the provided V8 source code.
Let's break down the functionality of `v8/test/cctest/compiler/test-run-load-store.cc`.

**Core Functionality:**

This C++ source file is a **unit test suite** for the V8 JavaScript engine's **compiler**, specifically targeting the **code generation** of **load and store instructions**. It aims to verify that the compiler correctly generates machine code for reading (loading) and writing (storing) data to memory under various conditions.

Here's a breakdown of the key aspects being tested:

* **Data Types:** The tests cover loading and storing various primitive data types, including:
    * Integer types (8-bit, 16-bit, 32-bit, and potentially 64-bit) - signed and unsigned.
    * Floating-point types (32-bit `float` and 64-bit `double`).
    * Pointers (`void*`).
    * Tagged values (`Tagged<Smi>`, `Tagged<HeapObject>`, `Tagged<Object>`), which are V8's internal representation for JavaScript values.

* **Alignment:**  The tests explicitly distinguish between **aligned** and **unaligned** memory access.
    * **Aligned access:** The memory address being accessed is a multiple of the data type's size (e.g., accessing a 4-byte integer at an address divisible by 4). This is generally faster and preferred by hardware.
    * **Unaligned access:** The memory address is *not* a multiple of the data type's size. This can be slower or even cause errors on some architectures.

* **Offsets:** The tests verify loading and storing data at various offsets from a base pointer, including positive and negative offsets.

* **Immediate Indices:** Some tests use immediate (constant) values for the index when accessing memory.

* **Sign and Zero Extension:** The tests check the correct sign-extension and zero-extension behavior when loading smaller data types into larger registers (e.g., loading an 8-bit integer into a 32-bit register).

* **Truncation:** The tests verify the truncation behavior when storing larger values into smaller memory locations.

**Regarding the `.tq` extension:**

The filename `v8/test/cctest/compiler/test-run-load-store.cc` ends in `.cc`, which is the standard extension for C++ source files. Therefore, **it is a C++ source file, not a V8 Torque source file.**

If the filename ended in `.tq`, then it would indeed be a Torque file. Torque is a domain-specific language used within V8 for generating compiler intrinsics and built-in functions.

**Relationship to JavaScript and Examples:**

This C++ code directly tests the low-level mechanisms that underpin how JavaScript code interacts with memory. Whenever a JavaScript program reads or writes to a variable, an object property, or an array element, the V8 engine's compiler eventually generates load and store instructions similar to those being tested here.

**JavaScript Examples:**

```javascript
// Example 1: Loading a variable

let x = 10; // Store operation (when x is first assigned)
let y = x;  // Load operation (reading the value of x)
console.log(y);

// Example 2: Loading and storing object properties

const obj = { a: 5 }; // Store operation (setting the 'a' property)
let z = obj.a;        // Load operation (reading the 'a' property)
obj.a = 15;           // Store operation (updating the 'a' property)

// Example 3: Loading and storing array elements

const arr = [1, 2, 3]; // Store operations (initializing array elements)
let first = arr[0];   // Load operation (reading the first element)
arr[1] = 4;          // Store operation (updating the second element)
```

In each of these JavaScript examples, the V8 engine's compiler will generate corresponding load and store machine instructions to perform the underlying memory access. `test-run-load-store.cc` ensures these generated instructions are correct.

**Code Logic Reasoning (Hypothetical Input and Output):**

Let's take the `RunLoadInt32` function as an example:

```c++
void RunLoadInt32(const TestAlignment t) {
  RawMachineAssemblerTester<int32_t> m;
  int32_t p1 = 0;  // loads directly from this location.

  if (t == TestAlignment::kAligned) {
    m.Return(m.LoadFromPointer(&p1, MachineType::Int32()));
  } else if (t == TestAlignment::kUnaligned) {
    m.Return(m.UnalignedLoadFromPointer(&p1, MachineType::Int32()));
  } else {
    UNREACHABLE();
  }

  FOR_INT32_INPUTS(i) {
    p1 = i;
    CHECK_EQ(p1, m.Call());
  }
}
```

**Hypothetical Input:** `t = TestAlignment::kAligned`, and the `FOR_INT32_INPUTS` macro iterates through the integer value `i = 12345`.

**Step-by-step Logic:**

1. `TestAlignment t = kAligned`.
2. `RawMachineAssemblerTester<int32_t> m;`: Creates a test environment for generating machine code that returns an `int32_t`.
3. `int32_t p1 = 0;`: Declares an integer variable `p1` and initializes it to 0.
4. `if (t == TestAlignment::kAligned)`: This condition is true.
5. `m.Return(m.LoadFromPointer(&p1, MachineType::Int32()));`: The assembler `m` is instructed to generate code that:
   - Loads an `Int32` value from the memory address of `p1`.
   - Returns this loaded value.
6. `FOR_INT32_INPUTS(i)`: The loop starts. Assume the current input `i` is 12345.
7. `p1 = i;`: The value of `p1` is set to 12345.
8. `CHECK_EQ(p1, m.Call());`:
   - `m.Call()` executes the generated machine code. This code will load the `int32_t` value from the memory location of `p1`, which is currently 12345.
   - `CHECK_EQ(12345, 12345)`: The assertion verifies that the loaded value is equal to the original value of `p1`.

**Hypothetical Output:** The test will pass because the generated load instruction correctly retrieves the value stored in `p1`.

If `t` were `TestAlignment::kUnaligned`, the logic would be similar, but it would test the `UnalignedLoadFromPointer` instruction, which is designed for potentially misaligned memory addresses.

**Common Programming Errors Related to Load and Store:**

This test suite helps to catch errors that could arise during compiler development related to memory access. Here are some common programming errors that these tests implicitly guard against:

1. **Incorrect Address Calculation:**
   - **Example:**  Calculating an offset incorrectly, leading to reading or writing to the wrong memory location.
   - **JavaScript Consequence:**  Accessing the wrong variable or object property, leading to unexpected behavior or crashes.

2. **Incorrect Data Type Size:**
   - **Example:** Loading or storing a different number of bytes than the intended data type requires.
   - **JavaScript Consequence:** Data corruption, where variables or object properties hold incorrect values.

3. **Alignment Issues (Especially for Unaligned Access):**
   - **Example:**  Generating aligned load/store instructions for unaligned memory locations on architectures that don't support it, leading to hardware exceptions (segmentation faults).
   - **JavaScript Consequence:**  Crashes or unpredictable behavior.

4. **Endianness Issues (Less directly tested here, but related):**
   - **Example:** Incorrectly handling the byte order (little-endian vs. big-endian) when loading or storing multi-byte data types.
   - **JavaScript Consequence:**  Data corruption, where the bytes of a number are interpreted in the wrong order.

5. **Sign/Zero Extension Errors:**
   - **Example:** Failing to correctly extend the sign or zero bits when loading a smaller integer into a larger register, leading to incorrect numerical values.
   - **JavaScript Consequence:**  Incorrect calculations or comparisons.

6. **Write Barrier Issues (Relevant for Tagged Values):**
   - **Example:**  Forgetting to trigger the write barrier when storing a tagged pointer into an object. The write barrier is crucial for the garbage collector to track object references.
   - **JavaScript Consequence:** Memory leaks or crashes due to the garbage collector not being aware of object relationships. (While not explicitly shown in the basic examples, the tests involving `Tagged` types touch upon this).

**Example of a potential programming error caught by these tests:**

Imagine a compiler bug where, for unaligned 32-bit integer loads, the generated machine code only loads 2 bytes instead of 4. The `RunUnalignedLoadInt32` test would likely fail because `CHECK_EQ(p1, m.Call())` would compare the original 4-byte integer `p1` with the incorrectly loaded 2-byte value, resulting in an assertion failure.

In summary, `v8/test/cctest/compiler/test-run-load-store.cc` is a vital part of V8's testing infrastructure, ensuring the correctness and reliability of the load and store instructions generated by its compiler, which directly impacts the performance and correctness of JavaScript execution.

### 提示词
```
这是目录为v8/test/cctest/compiler/test-run-load-store.cc的一个v8源代码， 请列举一下它的功能, 
如果v8/test/cctest/compiler/test-run-load-store.cc以.tq结尾，那它是个v8 torque源代码，
如果它与javascript的功能有关系，请用javascript举例说明,
如果有代码逻辑推理，请给出假设输入与输出，
如果涉及用户常见的编程错误，请举例说明
```

### 源代码
```
// Copyright 2016 the V8 project authors. All rights reserved. Use of this
// source code is governed by a BSD-style license that can be found in the
// LICENSE file.

#include <cmath>
#include <functional>
#include <limits>

#include "src/base/bits.h"
#include "src/base/overflowing-math.h"
#include "src/base/template-utils.h"
#include "src/base/utils/random-number-generator.h"
#include "src/objects/objects-inl.h"
#include "src/objects/tagged.h"
#include "test/cctest/cctest.h"
#include "test/cctest/compiler/codegen-tester.h"
#include "test/common/value-helper.h"

namespace v8 {
namespace internal {
namespace compiler {

enum TestAlignment {
  kAligned,
  kUnaligned,
};

#if V8_TARGET_LITTLE_ENDIAN
#define LSB(addr, bytes) addr
#elif V8_TARGET_BIG_ENDIAN
#define LSB(addr, bytes) reinterpret_cast<uint8_t*>(addr + 1) - (bytes)
#else
#error "Unknown Architecture"
#endif

// This is America!
#define A_BILLION 1000000000ULL
#define A_GIG (1024ULL * 1024ULL * 1024ULL)

namespace {
uint8_t* ComputeOffset(void* real_address, int32_t offset) {
  return reinterpret_cast<uint8_t*>(reinterpret_cast<Address>(real_address) -
                                    offset);
}

void RunLoadInt32(const TestAlignment t) {
  RawMachineAssemblerTester<int32_t> m;

  int32_t p1 = 0;  // loads directly from this location.

  if (t == TestAlignment::kAligned) {
    m.Return(m.LoadFromPointer(&p1, MachineType::Int32()));
  } else if (t == TestAlignment::kUnaligned) {
    m.Return(m.UnalignedLoadFromPointer(&p1, MachineType::Int32()));
  } else {
    UNREACHABLE();
  }

  FOR_INT32_INPUTS(i) {
    p1 = i;
    CHECK_EQ(p1, m.Call());
  }
}

void RunLoadInt32Offset(TestAlignment t) {
  int32_t p1 = 0;  // loads directly from this location.

  int32_t offsets[] = {-2000000, -100, -101, 1,          3,
                       7,        120,  2000, 2000000000, 0xFF};

  for (size_t i = 0; i < arraysize(offsets); i++) {
    RawMachineAssemblerTester<int32_t> m;
    int32_t offset = offsets[i];
    uint8_t* pointer = ComputeOffset(&p1, offset);

    // generate load [#base + #index]
    if (t == TestAlignment::kAligned) {
      m.Return(m.LoadFromPointer(pointer, MachineType::Int32(), offset));
    } else if (t == TestAlignment::kUnaligned) {
      m.Return(
          m.UnalignedLoadFromPointer(pointer, MachineType::Int32(), offset));
    } else {
      UNREACHABLE();
    }

    FOR_INT32_INPUTS(j) {
      p1 = j;
      CHECK_EQ(p1, m.Call());
    }
  }
}

void RunLoadStoreFloat32Offset(TestAlignment t) {
  float p1 = 0.0f;  // loads directly from this location.
  float p2 = 0.0f;  // and stores directly into this location.

  FOR_INT32_INPUTS(i) {
    int32_t magic =
        base::AddWithWraparound(0x2342AABB, base::MulWithWraparound(i, 3));
    RawMachineAssemblerTester<int32_t> m;
    int32_t offset = i;
    uint8_t* from = ComputeOffset(&p1, offset);
    uint8_t* to = ComputeOffset(&p2, offset);
    // generate load [#base + #index]
    if (t == TestAlignment::kAligned) {
      Node* load = m.Load(MachineType::Float32(), m.PointerConstant(from),
                          m.IntPtrConstant(offset));
      m.Store(MachineRepresentation::kFloat32, m.PointerConstant(to),
              m.IntPtrConstant(offset), load, kNoWriteBarrier);
    } else if (t == TestAlignment::kUnaligned) {
      Node* load =
          m.UnalignedLoad(MachineType::Float32(), m.PointerConstant(from),
                          m.IntPtrConstant(offset));
      m.UnalignedStore(MachineRepresentation::kFloat32, m.PointerConstant(to),
                       m.IntPtrConstant(offset), load);

    } else {
      UNREACHABLE();
    }
    m.Return(m.Int32Constant(magic));

    FOR_FLOAT32_INPUTS(j) {
      p1 = j;
      p2 = j - 5;
      CHECK_EQ(magic, m.Call());
      CHECK_DOUBLE_EQ(p1, p2);
    }
  }
}

void RunLoadStoreFloat64Offset(TestAlignment t) {
  double p1 = 0;  // loads directly from this location.
  double p2 = 0;  // and stores directly into this location.

  FOR_INT32_INPUTS(i) {
    int32_t magic =
        base::AddWithWraparound(0x2342AABB, base::MulWithWraparound(i, 3));
    RawMachineAssemblerTester<int32_t> m;
    int32_t offset = i;
    uint8_t* from = ComputeOffset(&p1, offset);
    uint8_t* to = ComputeOffset(&p2, offset);
    // generate load [#base + #index]
    if (t == TestAlignment::kAligned) {
      Node* load = m.Load(MachineType::Float64(), m.PointerConstant(from),
                          m.IntPtrConstant(offset));
      m.Store(MachineRepresentation::kFloat64, m.PointerConstant(to),
              m.IntPtrConstant(offset), load, kNoWriteBarrier);
    } else if (t == TestAlignment::kUnaligned) {
      Node* load =
          m.UnalignedLoad(MachineType::Float64(), m.PointerConstant(from),
                          m.IntPtrConstant(offset));
      m.UnalignedStore(MachineRepresentation::kFloat64, m.PointerConstant(to),
                       m.IntPtrConstant(offset), load);
    } else {
      UNREACHABLE();
    }
    m.Return(m.Int32Constant(magic));

    FOR_FLOAT64_INPUTS(j) {
      p1 = j;
      p2 = j - 5;
      CHECK_EQ(magic, m.Call());
      CHECK_DOUBLE_EQ(p1, p2);
    }
  }
}
}  // namespace

TEST(RunLoadInt32) { RunLoadInt32(TestAlignment::kAligned); }

TEST(RunUnalignedLoadInt32) { RunLoadInt32(TestAlignment::kUnaligned); }

TEST(RunLoadInt32Offset) { RunLoadInt32Offset(TestAlignment::kAligned); }

TEST(RunUnalignedLoadInt32Offset) {
  RunLoadInt32Offset(TestAlignment::kUnaligned);
}

TEST(RunLoadStoreFloat32Offset) {
  RunLoadStoreFloat32Offset(TestAlignment::kAligned);
}

TEST(RunUnalignedLoadStoreFloat32Offset) {
  RunLoadStoreFloat32Offset(TestAlignment::kUnaligned);
}

TEST(RunLoadStoreFloat64Offset) {
  RunLoadStoreFloat64Offset(TestAlignment::kAligned);
}

TEST(RunUnalignedLoadStoreFloat64Offset) {
  RunLoadStoreFloat64Offset(TestAlignment::kUnaligned);
}

namespace {

// Mostly same as CHECK_EQ() but customized for compressed tagged values.
template <typename CType>
void CheckEq(CType in_value, CType out_value) {
  CHECK_EQ(in_value, out_value);
}

#ifdef V8_COMPRESS_POINTERS
// Specializations for checking the result of compressing store.
template <>
void CheckEq<Tagged<Object>>(Tagged<Object> in_value,
                             Tagged<Object> out_value) {
  // Compare only lower 32-bits of the value because tagged load/stores are
  // 32-bit operations anyway.
  CHECK_EQ(static_cast<Tagged_t>(in_value.ptr()),
           static_cast<Tagged_t>(out_value.ptr()));
}

template <>
void CheckEq<Tagged<HeapObject>>(Tagged<HeapObject> in_value,
                                 Tagged<HeapObject> out_value) {
  return CheckEq<Tagged<Object>>(in_value, out_value);
}

template <>
void CheckEq<Tagged<Smi>>(Tagged<Smi> in_value, Tagged<Smi> out_value) {
  return CheckEq<Tagged<Object>>(in_value, out_value);
}
#endif

// Initializes the buffer with some raw data respecting requested representation
// of the values.
template <typename CType>
void InitBuffer(CType* buffer, size_t length, MachineType type) {
  const size_t kBufferSize = sizeof(CType) * length;
  if (!type.IsTagged()) {
    uint8_t* raw = reinterpret_cast<uint8_t*>(buffer);
    for (size_t i = 0; i < kBufferSize; i++) {
      raw[i] = static_cast<uint8_t>((i + kBufferSize) ^ 0xAA);
    }
    return;
  }

  // Tagged field loads require values to be properly tagged because of
  // pointer decompression that may be happenning during load.
  Isolate* isolate = CcTest::InitIsolateOnce();
  Tagged<Smi>* smi_view = reinterpret_cast<Tagged<Smi>*>(&buffer[0]);
  if (type.IsTaggedSigned()) {
    for (size_t i = 0; i < length; i++) {
      smi_view[i] = Smi::FromInt(static_cast<int>(i + kBufferSize) ^ 0xABCDEF0);
    }
  } else {
    memcpy(&buffer[0], &isolate->roots_table(), kBufferSize);
    if (!type.IsTaggedPointer()) {
      // Also add some Smis if we are checking AnyTagged case.
      for (size_t i = 0; i < length / 2; i++) {
        smi_view[i] =
            Smi::FromInt(static_cast<int>(i + kBufferSize) ^ 0xABCDEF0);
      }
    }
  }
}

template <typename CType>
void RunLoadImmIndex(MachineType type, TestAlignment t) {
  const int kNumElems = 16;
  CType buffer[kNumElems];

  InitBuffer(buffer, kNumElems, type);

  // Test with various large and small offsets.
  for (int offset = -1; offset <= 200000; offset *= -5) {
    for (int i = 0; i < kNumElems; i++) {
      BufferedRawMachineAssemblerTester<CType> m;
      CType* base_pointer = reinterpret_cast<CType*>(
          ComputeOffset(&buffer[0], offset * sizeof(CType)));
#ifdef V8_COMPRESS_POINTERS
      if (type.IsTagged()) {
        // When pointer compression is enabled then we need to access only
        // the lower 32-bit of the tagged value while the buffer contains
        // full 64-bit values.
        base_pointer = reinterpret_cast<CType*>(LSB(base_pointer, kTaggedSize));
      }
#endif

      Node* base = m.PointerConstant(base_pointer);
      Node* index = m.Int32Constant((offset + i) * sizeof(buffer[0]));
      if (t == TestAlignment::kAligned) {
        m.Return(m.Load(type, base, index));
      } else if (t == TestAlignment::kUnaligned) {
        m.Return(m.UnalignedLoad(type, base, index));
      } else {
        UNREACHABLE();
      }

      CheckEq<CType>(buffer[i], m.Call());
    }
  }
}

template <typename CType>
void RunLoadStore(MachineType type, TestAlignment t) {
  const int kNumElems = 16;
  CType in_buffer[kNumElems];
  CType out_buffer[kNumElems];
  uintptr_t zap_data[] = {kZapValue, kZapValue};
  CType zap_value;

  static_assert(sizeof(CType) <= sizeof(zap_data));
  MemCopy(&zap_value, &zap_data, sizeof(CType));
  InitBuffer(in_buffer, kNumElems, type);

#ifdef V8_TARGET_BIG_ENDIAN
  int offset = sizeof(CType) - ElementSizeInBytes(type.representation());
#else
  int offset = 0;
#endif

  for (int32_t x = 0; x < kNumElems; x++) {
    int32_t y = kNumElems - x - 1;

    RawMachineAssemblerTester<int32_t> m;
    int32_t OK = 0x29000 + x;
    Node* in_base = m.PointerConstant(in_buffer);
    Node* in_index = m.IntPtrConstant(x * sizeof(CType) + offset);
    Node* out_base = m.PointerConstant(out_buffer);
    Node* out_index = m.IntPtrConstant(y * sizeof(CType) + offset);
    if (t == TestAlignment::kAligned) {
      Node* load = m.Load(type, in_base, in_index);
      m.Store(type.representation(), out_base, out_index, load,
              kNoWriteBarrier);
    } else if (t == TestAlignment::kUnaligned) {
      Node* load = m.UnalignedLoad(type, in_base, in_index);
      m.UnalignedStore(type.representation(), out_base, out_index, load);
    }

    m.Return(m.Int32Constant(OK));

    for (int32_t z = 0; z < kNumElems; z++) {
      out_buffer[z] = zap_value;
    }
    CHECK_NE(in_buffer[x], out_buffer[y]);
    CHECK_EQ(OK, m.Call());
    // Mostly same as CHECK_EQ() but customized for compressed tagged values.
    CheckEq<CType>(in_buffer[x], out_buffer[y]);
    for (int32_t z = 0; z < kNumElems; z++) {
      if (z != y) CHECK_EQ(zap_value, out_buffer[z]);
    }
  }
}

template <typename CType>
void RunUnalignedLoadStoreUnalignedAccess(MachineType type) {
  CType in, out;
  uint8_t in_buffer[2 * sizeof(CType)];
  uint8_t out_buffer[2 * sizeof(CType)];

  InitBuffer(&in, 1, type);

  for (int x = 0; x < static_cast<int>(sizeof(CType)); x++) {
    // Direct write to &in_buffer[x] may cause unaligned access in C++ code so
    // we use MemCopy() to handle that.
    MemCopy(&in_buffer[x], &in, sizeof(CType));

    for (int y = 0; y < static_cast<int>(sizeof(CType)); y++) {
      RawMachineAssemblerTester<int32_t> m;
      int32_t OK = 0x29000 + x;

      Node* in_base = m.PointerConstant(in_buffer);
      Node* in_index = m.IntPtrConstant(x);
      Node* load = m.UnalignedLoad(type, in_base, in_index);

      Node* out_base = m.PointerConstant(out_buffer);
      Node* out_index = m.IntPtrConstant(y);
      m.UnalignedStore(type.representation(), out_base, out_index, load);

      m.Return(m.Int32Constant(OK));

      CHECK_EQ(OK, m.Call());
      // Direct read of &out_buffer[y] may cause unaligned access in C++ code
      // so we use MemCopy() to handle that.
      MemCopy(&out, &out_buffer[y], sizeof(CType));
      // Mostly same as CHECK_EQ() but customized for compressed tagged values.
      CheckEq<CType>(in, out);
    }
  }
}
}  // namespace

TEST(RunLoadImmIndex) {
  RunLoadImmIndex<int8_t>(MachineType::Int8(), TestAlignment::kAligned);
  RunLoadImmIndex<uint8_t>(MachineType::Uint8(), TestAlignment::kAligned);
  RunLoadImmIndex<int16_t>(MachineType::Int16(), TestAlignment::kAligned);
  RunLoadImmIndex<uint16_t>(MachineType::Uint16(), TestAlignment::kAligned);
  RunLoadImmIndex<int32_t>(MachineType::Int32(), TestAlignment::kAligned);
  RunLoadImmIndex<uint32_t>(MachineType::Uint32(), TestAlignment::kAligned);
  RunLoadImmIndex<void*>(MachineType::Pointer(), TestAlignment::kAligned);
  RunLoadImmIndex<Tagged<Smi>>(MachineType::TaggedSigned(),
                               TestAlignment::kAligned);
  RunLoadImmIndex<Tagged<HeapObject>>(MachineType::TaggedPointer(),
                                      TestAlignment::kAligned);
  RunLoadImmIndex<Tagged<Object>>(MachineType::AnyTagged(),
                                  TestAlignment::kAligned);
  RunLoadImmIndex<float>(MachineType::Float32(), TestAlignment::kAligned);
  RunLoadImmIndex<double>(MachineType::Float64(), TestAlignment::kAligned);
#if V8_TARGET_ARCH_64_BIT
  RunLoadImmIndex<int64_t>(MachineType::Int64(), TestAlignment::kAligned);
#endif
  // TODO(titzer): test various indexing modes.
}

TEST(RunUnalignedLoadImmIndex) {
  RunLoadImmIndex<int16_t>(MachineType::Int16(), TestAlignment::kUnaligned);
  RunLoadImmIndex<uint16_t>(MachineType::Uint16(), TestAlignment::kUnaligned);
  RunLoadImmIndex<int32_t>(MachineType::Int32(), TestAlignment::kUnaligned);
  RunLoadImmIndex<uint32_t>(MachineType::Uint32(), TestAlignment::kUnaligned);
  RunLoadImmIndex<void*>(MachineType::Pointer(), TestAlignment::kUnaligned);
  RunLoadImmIndex<float>(MachineType::Float32(), TestAlignment::kUnaligned);
  RunLoadImmIndex<double>(MachineType::Float64(), TestAlignment::kUnaligned);
#if V8_TARGET_ARCH_64_BIT
  RunLoadImmIndex<int64_t>(MachineType::Int64(), TestAlignment::kUnaligned);
#endif
  // TODO(titzer): test various indexing modes.
}

TEST(RunLoadStore) {
  RunLoadStore<int8_t>(MachineType::Int8(), TestAlignment::kAligned);
  RunLoadStore<uint8_t>(MachineType::Uint8(), TestAlignment::kAligned);
  RunLoadStore<int16_t>(MachineType::Int16(), TestAlignment::kAligned);
  RunLoadStore<uint16_t>(MachineType::Uint16(), TestAlignment::kAligned);
  RunLoadStore<int32_t>(MachineType::Int32(), TestAlignment::kAligned);
  RunLoadStore<uint32_t>(MachineType::Uint32(), TestAlignment::kAligned);
  RunLoadStore<void*>(MachineType::Pointer(), TestAlignment::kAligned);
  RunLoadStore<Tagged<Smi>>(MachineType::TaggedSigned(),
                            TestAlignment::kAligned);
  RunLoadStore<Tagged<HeapObject>>(MachineType::TaggedPointer(),
                                   TestAlignment::kAligned);
  RunLoadStore<Tagged<Object>>(MachineType::AnyTagged(),
                               TestAlignment::kAligned);
  RunLoadStore<float>(MachineType::Float32(), TestAlignment::kAligned);
  RunLoadStore<double>(MachineType::Float64(), TestAlignment::kAligned);
#if V8_TARGET_ARCH_64_BIT
  RunLoadStore<int64_t>(MachineType::Int64(), TestAlignment::kAligned);
#endif
}

TEST(RunUnalignedLoadStore) {
  RunLoadStore<int16_t>(MachineType::Int16(), TestAlignment::kUnaligned);
  RunLoadStore<uint16_t>(MachineType::Uint16(), TestAlignment::kUnaligned);
  RunLoadStore<int32_t>(MachineType::Int32(), TestAlignment::kUnaligned);
  RunLoadStore<uint32_t>(MachineType::Uint32(), TestAlignment::kUnaligned);
  RunLoadStore<void*>(MachineType::Pointer(), TestAlignment::kUnaligned);
  RunLoadStore<float>(MachineType::Float32(), TestAlignment::kUnaligned);
  RunLoadStore<double>(MachineType::Float64(), TestAlignment::kUnaligned);
#if V8_TARGET_ARCH_64_BIT
  RunLoadStore<int64_t>(MachineType::Int64(), TestAlignment::kUnaligned);
#endif
}

TEST(RunUnalignedLoadStoreUnalignedAccess) {
  RunUnalignedLoadStoreUnalignedAccess<int16_t>(MachineType::Int16());
  RunUnalignedLoadStoreUnalignedAccess<uint16_t>(MachineType::Uint16());
  RunUnalignedLoadStoreUnalignedAccess<int32_t>(MachineType::Int32());
  RunUnalignedLoadStoreUnalignedAccess<uint32_t>(MachineType::Uint32());
  RunUnalignedLoadStoreUnalignedAccess<void*>(MachineType::Pointer());
  RunUnalignedLoadStoreUnalignedAccess<float>(MachineType::Float32());
  RunUnalignedLoadStoreUnalignedAccess<double>(MachineType::Float64());
#if V8_TARGET_ARCH_64_BIT
  RunUnalignedLoadStoreUnalignedAccess<int64_t>(MachineType::Int64());
#endif
}

namespace {
void RunLoadStoreSignExtend32(TestAlignment t) {
  int32_t buffer[4];
  RawMachineAssemblerTester<int32_t> m;
  Node* load8 = m.LoadFromPointer(LSB(&buffer[0], 1), MachineType::Int8());
  if (t == TestAlignment::kAligned) {
    Node* load16 = m.LoadFromPointer(LSB(&buffer[0], 2), MachineType::Int16());
    Node* load32 = m.LoadFromPointer(&buffer[0], MachineType::Int32());
    m.StoreToPointer(&buffer[1], MachineRepresentation::kWord32, load8);
    m.StoreToPointer(&buffer[2], MachineRepresentation::kWord32, load16);
    m.StoreToPointer(&buffer[3], MachineRepresentation::kWord32, load32);
  } else if (t == TestAlignment::kUnaligned) {
    Node* load16 =
        m.UnalignedLoadFromPointer(LSB(&buffer[0], 2), MachineType::Int16());
    Node* load32 = m.UnalignedLoadFromPointer(&buffer[0], MachineType::Int32());
    m.StoreToPointer(&buffer[1], MachineRepresentation::kWord32, load8);
    m.UnalignedStoreToPointer(&buffer[2], MachineRepresentation::kWord32,
                              load16);
    m.UnalignedStoreToPointer(&buffer[3], MachineRepresentation::kWord32,
                              load32);
  } else {
    UNREACHABLE();
  }
  m.Return(load8);

  FOR_INT32_INPUTS(i) {
    buffer[0] = i;

    CHECK_EQ(static_cast<int8_t>(i & 0xFF), m.Call());
    CHECK_EQ(static_cast<int8_t>(i & 0xFF), buffer[1]);
    CHECK_EQ(static_cast<int16_t>(i & 0xFFFF), buffer[2]);
    CHECK_EQ(i, buffer[3]);
  }
}

void RunLoadStoreZeroExtend32(TestAlignment t) {
  uint32_t buffer[4];
  RawMachineAssemblerTester<uint32_t> m;
  Node* load8 = m.LoadFromPointer(LSB(&buffer[0], 1), MachineType::Uint8());
  if (t == TestAlignment::kAligned) {
    Node* load16 = m.LoadFromPointer(LSB(&buffer[0], 2), MachineType::Uint16());
    Node* load32 = m.LoadFromPointer(&buffer[0], MachineType::Uint32());
    m.StoreToPointer(&buffer[1], MachineRepresentation::kWord32, load8);
    m.StoreToPointer(&buffer[2], MachineRepresentation::kWord32, load16);
    m.StoreToPointer(&buffer[3], MachineRepresentation::kWord32, load32);
  } else if (t == TestAlignment::kUnaligned) {
    Node* load16 =
        m.UnalignedLoadFromPointer(LSB(&buffer[0], 2), MachineType::Uint16());
    Node* load32 =
        m.UnalignedLoadFromPointer(&buffer[0], MachineType::Uint32());
    m.StoreToPointer(&buffer[1], MachineRepresentation::kWord32, load8);
    m.UnalignedStoreToPointer(&buffer[2], MachineRepresentation::kWord32,
                              load16);
    m.UnalignedStoreToPointer(&buffer[3], MachineRepresentation::kWord32,
                              load32);
  }
  m.Return(load8);

  FOR_UINT32_INPUTS(i) {
    buffer[0] = i;

    CHECK_EQ((i & 0xFF), m.Call());
    CHECK_EQ((i & 0xFF), buffer[1]);
    CHECK_EQ((i & 0xFFFF), buffer[2]);
    CHECK_EQ(i, buffer[3]);
  }
}
}  // namespace

TEST(RunLoadStoreSignExtend32) {
  RunLoadStoreSignExtend32(TestAlignment::kAligned);
}

TEST(RunUnalignedLoadStoreSignExtend32) {
  RunLoadStoreSignExtend32(TestAlignment::kUnaligned);
}

TEST(RunLoadStoreZeroExtend32) {
  RunLoadStoreZeroExtend32(TestAlignment::kAligned);
}

TEST(RunUnalignedLoadStoreZeroExtend32) {
  RunLoadStoreZeroExtend32(TestAlignment::kUnaligned);
}

#if V8_TARGET_ARCH_64_BIT

namespace {
void RunLoadStoreSignExtend64(TestAlignment t) {
  if ((true)) return;  // TODO(titzer): sign extension of loads to 64-bit.
  int64_t buffer[5];
  RawMachineAssemblerTester<int64_t> m;
  Node* load8 = m.LoadFromPointer(LSB(&buffer[0], 1), MachineType::Int8());
  if (t == TestAlignment::kAligned) {
    Node* load16 = m.LoadFromPointer(LSB(&buffer[0], 2), MachineType::Int16());
    Node* load32 = m.LoadFromPointer(LSB(&buffer[0], 4), MachineType::Int32());
    Node* load64 = m.LoadFromPointer(&buffer[0], MachineType::Int64());
    m.StoreToPointer(&buffer[1], MachineRepresentation::kWord64, load8);
    m.StoreToPointer(&buffer[2], MachineRepresentation::kWord64, load16);
    m.StoreToPointer(&buffer[3], MachineRepresentation::kWord64, load32);
    m.StoreToPointer(&buffer[4], MachineRepresentation::kWord64, load64);
  } else if (t == TestAlignment::kUnaligned) {
    Node* load16 =
        m.UnalignedLoadFromPointer(LSB(&buffer[0], 2), MachineType::Int16());
    Node* load32 =
        m.UnalignedLoadFromPointer(LSB(&buffer[0], 4), MachineType::Int32());
    Node* load64 = m.UnalignedLoadFromPointer(&buffer[0], MachineType::Int64());
    m.StoreToPointer(&buffer[1], MachineRepresentation::kWord64, load8);
    m.UnalignedStoreToPointer(&buffer[2], MachineRepresentation::kWord64,
                              load16);
    m.UnalignedStoreToPointer(&buffer[3], MachineRepresentation::kWord64,
                              load32);
    m.UnalignedStoreToPointer(&buffer[4], MachineRepresentation::kWord64,
                              load64);
  } else {
    UNREACHABLE();
  }
  m.Return(load8);

  FOR_INT64_INPUTS(i) {
    buffer[0] = i;

    CHECK_EQ(static_cast<int8_t>(i & 0xFF), m.Call());
    CHECK_EQ(static_cast<int8_t>(i & 0xFF), buffer[1]);
    CHECK_EQ(static_cast<int16_t>(i & 0xFFFF), buffer[2]);
    CHECK_EQ(static_cast<int32_t>(i & 0xFFFFFFFF), buffer[3]);
    CHECK_EQ(i, buffer[4]);
  }
}

void RunLoadStoreZeroExtend64(TestAlignment t) {
  if (kSystemPointerSize < 8) return;
  uint64_t buffer[5];
  RawMachineAssemblerTester<uint64_t> m;
  Node* load8 = m.LoadFromPointer(LSB(&buffer[0], 1), MachineType::Uint8());
  if (t == TestAlignment::kAligned) {
    Node* load16 = m.LoadFromPointer(LSB(&buffer[0], 2), MachineType::Uint16());
    Node* load32 = m.LoadFromPointer(LSB(&buffer[0], 4), MachineType::Uint32());
    Node* load64 = m.LoadFromPointer(&buffer[0], MachineType::Uint64());
    m.StoreToPointer(&buffer[1], MachineRepresentation::kWord64, load8);
    m.StoreToPointer(&buffer[2], MachineRepresentation::kWord64, load16);
    m.StoreToPointer(&buffer[3], MachineRepresentation::kWord64, load32);
    m.StoreToPointer(&buffer[4], MachineRepresentation::kWord64, load64);
  } else if (t == TestAlignment::kUnaligned) {
    Node* load16 =
        m.UnalignedLoadFromPointer(LSB(&buffer[0], 2), MachineType::Uint16());
    Node* load32 =
        m.UnalignedLoadFromPointer(LSB(&buffer[0], 4), MachineType::Uint32());
    Node* load64 =
        m.UnalignedLoadFromPointer(&buffer[0], MachineType::Uint64());
    m.StoreToPointer(&buffer[1], MachineRepresentation::kWord64, load8);
    m.UnalignedStoreToPointer(&buffer[2], MachineRepresentation::kWord64,
                              load16);
    m.UnalignedStoreToPointer(&buffer[3], MachineRepresentation::kWord64,
                              load32);
    m.UnalignedStoreToPointer(&buffer[4], MachineRepresentation::kWord64,
                              load64);
  } else {
    UNREACHABLE();
  }
  m.Return(load8);

  FOR_UINT64_INPUTS(i) {
    buffer[0] = i;

    CHECK_EQ((i & 0xFF), m.Call());
    CHECK_EQ((i & 0xFF), buffer[1]);
    CHECK_EQ((i & 0xFFFF), buffer[2]);
    CHECK_EQ((i & 0xFFFFFFFF), buffer[3]);
    CHECK_EQ(i, buffer[4]);
  }
}

}  // namespace

TEST(RunLoadStoreSignExtend64) {
  RunLoadStoreSignExtend64(TestAlignment::kAligned);
}

TEST(RunUnalignedLoadStoreSignExtend64) {
  RunLoadStoreSignExtend64(TestAlignment::kUnaligned);
}

TEST(RunLoadStoreZeroExtend64) {
  RunLoadStoreZeroExtend64(TestAlignment::kAligned);
}

TEST(RunUnalignedLoadStoreZeroExtend64) {
  RunLoadStoreZeroExtend64(TestAlignment::kUnaligned);
}

#endif

namespace {
template <typename IntType>
void LoadStoreTruncation(MachineType kRepresentation, TestAlignment t) {
  IntType input;

  RawMachineAssemblerTester<int32_t> m;
  Node* ap1;
  if (t == TestAlignment::kAligned) {
    Node* a = m.LoadFromPointer(&input, kRepresentation);
    ap1 = m.Int32Add(a, m.Int32Constant(1));
    m.StoreToPointer(&input, kRepresentation.representation(), ap1);
  } else if (t == TestAlignment::kUnaligned) {
    Node* a = m.UnalignedLoadFromPointer(&input, kRepresentation);
    ap1 = m.Int32Add(a, m.Int32Constant(1));
    m.UnalignedStoreToPointer(&input, kRepresentation.representation(), ap1);
  } else {
    UNREACHABLE();
  }
  m.Return(ap1);

  const IntType max = std::numeric_limits<IntType>::max();
  const IntType min = std::numeric_limits<IntType>::min();

  // Test upper bound.
  input = max;
  CHECK_EQ(max + 1, m.Call());
  CHECK_EQ(min, input);

  // Test lower bound.
  input = min;
  CHECK_EQ(static_cast<IntType>(max + 2), m.Call());
  CHECK_EQ(min + 1, input);

  // Test all one byte values that are not one byte bounds.
  for (int i = -127; i < 127; i++) {
    input = i;
    int expected = i >= 0 ? i + 1 : max + (i - min) + 2;
    CHECK_EQ(static_cast<IntType>(expected), m.Call());
    CHECK_EQ(static_cast<IntType>(i + 1), input);
  }
}
}  // namespace

TEST(RunLoadStoreTruncation) {
  LoadStoreTruncation<int8_t>(MachineType::Int8(), TestAlignment::kAligned);
  LoadStoreTruncation<int16_t>(MachineType::Int16(), TestAlignment::kAligned);
}

TEST(RunUnalignedLoadStoreTruncation) {
  LoadStoreTruncation<int16_t>(MachineType::Int16(), TestAlignment::kUnaligned);
}

#undef LSB
#undef A_BILLION
#undef A_GIG

}  // namespace compiler
}  // namespace internal
}  // namespace v8
```