Response:
The user wants a summary of the functionality of the provided C++ code snippet from `v8/src/maglev/maglev-code-generator.cc`. The response should also consider a hypothetical scenario where the file has a `.tq` extension (implying Torque usage), relate the functionality to JavaScript, provide an example of code logic inference, and discuss common programming errors.

Here's a breakdown of the code and how to address each point:

1. **Overall Functionality:** The code deals with generating machine code for the Maglev compiler in V8. It focuses on tasks like spilling registers, handling control flow (jumps, branches, switches), and managing deoptimization information. Key aspects include:
    * **Spilling:** Moving register values to the stack.
    * **Gap Moves:** Moving values between registers and memory at the boundaries of basic blocks, especially for Phi nodes.
    * **Deferred Blocks:** Optimizing code generation by handling less frequently executed code paths separately.
    * **Deoptimization:**  Generating information needed to revert to an interpreter state if optimizations are no longer valid. This involves frame translation – describing the layout of the stack and registers at the point of deoptimization.

2. **`.tq` Extension:** If the file ended in `.tq`, it would be a Torque file. Torque is a domain-specific language used in V8 for generating C++ code, often for low-level operations and built-in functions. The code provided is clearly C++, so the `.tq` condition is false in this case.

3. **Relationship to JavaScript:**  The code directly supports the execution of JavaScript. Maglev is a compiler for JavaScript. The code generated by this file handles the low-level execution of JavaScript constructs. For example, the deoptimization logic is crucial for maintaining correctness when optimizations fail, ensuring the JavaScript program continues to run as expected.

4. **Code Logic Inference:**  The `EmitBlockEndGapMoves` function is a good candidate for this. It infers the necessary moves based on Phi nodes and register state at block boundaries.
    * **Input:**  A `UnconditionalControlNode` (like a jump) and the `ProcessingState` of the current block.
    * **Output:** Machine code that moves values as needed to the target block.
    * **Assumptions:**  Phi nodes represent values that can come from different predecessors. The register allocator has assigned registers.

5. **Common Programming Errors:**  The deoptimization section provides a lot of potential for errors related to incorrect assumptions about the state of variables. For example:
    * **Incorrect Type Assumptions:**  Assuming a variable is always a number when it could be undefined.
    * **Outdated Values:**  Caching a value and not realizing it has changed.

6. **Summary of Functionality (Part 2):** This section focuses heavily on the logic for handling the end of basic blocks, specifically the "gap moves" needed to reconcile register assignments between blocks, and the initialization of the deoptimization frame building process.

**Mental Sandbox/Reasoning:**

* **Initial Read-Through:**  The code uses V8-specific classes like `MaglevAssembler`, `ValueNode`, `BasicBlock`, `Phi`, `DeoptFrame`, etc. This confirms it's related to V8's internal workings.
* **Keywords:** Terms like "Spill," "Gap moves," "Deferred," and "Deopt" are strong indicators of the code's purpose.
* **Control Flow:** The `PatchJumps` function clearly modifies the control flow graph.
* **Deoptimization Details:** The `MaglevFrameTranslationBuilder` class is central to deoptimization. It builds the information needed to reconstruct the interpreter state.
* **Conditional Logic:** The `if constexpr` and `switch` statements suggest handling different types of nodes or situations.
* **Data Structures:** The use of `SmallZoneVector`, `ParallelMoveResolver`, and `IdentityMap` hints at memory management and efficient lookups within the compilation process.

**Refinement for Output:**

* **JavaScript Examples:** Need to craft simple JavaScript code that might trigger the scenarios described in the C++ (e.g., function calls leading to inlining, operations that could deoptimize).
* **Code Logic Example:**  Provide a simplified scenario for `EmitBlockEndGapMoves` with a Phi node.
* **Programming Error Examples:** Focus on errors that are relevant to the optimizations and deoptimization mechanisms.

By following these steps, I can construct a comprehensive and accurate answer to the user's request.
好的，让我们来归纳一下这段代码的功能。

这段代码是 `v8/src/maglev/maglev-code-generator.cc` 的一部分，它主要负责以下几个核心功能：

**1. 基本块处理的辅助功能：**

* **`EmitBlockEndSpills`:**  在基本块结束时，如果某个值的节点被“溢出”（spilled），则将该值从寄存器或浮点寄存器移动到其在栈上的预留位置。这是一种优化策略，当寄存器不足时，将不常用的值暂时存储在内存中。
* **`EmitBlockEndGapMoves`:**  处理基本块之间的“间隙移动”（gap moves）。这主要发生在控制流跳转到另一个基本块时，需要将当前块的某些值传递到目标块。特别是处理 Phi 节点，它代表了在控制流汇合点处，根据不同的前驱基本块可能具有不同来源的值。这段代码负责生成必要的移动指令，将 Phi 节点的输入值移动到其目标寄存器或栈槽。

**2. 代码跳转和控制流优化：**

* **`PatchJumps`:**  对控制流指令（跳转、分支、Switch）的目标进行优化，如果目标块是一个空的、直接跳转到另一个块的块，则将跳转目标直接设置为最终的目标块，从而消除不必要的中间跳转。这被称为“跳转线程化”（jump threading）。

**3. 延迟块处理：**

* **`ComputeDeferred`:**  计算和标记哪些基本块应该被“延迟”处理。延迟块通常是那些不太可能被执行到的代码路径（例如，错误处理分支）。将这些块标记为延迟可以优化代码生成，将这些代码的生成推迟到稍后，或者在某些情况下完全避免生成。它使用一个工作队列来传播延迟属性，如果一个块被延迟，它的所有后继块（除非有未延迟的前驱）和所有前驱块（如果所有后继块都被延迟）也会被标记为延迟。

**4. 安全点插入：**

* `SafepointingNodeProcessor` 类（尽管在此片段中仅展示了 `Process` 方法）的功能是在生成的代码中插入安全点。安全点是垃圾回收器可以安全地暂停程序并检查对象引用的位置。`local_isolate_->heap()->Safepoint();`  这行代码会在处理每个节点时插入一个安全点。

**5. 延迟反优化（Deoptimization）框架构建：**

* `MaglevFrameTranslationBuilder` 类及其方法是用来构建在发生延迟反优化时所需的信息的。反优化是指当优化的代码不再有效时，程序需要回退到未优化的状态（通常是解释器）。为了实现这一点，需要记录当前执行状态的关键信息，例如寄存器和栈上的值。
    * **`BuildEagerDeopt` 和 `BuildLazyDeopt`:** 分别处理急切反优化和延迟反优化。
    * **`RecursiveBuildDeoptFrame` 和 `BuildSingleDeoptFrame`:**  递归地构建反优化帧的信息，处理不同类型的栈帧（解释器帧、内联参数帧、构造调用桩帧、内置延续帧）。
    * **`BuildDeoptStoreRegister` 和 `BuildDeoptStoreStackSlot`:**  将寄存器或栈槽中的值存储到反优化信息中。
    * **`BuildVirtualObject`:**  处理虚拟对象的构建，这通常涉及到内联分配的对象。
    * **`BuildDeoptFrameValues`:**  构建特定类型栈帧的值，包括闭包、参数、上下文和局部变量。

**如果 `v8/src/maglev/maglev-code-generator.cc` 以 `.tq` 结尾：**

如果文件以 `.tq` 结尾，那么它将是一个 **V8 Torque 源代码**。Torque 是一种用于定义 V8 内部运行时函数的领域特定语言。Torque 代码会被编译成 C++ 代码。在这种情况下，上面展示的 C++ 代码很可能是由某个 Torque 文件生成的。

**与 JavaScript 的功能关系以及 JavaScript 示例：**

这段代码直接关系到 JavaScript 的执行效率和正确性。Maglev 是 V8 的一个编译器，负责将 JavaScript 代码编译成机器码。

* **Spilling:** 当 JavaScript 函数中使用了大量的局部变量，导致寄存器不足时，Maglev 可能会将一些变量的值溢出到栈上。

```javascript
function manyVariables() {
  let a = 1;
  let b = 2;
  let c = 3;
  let d = 4;
  let e = 5;
  let f = 6;
  let g = 7;
  let h = 8;
  return a + b + c + d + e + f + g + h;
}
```

* **Gap Moves 和 Phi 节点:**  考虑 `if-else` 语句，`result` 变量可能从 `if` 或 `else` 块中获得不同的值。Phi 节点就代表了这种控制流汇合的情况。

```javascript
function conditionalValue(x) {
  let result;
  if (x > 0) {
    result = 10;
  } else {
    result = 20;
  }
  return result;
}
```

* **Deferred Blocks:**  错误处理代码块通常不太可能被执行到，因此可能被 Maglev 标记为延迟块。

```javascript
function mightThrow(x) {
  if (x < 0) {
    throw new Error("Negative input");
  }
  return x * 2;
}
```

* **Deoptimization:**  当 Maglev 做出了一些基于类型或其他假设的优化，但这些假设在运行时被违反时，就需要进行反优化。例如，Maglev 可能会假设一个变量总是整数，但实际上它变成了字符串。

```javascript
function add(a, b) {
  return a + b; // Maglev 可能优化为整数加法
}

add(5, 10);
add("hello", "world"); // 触发反优化，因为假设的类型不匹配
```

**代码逻辑推理的假设输入与输出：**

以 `EmitBlockEndGapMoves` 和 Phi 节点为例：

**假设输入：**

* 一个 `UnconditionalControlNode`，代表一个跳转到一个包含 Phi 节点的 `target` 基本块。
* `ProcessingState` 指示当前基本块的前驱 ID。
* `target` 基本块包含一个 Phi 节点，其目标寄存器是 `r1`，并且有两个输入：
    * 来自前驱块 0 的输入操作数是寄存器 `r2`。
    * 来自前驱块 1 的输入操作数是栈槽 `[fp - 8]`。
* 当前的 `ProcessingState` 的 `predecessor_id` 是 0。

**输出（生成的汇编代码片段，简化表示）：**

```assembly
  // --   Gap moves:
  // --   * r2 → r1 (n[PhiNodeId])
  mov r1, r2
```

**假设输入（如果 `predecessor_id` 是 1）：**

**输出（生成的汇编代码片段，简化表示）：**

```assembly
  // --   Gap moves:
  // --   * [fp - 8] → r1 (n[PhiNodeId])
  mov r1, [fp - 8]
```

**涉及用户常见的编程错误：**

* **类型不一致导致的 deoptimization：** 用户在编写 JavaScript 代码时，可能会无意中改变变量的类型，这可能导致 Maglev 编译器做出的类型假设失效，从而触发反优化。

```javascript
function operateOnNumber(x) {
  return x * 2;
}

let value = 10;
operateOnNumber(value); // Maglev 可能会优化 `operateOnNumber` 假设 `x` 是数字
value = "not a number";
operateOnNumber(value); // 错误：`x` 现在是字符串，触发反优化
```

* **频繁改变对象形状（hidden class）：** V8 依赖于对象的隐藏类进行优化。如果用户频繁地给对象添加或删除属性，会导致对象的隐藏类频繁变化，这会降低性能，并可能导致优化失效。

```javascript
function Point(x, y) {
  this.x = x;
  this.y = y;
}

const p1 = new Point(1, 2);
const p2 = new Point(3, 4);
p1.z = 5; // 改变了 p1 的形状，可能影响到对 `Point` 对象的优化
```

**归纳一下它的功能（第 2 部分）：**

这段代码主要关注于 **基本块处理的收尾工作** 和 **为延迟反优化做准备**。具体来说：

1. **处理基本块结束时的状态保存（Spills）**，确保被溢出的值被正确地写入内存。
2. **处理基本块之间的值传递（Gap Moves）**，特别是为了满足 Phi 节点的要求，确保控制流跳转后，目标块能够接收到正确来源的值。
3. **优化控制流**，通过消除不必要的中间跳转来提高代码效率。
4. **识别和标记延迟执行的代码块**，以便在代码生成时可以区别对待，可能进行不同的优化或者推迟生成。
5. **开始构建延迟反优化所需的信息**，这是在优化代码执行过程中保持程序正确性的关键机制。它涉及到记录程序状态，以便在必要时能够安全地回退到未优化的状态。

总的来说，这段代码是 Maglev 代码生成器中至关重要的一部分，它负责保证生成的代码在控制流跳转时的正确性，并通过延迟处理和反优化机制来提升性能和保证程序的健壮性。

### 提示词
```
这是目录为v8/src/maglev/maglev-code-generator.cc的一个v8源代码， 请列举一下它的功能, 
如果v8/src/maglev/maglev-code-generator.cc以.tq结尾，那它是个v8 torque源代码，
如果它与javascript的功能有关系，请用javascript举例说明,
如果有代码逻辑推理，请给出假设输入与输出，
如果涉及用户常见的编程错误，请举例说明
这是第2部分，共3部分，请归纳一下它的功能
```

### 源代码
```cpp
e);
    masm()->set_allow_call(false);
    masm()->set_allow_deferred_call(false);
#endif

    if (std::is_base_of<ValueNode, NodeT>::value) {
      ValueNode* value_node = node->template Cast<ValueNode>();
      if (value_node->has_valid_live_range() && value_node->is_spilled()) {
        compiler::AllocatedOperand source =
            compiler::AllocatedOperand::cast(value_node->result().operand());
        // We shouldn't spill nodes which already output to the stack.
        if (!source.IsAnyStackSlot()) {
          if (v8_flags.code_comments) __ RecordComment("--   Spill:");
          if (source.IsRegister()) {
            __ Move(masm()->GetStackSlot(value_node->spill_slot()),
                    ToRegister(source));
          } else {
            __ StoreFloat64(masm()->GetStackSlot(value_node->spill_slot()),
                            ToDoubleRegister(source));
          }
        } else {
          // Otherwise, the result source stack slot should be equal to the
          // spill slot.
          DCHECK_EQ(source.index(), value_node->spill_slot().index());
        }
      }
    }
    return ProcessResult::kContinue;
  }

  void EmitBlockEndGapMoves(UnconditionalControlNode* node,
                            const ProcessingState& state) {
    BasicBlock* target = node->target();
    if (!target->has_state()) {
      __ RecordComment("--   Target has no state, must be a fallthrough");
      return;
    }

    int predecessor_id = state.block()->predecessor_id();

    MaglevAssembler::TemporaryRegisterScope temps(masm_);
    Register scratch = temps.AcquireScratch();
    DoubleRegister double_scratch = temps.AcquireScratchDouble();

    // TODO(leszeks): Move these to fields, to allow their data structure
    // allocations to be reused. Will need some sort of state resetting.
    ParallelMoveResolver<Register, false> register_moves(masm_);
    ParallelMoveResolver<DoubleRegister, false> double_register_moves(masm_);

    // Remember what registers were assigned to by a Phi, to avoid clobbering
    // them with RegisterMoves.
    RegList registers_set_by_phis;
    DoubleRegList double_registers_set_by_phis;

    __ RecordComment("--   Gap moves:");

    if (target->has_phi()) {
      Phi::List* phis = target->phis();
      for (Phi* phi : *phis) {
        // Ignore dead phis.
        // TODO(leszeks): We should remove dead phis entirely and turn this into
        // a DCHECK.
        if (!phi->has_valid_live_range()) {
          if (v8_flags.code_comments) {
            std::stringstream ss;
            ss << "--   * "
               << phi->input(state.block()->predecessor_id()).operand() << " → "
               << target << " (n" << graph_labeller()->NodeId(phi)
               << ") [DEAD]";
            __ RecordComment(ss.str());
          }
          continue;
        }
        Input& input = phi->input(state.block()->predecessor_id());
        ValueNode* node = input.node();
        compiler::InstructionOperand source = input.operand();
        compiler::AllocatedOperand target =
            compiler::AllocatedOperand::cast(phi->result().operand());
        if (v8_flags.code_comments) {
          std::stringstream ss;
          ss << "--   * " << source << " → " << target << " (n"
             << graph_labeller()->NodeId(phi) << ")";
          __ RecordComment(ss.str());
        }
        if (phi->use_double_register()) {
          DCHECK(!phi->decompresses_tagged_result());
          double_register_moves.RecordMove(node, source, target, false);
        } else {
          register_moves.RecordMove(node, source, target,
                                    kDoesNotNeedDecompression);
        }
        if (target.IsAnyRegister()) {
          if (phi->use_double_register()) {
            double_registers_set_by_phis.set(target.GetDoubleRegister());
          } else {
            registers_set_by_phis.set(target.GetRegister());
          }
        }
      }
    }

    target->state()->register_state().ForEachGeneralRegister(
        [&](Register reg, RegisterState& state) {
          // Don't clobber registers set by a Phi.
          if (registers_set_by_phis.has(reg)) return;

          ValueNode* node;
          RegisterMerge* merge;
          if (LoadMergeState(state, &node, &merge)) {
            compiler::InstructionOperand source =
                merge->operand(predecessor_id);
            if (v8_flags.code_comments) {
              std::stringstream ss;
              ss << "--   * " << source << " → " << reg;
              __ RecordComment(ss.str());
            }
            register_moves.RecordMove(node, source, reg,
                                      kDoesNotNeedDecompression);
          }
        });

    register_moves.EmitMoves(scratch);

    __ RecordComment("--   Double gap moves:");

    target->state()->register_state().ForEachDoubleRegister(
        [&](DoubleRegister reg, RegisterState& state) {
          // Don't clobber registers set by a Phi.
          if (double_registers_set_by_phis.has(reg)) return;

          ValueNode* node;
          RegisterMerge* merge;
          if (LoadMergeState(state, &node, &merge)) {
            compiler::InstructionOperand source =
                merge->operand(predecessor_id);
            if (v8_flags.code_comments) {
              std::stringstream ss;
              ss << "--   * " << source << " → " << reg;
              __ RecordComment(ss.str());
            }
            double_register_moves.RecordMove(node, source, reg,
                                             kDoesNotNeedDecompression);
          }
        });

    double_register_moves.EmitMoves(double_scratch);
  }

  Isolate* isolate() const { return masm_->isolate(); }
  MaglevAssembler* masm() const { return masm_; }
  MaglevCodeGenState* code_gen_state() const {
    return masm()->code_gen_state();
  }
  MaglevGraphLabeller* graph_labeller() const {
    return code_gen_state()->graph_labeller();
  }

 private:
  // Jump threading: instead of jumping to an empty block A which just
  // unconditionally jumps to B, redirect the jump to B directly.
  template <typename NodeT>
  void PatchJumps(NodeT* node) {
    if constexpr (IsUnconditionalControlNode(Node::opcode_of<NodeT>)) {
      UnconditionalControlNode* control_node =
          node->template Cast<UnconditionalControlNode>();
      control_node->set_target(control_node->target()->RealJumpTarget());
    } else if constexpr (IsBranchControlNode(Node::opcode_of<NodeT>)) {
      BranchControlNode* control_node =
          node->template Cast<BranchControlNode>();
      control_node->set_if_true(control_node->if_true()->RealJumpTarget());
      control_node->set_if_false(control_node->if_false()->RealJumpTarget());
    } else if constexpr (Node::opcode_of<NodeT> == Opcode::kSwitch) {
      Switch* switch_node = node->template Cast<Switch>();
      BasicBlockRef* targets = switch_node->targets();
      for (int i = 0; i < switch_node->size(); ++i) {
        targets[i].set_block_ptr(targets[i].block_ptr()->RealJumpTarget());
      }
      if (switch_node->has_fallthrough()) {
        switch_node->set_fallthrough(
            switch_node->fallthrough()->RealJumpTarget());
      }
    }
  }

  int ComputeDeferred(Graph* graph) {
    int deferred_count = 0;
    // Propagate deferredness: If a block is deferred, defer all its successors,
    // except if a successor has another predecessor which is not deferred.

    // In addition, if all successors of a block are deferred, defer it too.

    // Work queue is a queue of blocks which are deferred, so we'll need to
    // check whether to defer their successors and predecessors.
    SmallZoneVector<BasicBlock*, 32> work_queue(zone_);
    for (auto block_it = graph->begin(); block_it != graph->end(); ++block_it) {
      BasicBlock* block = *block_it;
      if (block->is_deferred()) {
        ++deferred_count;
        work_queue.emplace_back(block);
      }
    }

    // The algorithm below is O(N * e^2) where e is the maximum number of
    // predecessors / successors. We check whether we should defer a block at
    // most e times. When doing the check, we check each predecessor / successor
    // once.
    while (!work_queue.empty()) {
      BasicBlock* block = work_queue.back();
      work_queue.pop_back();
      DCHECK(block->is_deferred());

      // Check if we should defer any successor.
      block->ForEachSuccessor([&work_queue,
                               &deferred_count](BasicBlock* successor) {
        if (successor->is_deferred()) {
          return;
        }
        bool should_defer = true;
        successor->ForEachPredecessor([&should_defer](BasicBlock* predecessor) {
          if (!predecessor->is_deferred()) {
            should_defer = false;
          }
        });
        if (should_defer) {
          ++deferred_count;
          work_queue.emplace_back(successor);
          successor->set_deferred(true);
        }
      });

      // Check if we should defer any predecessor.
      block->ForEachPredecessor([&work_queue,
                                 &deferred_count](BasicBlock* predecessor) {
        if (predecessor->is_deferred()) {
          return;
        }
        bool should_defer = true;
        predecessor->ForEachSuccessor([&should_defer](BasicBlock* successor) {
          if (!successor->is_deferred()) {
            should_defer = false;
          }
        });
        if (should_defer) {
          ++deferred_count;
          work_queue.emplace_back(predecessor);
          predecessor->set_deferred(true);
        }
      });
    }
    return deferred_count;
  }
  MaglevAssembler* const masm_;
  Zone* zone_;
};

class SafepointingNodeProcessor {
 public:
  explicit SafepointingNodeProcessor(LocalIsolate* local_isolate)
      : local_isolate_(local_isolate) {}

  void PreProcessGraph(Graph* graph) {}
  void PostProcessGraph(Graph* graph) {}
  BlockProcessResult PreProcessBasicBlock(BasicBlock* block) {
    return BlockProcessResult::kContinue;
  }
  void PostPhiProcessing() {}
  ProcessResult Process(NodeBase* node, const ProcessingState& state) {
    local_isolate_->heap()->Safepoint();
    return ProcessResult::kContinue;
  }

 private:
  LocalIsolate* local_isolate_;
};

namespace {
DeoptimizationFrameTranslation::FrameCount GetFrameCount(
    const DeoptFrame* deopt_frame) {
  int total = 0;
  int js_frame = 0;
  do {
    if (deopt_frame->IsJsFrame()) {
      js_frame++;
    }
    total++;
    deopt_frame = deopt_frame->parent();
  } while (deopt_frame);
  return {total, js_frame};
}

BytecodeOffset GetBytecodeOffset(const DeoptFrame& deopt_frame) {
  switch (deopt_frame.type()) {
    case DeoptFrame::FrameType::kInterpretedFrame:
      return deopt_frame.as_interpreted().bytecode_position();
    case DeoptFrame::FrameType::kInlinedArgumentsFrame:
      DCHECK_NOT_NULL(deopt_frame.parent());
      return GetBytecodeOffset(*deopt_frame.parent());
    case DeoptFrame::FrameType::kConstructInvokeStubFrame:
      return BytecodeOffset::None();
    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
      return Builtins::GetContinuationBytecodeOffset(
          deopt_frame.as_builtin_continuation().builtin_id());
  }
}
SourcePosition GetSourcePosition(const DeoptFrame& deopt_frame) {
  switch (deopt_frame.type()) {
    case DeoptFrame::FrameType::kInterpretedFrame:
      return deopt_frame.as_interpreted().source_position();
    case DeoptFrame::FrameType::kInlinedArgumentsFrame:
      DCHECK_NOT_NULL(deopt_frame.parent());
      return GetSourcePosition(*deopt_frame.parent());
    case DeoptFrame::FrameType::kConstructInvokeStubFrame:
      return deopt_frame.as_construct_stub().source_position();
    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
      DCHECK_NOT_NULL(deopt_frame.parent());
      return GetSourcePosition(*deopt_frame.parent());
  }
}
compiler::SharedFunctionInfoRef GetSharedFunctionInfo(
    const DeoptFrame& deopt_frame) {
  switch (deopt_frame.type()) {
    case DeoptFrame::FrameType::kInterpretedFrame:
      return deopt_frame.as_interpreted().unit().shared_function_info();
    case DeoptFrame::FrameType::kInlinedArgumentsFrame:
      return deopt_frame.as_inlined_arguments().unit().shared_function_info();
    case DeoptFrame::FrameType::kConstructInvokeStubFrame:
      return deopt_frame.as_construct_stub().unit().shared_function_info();
    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
      return GetSharedFunctionInfo(*deopt_frame.parent());
  }
}
compiler::BytecodeArrayRef GetBytecodeArray(const DeoptFrame& deopt_frame) {
  switch (deopt_frame.type()) {
    case DeoptFrame::FrameType::kInterpretedFrame:
      return deopt_frame.as_interpreted().unit().bytecode();
    case DeoptFrame::FrameType::kInlinedArgumentsFrame:
      return deopt_frame.as_inlined_arguments().unit().bytecode();
    case DeoptFrame::FrameType::kConstructInvokeStubFrame:
      return deopt_frame.as_construct_stub().unit().bytecode();
    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
      return GetBytecodeArray(*deopt_frame.parent());
  }
}
}  // namespace

class MaglevFrameTranslationBuilder {
 public:
  MaglevFrameTranslationBuilder(
      LocalIsolate* local_isolate, MaglevAssembler* masm,
      FrameTranslationBuilder* translation_array_builder,
      IdentityMap<int, base::DefaultAllocationPolicy>* protected_deopt_literals,
      IdentityMap<int, base::DefaultAllocationPolicy>* deopt_literals)
      : local_isolate_(local_isolate),
        masm_(masm),
        translation_array_builder_(translation_array_builder),
        protected_deopt_literals_(protected_deopt_literals),
        deopt_literals_(deopt_literals),
        object_ids_(10) {}

  void BuildEagerDeopt(EagerDeoptInfo* deopt_info) {
    BuildBeginDeopt(deopt_info);

    const InputLocation* current_input_location = deopt_info->input_locations();
    const VirtualObject::List& virtual_objects =
        GetVirtualObjects(deopt_info->top_frame());
    RecursiveBuildDeoptFrame(deopt_info->top_frame(), current_input_location,
                             virtual_objects);
  }

  void BuildLazyDeopt(LazyDeoptInfo* deopt_info) {
    BuildBeginDeopt(deopt_info);

    const InputLocation* current_input_location = deopt_info->input_locations();
    const VirtualObject::List& virtual_objects =
        GetVirtualObjects(deopt_info->top_frame());

    if (deopt_info->top_frame().parent()) {
      // Deopt input locations are in the order of deopt frame emission, so
      // update the pointer after emitting the parent frame.
      RecursiveBuildDeoptFrame(*deopt_info->top_frame().parent(),
                               current_input_location, virtual_objects);
    }

    const DeoptFrame& top_frame = deopt_info->top_frame();
    switch (top_frame.type()) {
      case DeoptFrame::FrameType::kInterpretedFrame:
        return BuildSingleDeoptFrame(
            top_frame.as_interpreted(), current_input_location, virtual_objects,
            deopt_info->result_location(), deopt_info->result_size());
      case DeoptFrame::FrameType::kInlinedArgumentsFrame:
        // The inlined arguments frame can never be the top frame.
        UNREACHABLE();
      case DeoptFrame::FrameType::kConstructInvokeStubFrame:
        return BuildSingleDeoptFrame(top_frame.as_construct_stub(),
                                     current_input_location, virtual_objects);
      case DeoptFrame::FrameType::kBuiltinContinuationFrame:
        return BuildSingleDeoptFrame(top_frame.as_builtin_continuation(),
                                     current_input_location, virtual_objects);
    }
  }

 private:
  constexpr int DeoptStackSlotIndexFromFPOffset(int offset) {
    return 1 - offset / kSystemPointerSize;
  }

  int DeoptStackSlotFromStackSlot(const compiler::AllocatedOperand& operand) {
    return DeoptStackSlotIndexFromFPOffset(
        masm_->GetFramePointerOffsetForStackSlot(operand));
  }

  void BuildBeginDeopt(DeoptInfo* deopt_info) {
    object_ids_.clear();
    auto [frame_count, jsframe_count] = GetFrameCount(&deopt_info->top_frame());
    deopt_info->set_translation_index(
        translation_array_builder_->BeginTranslation(
            frame_count, jsframe_count,
            deopt_info->feedback_to_update().IsValid()));
    if (deopt_info->feedback_to_update().IsValid()) {
      translation_array_builder_->AddUpdateFeedback(
          GetDeoptLiteral(*deopt_info->feedback_to_update().vector),
          deopt_info->feedback_to_update().index());
    }
  }

  void RecursiveBuildDeoptFrame(const DeoptFrame& frame,
                                const InputLocation*& current_input_location,
                                const VirtualObject::List& virtual_objects) {
    if (frame.parent()) {
      // Deopt input locations are in the order of deopt frame emission, so
      // update the pointer after emitting the parent frame.
      RecursiveBuildDeoptFrame(*frame.parent(), current_input_location,
                               virtual_objects);
    }

    switch (frame.type()) {
      case DeoptFrame::FrameType::kInterpretedFrame:
        return BuildSingleDeoptFrame(frame.as_interpreted(),
                                     current_input_location, virtual_objects);
      case DeoptFrame::FrameType::kInlinedArgumentsFrame:
        return BuildSingleDeoptFrame(frame.as_inlined_arguments(),
                                     current_input_location, virtual_objects);
      case DeoptFrame::FrameType::kConstructInvokeStubFrame:
        return BuildSingleDeoptFrame(frame.as_construct_stub(),
                                     current_input_location, virtual_objects);
      case DeoptFrame::FrameType::kBuiltinContinuationFrame:
        return BuildSingleDeoptFrame(frame.as_builtin_continuation(),
                                     current_input_location, virtual_objects);
    }
  }

  void BuildSingleDeoptFrame(const InterpretedDeoptFrame& frame,
                             const InputLocation*& current_input_location,
                             const VirtualObject::List& virtual_objects,
                             interpreter::Register result_location,
                             int result_size) {
    int return_offset = frame.ComputeReturnOffset(result_location, result_size);
    translation_array_builder_->BeginInterpretedFrame(
        frame.bytecode_position(),
        GetDeoptLiteral(GetSharedFunctionInfo(frame)),
        GetProtectedDeoptLiteral(*GetBytecodeArray(frame).object()),
        frame.unit().register_count(), return_offset, result_size);

    BuildDeoptFrameValues(frame.unit(), frame.frame_state(), frame.closure(),
                          current_input_location, virtual_objects,
                          result_location, result_size);
  }

  void BuildSingleDeoptFrame(const InterpretedDeoptFrame& frame,
                             const InputLocation*& current_input_location,
                             const VirtualObject::List& virtual_objects) {
    // Returns offset/count is used for updating an accumulator or register
    // after a lazy deopt -- this function is overloaded to allow them to be
    // passed in.
    const int return_offset = 0;
    const int return_count = 0;
    translation_array_builder_->BeginInterpretedFrame(
        frame.bytecode_position(),
        GetDeoptLiteral(GetSharedFunctionInfo(frame)),
        GetProtectedDeoptLiteral(*GetBytecodeArray(frame).object()),
        frame.unit().register_count(), return_offset, return_count);

    BuildDeoptFrameValues(frame.unit(), frame.frame_state(), frame.closure(),
                          current_input_location, virtual_objects,
                          interpreter::Register::invalid_value(), return_count);
  }

  void BuildSingleDeoptFrame(const InlinedArgumentsDeoptFrame& frame,
                             const InputLocation*& current_input_location,
                             const VirtualObject::List& virtual_objects) {
    translation_array_builder_->BeginInlinedExtraArguments(
        GetDeoptLiteral(GetSharedFunctionInfo(frame)),
        static_cast<uint32_t>(frame.arguments().size()));

    // Closure
    BuildDeoptFrameSingleValue(frame.closure(), current_input_location,
                               virtual_objects);

    // Arguments
    // TODO(victorgomes): Technically we don't need all arguments, only the
    // extra ones. But doing this at the moment, since it matches the
    // TurboFan behaviour.
    for (ValueNode* value : frame.arguments()) {
      BuildDeoptFrameSingleValue(value, current_input_location,
                                 virtual_objects);
    }
  }

  void BuildSingleDeoptFrame(const ConstructInvokeStubDeoptFrame& frame,
                             const InputLocation*& current_input_location,
                             const VirtualObject::List& virtual_objects) {
    translation_array_builder_->BeginConstructInvokeStubFrame(
        GetDeoptLiteral(GetSharedFunctionInfo(frame)));

    // Implicit receiver
    BuildDeoptFrameSingleValue(frame.receiver(), current_input_location,
                               virtual_objects);

    // Context
    BuildDeoptFrameSingleValue(frame.context(), current_input_location,
                               virtual_objects);
  }

  void BuildSingleDeoptFrame(const BuiltinContinuationDeoptFrame& frame,
                             const InputLocation*& current_input_location,
                             const VirtualObject::List& virtual_objects) {
    BytecodeOffset bailout_id =
        Builtins::GetContinuationBytecodeOffset(frame.builtin_id());
    int literal_id = GetDeoptLiteral(GetSharedFunctionInfo(frame));
    int height = frame.parameters().length();

    constexpr int kExtraFixedJSFrameParameters =
        V8_ENABLE_LEAPTIERING_BOOL ? 4 : 3;
    if (frame.is_javascript()) {
      translation_array_builder_->BeginJavaScriptBuiltinContinuationFrame(
          bailout_id, literal_id, height + kExtraFixedJSFrameParameters);
    } else {
      translation_array_builder_->BeginBuiltinContinuationFrame(
          bailout_id, literal_id, height);
    }

    // Closure
    if (frame.is_javascript()) {
      translation_array_builder_->StoreLiteral(
          GetDeoptLiteral(frame.javascript_target()));
    } else {
      translation_array_builder_->StoreOptimizedOut();
    }

    // Parameters
    for (ValueNode* value : frame.parameters()) {
      BuildDeoptFrameSingleValue(value, current_input_location,
                                 virtual_objects);
    }

    // Extra fixed JS frame parameters. These at the end since JS builtins
    // push their parameters in reverse order.
    if (frame.is_javascript()) {
      DCHECK_EQ(Builtins::CallInterfaceDescriptorFor(frame.builtin_id())
                    .GetRegisterParameterCount(),
                kExtraFixedJSFrameParameters);
      static_assert(kExtraFixedJSFrameParameters ==
                    3 + (V8_ENABLE_LEAPTIERING_BOOL ? 1 : 0));
      // kJavaScriptCallTargetRegister
      translation_array_builder_->StoreLiteral(
          GetDeoptLiteral(frame.javascript_target()));
      // kJavaScriptCallNewTargetRegister
      translation_array_builder_->StoreLiteral(
          GetDeoptLiteral(ReadOnlyRoots(local_isolate_).undefined_value()));
      // kJavaScriptCallArgCountRegister
      translation_array_builder_->StoreLiteral(GetDeoptLiteral(
          Smi::FromInt(Builtins::GetStackParameterCount(frame.builtin_id()))));
#ifdef V8_ENABLE_LEAPTIERING
      // kJavaScriptCallDispatchHandleRegister
      translation_array_builder_->StoreLiteral(
          GetDeoptLiteral(Smi::FromInt(kInvalidDispatchHandle)));
#endif
    }

    // Context
    ValueNode* value = frame.context();
    BuildDeoptFrameSingleValue(value, current_input_location, virtual_objects);
  }

  void BuildDeoptStoreRegister(const compiler::AllocatedOperand& operand,
                               ValueRepresentation repr) {
    switch (repr) {
      case ValueRepresentation::kIntPtr:
        UNREACHABLE();
      case ValueRepresentation::kTagged:
        translation_array_builder_->StoreRegister(operand.GetRegister());
        break;
      case ValueRepresentation::kInt32:
        translation_array_builder_->StoreInt32Register(operand.GetRegister());
        break;
      case ValueRepresentation::kUint32:
        translation_array_builder_->StoreUint32Register(operand.GetRegister());
        break;
      case ValueRepresentation::kFloat64:
        translation_array_builder_->StoreDoubleRegister(
            operand.GetDoubleRegister());
        break;
      case ValueRepresentation::kHoleyFloat64:
        translation_array_builder_->StoreHoleyDoubleRegister(
            operand.GetDoubleRegister());
        break;
    }
  }

  void BuildDeoptStoreStackSlot(const compiler::AllocatedOperand& operand,
                                ValueRepresentation repr) {
    int stack_slot = DeoptStackSlotFromStackSlot(operand);
    switch (repr) {
      case ValueRepresentation::kIntPtr:
        UNREACHABLE();
      case ValueRepresentation::kTagged:
        translation_array_builder_->StoreStackSlot(stack_slot);
        break;
      case ValueRepresentation::kInt32:
        translation_array_builder_->StoreInt32StackSlot(stack_slot);
        break;
      case ValueRepresentation::kUint32:
        translation_array_builder_->StoreUint32StackSlot(stack_slot);
        break;
      case ValueRepresentation::kFloat64:
        translation_array_builder_->StoreDoubleStackSlot(stack_slot);
        break;
      case ValueRepresentation::kHoleyFloat64:
        translation_array_builder_->StoreHoleyDoubleStackSlot(stack_slot);
        break;
    }
  }

  int GetDuplicatedId(intptr_t id) {
    for (int idx = 0; idx < static_cast<int>(object_ids_.size()); idx++) {
      if (object_ids_[idx] == id) {
        // Although this is not technically necessary, the translated state
        // machinery assign ids to duplicates, so we need to push something to
        // get fresh ids.
        object_ids_.push_back(id);
        return idx;
      }
    }
    object_ids_.push_back(id);
    return kNotDuplicated;
  }

  void BuildHeapNumber(Float64 number) {
    DirectHandle<Object> value =
        local_isolate_->factory()->NewHeapNumberFromBits<AllocationType::kOld>(
            number.get_bits());
    translation_array_builder_->StoreLiteral(GetDeoptLiteral(*value));
  }

  void BuildFixedDoubleArray(uint32_t length,
                             compiler::FixedDoubleArrayRef array) {
    translation_array_builder_->BeginCapturedObject(length + 2);
    translation_array_builder_->StoreLiteral(
        GetDeoptLiteral(*local_isolate_->factory()->fixed_double_array_map()));
    translation_array_builder_->StoreLiteral(
        GetDeoptLiteral(Smi::FromInt(length)));
    for (uint32_t i = 0; i < length; i++) {
      Float64 value = array.GetFromImmutableFixedDoubleArray(i);
      if (value.is_hole_nan()) {
        translation_array_builder_->StoreLiteral(
            GetDeoptLiteral(ReadOnlyRoots(local_isolate_).the_hole_value()));
      } else {
        BuildHeapNumber(value);
      }
    }
  }

  void BuildNestedValue(const ValueNode* value,
                        const InputLocation*& input_location,
                        const VirtualObject::List& virtual_objects) {
    if (IsConstantNode(value->opcode())) {
      translation_array_builder_->StoreLiteral(
          GetDeoptLiteral(*value->Reify(local_isolate_)));
      return;
    }
    // Special nodes.
    switch (value->opcode()) {
      case Opcode::kArgumentsElements:
        translation_array_builder_->ArgumentsElements(
            value->Cast<ArgumentsElements>()->type());
        // We simulate the deoptimizer deduplication machinery, which will give
        // a fresh id to the ArgumentsElements. For that, we need to push
        // something object_ids_ We push -1, since no object should have id -1.
        object_ids_.push_back(-1);
        break;
      case Opcode::kArgumentsLength:
        translation_array_builder_->ArgumentsLength();
        break;
      case Opcode::kRestLength:
        translation_array_builder_->RestLength();
        break;
      case Opcode::kVirtualObject:
        UNREACHABLE();
      default:
        BuildDeoptFrameSingleValue(value, input_location, virtual_objects);
        break;
    }
  }

  void BuildVirtualObject(const VirtualObject* object,
                          const InputLocation*& input_location,
                          const VirtualObject::List& virtual_objects) {
    if (object->type() == VirtualObject::kHeapNumber) {
      return BuildHeapNumber(object->number());
    }
    int dup_id =
        GetDuplicatedId(reinterpret_cast<intptr_t>(object->allocation()));
    if (dup_id != kNotDuplicated) {
      translation_array_builder_->DuplicateObject(dup_id);
      input_location += object->InputLocationSizeNeeded(virtual_objects);
      return;
    }
    if (object->type() == VirtualObject::kFixedDoubleArray) {
      return BuildFixedDoubleArray(object->double_elements_length(),
                                   object->double_elements());
    }
    DCHECK_EQ(object->type(), VirtualObject::kDefault);
    translation_array_builder_->BeginCapturedObject(object->slot_count() + 1);
    translation_array_builder_->StoreLiteral(
        GetDeoptLiteral(*object->map().object()));
    for (uint32_t i = 0; i < object->slot_count(); i++) {
      BuildNestedValue(object->get_by_index(i), input_location,
                       virtual_objects);
    }
  }

  void BuildDeoptFrameSingleValue(const ValueNode* value,
                                  const InputLocation*& input_location,
                                  const VirtualObject::List& virtual_objects) {
    DCHECK(!value->Is<Identity>());
    DCHECK(!value->Is<VirtualObject>());
    size_t input_locations_to_advance = 1;
    if (const InlinedAllocation* alloc = value->TryCast<InlinedAllocation>()) {
      VirtualObject* vobject = virtual_objects.FindAllocatedWith(alloc);
      CHECK_NOT_NULL(vobject);
      if (alloc->HasBeenElided()) {
        input_location++;
        BuildVirtualObject(vobject, input_location, virtual_objects);
        return;
      }
      input_locations_to_advance +=
          vobject->InputLocationSizeNeeded(virtual_objects);
    }
    if (input_location->operand().IsConstant()) {
      translation_array_builder_->StoreLiteral(
          GetDeoptLiteral(*value->Reify(local_isolate_)));
    } else {
      const compiler::AllocatedOperand& operand =
          compiler::AllocatedOperand::cast(input_location->operand());
      ValueRepresentation repr = value->properties().value_representation();
      if (operand.IsAnyRegister()) {
        BuildDeoptStoreRegister(operand, repr);
      } else {
        BuildDeoptStoreStackSlot(operand, repr);
      }
    }
    input_location += input_locations_to_advance;
  }

  void BuildDeoptFrameValues(
      const MaglevCompilationUnit& compilation_unit,
      const CompactInterpreterFrameState* checkpoint_state,
      const ValueNode* closure, const InputLocation*& input_location,
      const VirtualObject::List& virtual_objects,
      interpreter::Register result_location, int result_size) {
    // TODO(leszeks): The input locations array happens to be in the same
    // order as closure+parameters+context+locals+accumulator are accessed
    // here. We should make this clearer and guard against this invariant
    // failing.

    // Closure
    BuildDeoptFrameSingleValue(closure, input_location, virtual_objects);

    // Parameters
    {
      int i = 0;
      checkpoint_state->ForEachParameter(
          compilation_unit, [&](ValueNode* value, interpreter::Register reg) {
            DCHECK_EQ(reg.ToParameterIndex(), i);
            if (LazyDeoptInfo::InReturnValues(reg, result_location,
                                              result_size)) {
              translation_array_builder_->StoreOptimizedOut();
            } else {
              BuildDeoptFrameSingleValue(value, input_location,
                                         virtual_objects);
            }
            i++;
          });
    }

    // Context
    ValueNode* value = checkpoint_state->context(compilation_unit);
    BuildDeoptFrameSingleValue(value, input_location, virtual_objects);

    // Locals
    {
      int i = 0;
      checkpoint_state->ForEachLocal(
          compilation_unit, [&](ValueNode* value, interpreter::Register reg) {
            DCHECK_LE(i, reg.index());
            if (LazyDeoptInfo::InReturnValues(reg, result_location,
                                              result_size))
              return;
            while (i < reg.index()) {
              translation_array_builder_->StoreOptimizedOut();
              i++;
```