Response:
Let's break down the thought process for analyzing this C++ code.

1. **Understand the Goal:** The request asks for the functionality of the `model_execution_responder.cc` file within the Chromium Blink engine. It also asks for connections to JavaScript/HTML/CSS, logical inferences, common user errors, and debugging context.

2. **Initial Scan and Keyword Identification:**  First, I'll quickly read through the code, looking for important keywords and structures. These immediately jump out:
    * `#include`:  This tells me about dependencies (mojom interfaces, promises, streams, etc.).
    * `namespace blink`:  Confirms this is Blink-specific code.
    * `class Responder`, `class StreamingResponder`:  These are the core components.
    * `mojom::blink::ModelStreamingResponder`: This is a central interface, likely defined in a `.mojom` file (IDL).
    * `ScriptPromiseResolver`, `ReadableStream`: These are JavaScript integration points.
    * `AbortSignal`:  Indicates handling of cancellation.
    * `AIMetrics`:  Suggests performance tracking.
    * `OnStreaming`, `OnCompletion`, `OnError`:  Lifecycle methods for the streaming response.
    * `BindNewPipeAndPassRemote`: Points to Mojo inter-process communication.
    * `CreateModelExecutionResponder`, `CreateModelExecutionStreamingResponder`: Factory functions.

3. **Focusing on the Core Classes:**

    * **`Responder`:**  This class appears designed to handle a *non-streaming* request, ultimately resolving a JavaScript `Promise` with the complete result.
        * It receives data incrementally in `OnStreaming` but only resolves the promise in `OnCompletion`.
        * It uses a `ScriptPromiseResolver` to interact with JavaScript.
        * It handles abort signals and context destruction.
        * It records metrics.

    * **`StreamingResponder`:** This class is designed for *streaming* responses, providing data to JavaScript through a `ReadableStream`.
        * `OnStreaming` enqueues chunks of data into the stream.
        * `OnCompletion` closes the stream.
        * `OnError` signals an error on the stream.
        * It also handles abort signals (though there's a TODO about fixing the handling).
        * It records metrics.

4. **Identifying Connections to JavaScript/HTML/CSS:**

    * **JavaScript:** The use of `ScriptPromiseResolver` and `ReadableStream` directly links this code to JavaScript. These are standard JavaScript APIs. The promise is for a single result, the stream for multiple.
    * **HTML:**  While this C++ code doesn't directly manipulate HTML, the AI functionality it enables is likely triggered from JavaScript running within an HTML page. The AI might, for example, generate text that is then displayed in the HTML.
    * **CSS:**  Similar to HTML, there's no direct interaction here. However, the results of the AI (like generated text) might have their styling defined by CSS.

5. **Logical Inferences and Examples:**

    * **`Responder`:**
        * *Input:* A request to an AI model (handled elsewhere) that produces a text response in chunks.
        * *Output:* A JavaScript promise that resolves with the *entire* text response.
        * *Example:*  A user asks a question to an AI chatbot. The `Responder` collects all the text generated by the model and provides it as a single string when the model is finished.

    * **`StreamingResponder`:**
        * *Input:*  The same kind of request as above.
        * *Output:* A JavaScript `ReadableStream` that emits the text response in chunks as they are generated.
        * *Example:*  A user requests the AI to write a longer piece of text. The `StreamingResponder` allows the webpage to start displaying the text as it's being generated, improving perceived performance.

6. **Common User Errors:**  Think about how a *developer* using these APIs in JavaScript might make mistakes.

    * **Forgetting to handle errors:** The promise returned by the `Responder` can be rejected. The `ReadableStream` in the `StreamingResponder` can emit an error. Developers need to use `.catch()` on the promise and listen for 'error' events on the stream.
    * **Not handling abort signals:** If the user cancels an AI request, the `AbortSignal` will trigger. Developers should ideally stop processing or displaying results if the signal is aborted.
    * **Misunderstanding streaming vs. non-streaming:** Choosing the wrong responder type can lead to unexpected behavior or performance issues.

7. **Debugging Context (User Operations):** Trace the likely user interactions that would lead to this code being executed.

    * A user interacts with a webpage.
    * The webpage uses JavaScript to initiate a request to an AI model. This might be through a new JavaScript API.
    * The JavaScript code likely uses a `fetch`-like mechanism (or a dedicated AI API) that triggers a call to the browser's backend.
    * The browser's backend (likely in the browser process) communicates with an AI service (potentially remote).
    * The AI service sends back the response, possibly in a streaming fashion.
    * The browser's backend then uses the `CreateModelExecutionResponder` or `CreateModelExecutionStreamingResponder` to create the appropriate responder object.
    * The Mojo interface (`ModelStreamingResponder`) is used to pipe the AI's response data to these responder objects.
    * The `OnStreaming`, `OnCompletion`, and `OnError` methods of the responder are called based on the data received.
    * Finally, the `Responder` resolves the promise or the `StreamingResponder` pushes data into the `ReadableStream`, which JavaScript on the webpage can then consume.

8. **Refinement and Organization:**  Finally, organize the information clearly using headings and bullet points to make it easy to understand. Ensure that the explanations are accurate and cover all aspects of the original request. Pay attention to providing concrete examples.

This systematic approach, starting with a broad overview and then drilling down into specifics, helps in understanding complex code like this and answering the various parts of the request.
好的，让我们来分析一下 `blink/renderer/modules/ai/model_execution_responder.cc` 这个文件。

**功能概述**

这个文件定义了两个主要的 C++ 类：`Responder` 和 `StreamingResponder`。这两个类的主要功能是作为 Chromium Blink 引擎中与 AI 模型执行相关的 *响应处理者*。 它们接收来自底层 AI 模型执行服务的输出（通常通过 Mojo 接口），并将这些输出转换成 JavaScript 可用的形式。

* **`Responder`:**  这个类处理非流式的 AI 模型执行结果。它接收模型输出的各个部分，并将它们累积起来，最终当模型执行完成时，通过一个 JavaScript Promise 返回完整的字符串结果。

* **`StreamingResponder`:** 这个类处理流式的 AI 模型执行结果。它接收模型输出的各个部分，并将其放入一个 JavaScript 的 `ReadableStream` 中，允许 JavaScript 代码逐步地消费模型输出。

**与 JavaScript, HTML, CSS 的关系**

这两个类是 Blink 引擎中将底层 AI 能力暴露给 JavaScript 的关键桥梁。

* **JavaScript:**
    * **Promise (与 `Responder` 关联):**  `Responder` 类使用 `ScriptPromiseResolver` 来创建一个 JavaScript Promise 对象。当 AI 模型执行完成，`Responder` 接收到最终结果时，它会使用 `resolver_->Resolve(response_)` 来解析这个 Promise，并将模型生成的文本数据传递给 JavaScript。如果发生错误，则会使用 `resolver_->Reject(...)` 拒绝 Promise。
    * **ReadableStream (与 `StreamingResponder` 关联):** `StreamingResponder` 创建并管理一个 JavaScript 的 `ReadableStream` 对象。当 AI 模型产生输出时，`StreamingResponder` 的 `OnStreaming` 方法会将这些输出数据通过 `Controller()->Enqueue(...)` 方法添加到 `ReadableStream` 中，使得 JavaScript 代码可以异步地读取这些数据。
    * **AbortSignal:** 两个类都接受一个 `AbortSignal` 对象。这允许 JavaScript 代码在模型执行过程中取消操作。当 `AbortSignal` 被触发时，响应处理者会拒绝 Promise (对于 `Responder`) 或关闭/报错 `ReadableStream` (对于 `StreamingResponder`)。

* **HTML & CSS:** 这两个 C++ 文件本身不直接操作 HTML 或 CSS。然而，它们提供的 AI 功能最终会在网页上呈现，而 HTML 和 CSS 则负责内容的结构和样式。例如：
    * AI 模型生成的文本可能被插入到 HTML 的 `<div>` 或 `<p>` 元素中。
    * 可以使用 CSS 来设置 AI 生成内容的样式（字体、颜色、布局等）。

**举例说明**

假设我们有一个 JavaScript 函数，它调用了一个 AI 模型来生成一段文本：

```javascript
async function generateText(prompt, stream = false, signal) {
  const response = await fetch('/ai/generate', { // 假设有一个这样的后端端点
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ prompt, stream }),
    signal
  });

  if (!response.ok) {
    throw new Error(`HTTP error! status: ${response.status}`);
  }

  if (stream) {
    return response.body.getReader(); // 返回 ReadableStream 的读取器
  } else {
    return await response.text(); // 返回 Promise，解析为文本
  }
}

// 使用 Promise (对应 Responder)
generateText("请写一个关于夏天的短故事。").then(text => {
  document.getElementById('output').textContent = text;
}).catch(error => {
  console.error("生成文本失败:", error);
});

// 使用 ReadableStream (对应 StreamingResponder)
const reader = await generateText("请写一个关于旅行的诗歌。", true);
let result = '';
while (true) {
  const { done, value } = await reader.read();
  if (done) {
    break;
  }
  result += new TextDecoder().decode(value);
  document.getElementById('output').textContent = result; // 逐步更新显示
}
```

在这个例子中：

* 当 `stream` 为 `false` 时，后端（由 `model_execution_responder.cc` 处理）会使用 `Responder` 来收集模型生成的所有文本，然后将完整的文本作为 Promise 的结果返回给 JavaScript。
* 当 `stream` 为 `true` 时，后端会使用 `StreamingResponder` 创建一个 `ReadableStream`。模型生成的数据会通过这个流逐步发送到 JavaScript，JavaScript 可以边接收边处理（例如，实时显示生成的文本）。
* `signal` 参数对应于 `AbortSignal`，允许 JavaScript 取消正在进行的 AI 模型执行。

**逻辑推理与假设输入输出**

**针对 `Responder`:**

* **假设输入:**  模型执行服务逐步发送字符串片段: "你好，世界！", "这是一个", "测试。"
* **内部处理:** `Responder` 的 `OnStreaming` 方法会被多次调用，分别接收到这些片段。`response_` 变量会逐步累积这些字符串。
* **最终输入到 `OnCompletion`:**  假设 `OnCompletion` 方法被调用，且没有额外的 `context_info`。
* **输出到 JavaScript:**  Promise 将会解析为字符串 "你好，世界！这是一个测试。"

**针对 `StreamingResponder`:**

* **假设输入:** 模型执行服务逐步发送字符串片段: "第一行。\n", "第二行。\n", "结束。"
* **内部处理:** `StreamingResponder` 的 `OnStreaming` 方法会被多次调用。每次调用都会将接收到的字符串片段通过 `Controller()->Enqueue(...)` 添加到内部的 `ReadableStream` 队列中。
* **JavaScript 的读取:** JavaScript 通过 `ReadableStream` 的 `reader.read()` 方法可以逐步读取到这些片段，例如第一次读取到 "第一行。\n"，第二次读取到 "第二行。\n"，最后读取到 "结束。"，并且 `done` 变为 `true`。

**用户或编程常见的使用错误**

* **未处理 Promise 的 rejection (针对 `Responder`):**  如果 AI 模型执行出错，`Responder` 会拒绝 Promise。如果 JavaScript 代码没有使用 `.catch()` 或 `try...catch` 来处理这种情况，可能会导致未捕获的错误。
    ```javascript
    generateText("生成时会出错的 prompt").then(text => {
      // 这里可能不会执行
      console.log("生成成功:", text);
    }); // 缺少 .catch() 处理错误
    ```
* **未正确处理 ReadableStream 的错误 (针对 `StreamingResponder`):**  `ReadableStream` 可能会发出 'error' 事件。JavaScript 代码需要监听这个事件来处理流式传输过程中发生的错误。
    ```javascript
    const reader = await generateText("流式生成时会出错的 prompt", true).getReader();
    reader.read().catch(error => { // 处理读取过程中的错误
      console.error("读取流时出错:", error);
    });
    ```
* **忘记传递或使用 `AbortSignal`:** 如果用户希望取消 AI 操作，但 JavaScript 代码没有正确地创建和传递 `AbortSignal`，那么即使取消操作，底层的 AI 模型执行可能仍然会继续运行，浪费资源。
    ```javascript
    const controller = new AbortController();
    generateText("一个很长的生成任务", false, controller.signal).then(/* ... */);
    // ... 一段时间后 ...
    controller.abort(); // 希望取消，但如果 generateText 没有正确传递 signal，则无效
    ```

**用户操作如何一步步到达这里 (调试线索)**

1. **用户在网页上触发了与 AI 功能相关的操作。** 例如，点击了一个“生成文本”的按钮，或者在一个文本框中输入了内容并提交。
2. **网页上的 JavaScript 代码被执行。** 这个 JavaScript 代码会调用相关的 AI API，例如前面例子中的 `generateText` 函数。
3. **JavaScript 代码发起一个网络请求 (通常使用 `fetch`) 到后端。** 这个请求的目标 URL 可能包含 `/ai/generate` 或类似的路径，表明这是一个 AI 相关的请求。请求体中可能包含了用户的输入 (prompt) 和指示是否需要流式输出的参数。
4. **Blink 引擎接收到这个网络请求。** 根据请求的 URL 和内容，Blink 引擎会路由到相应的 C++ 代码进行处理，这可能涉及到调用底层的 AI 模型执行服务。
5. **AI 模型执行服务开始执行，并可能产生输出。**  输出可以是流式的 (逐步产生) 或非流式的 (一次性产生)。
6. **根据请求参数，Blink 引擎会创建 `Responder` 或 `StreamingResponder` 的实例。**
7. **底层 AI 模型执行服务的输出通过 Mojo 接口传递到 `Responder` 或 `StreamingResponder`。**  `OnStreaming` 方法会被多次调用来传递数据片段。
8. **如果模型执行完成，`OnCompletion` 方法会被调用。如果发生错误，`OnError` 方法会被调用。**
9. **`Responder` 会解析其关联的 JavaScript Promise，将最终结果传递给 JavaScript。`StreamingResponder` 会将数据放入 `ReadableStream`，供 JavaScript 异步读取。**
10. **JavaScript 代码接收到 Promise 的结果或从 `ReadableStream` 读取数据，并更新网页的显示。**

在调试过程中，可以使用 Chrome 的开发者工具：

* **Network 面板:**  查看发出的网络请求，确认请求的 URL、方法、headers 和 body 是否正确。可以查看响应的状态码和内容。
* **Sources 面板:**  设置断点在 JavaScript 代码中，查看变量的值，跟踪代码的执行流程。也可以尝试在 `model_execution_responder.cc` 中设置断点 (如果可以构建 debug 版本的 Chromium)。
* **Console 面板:** 查看 JavaScript 代码的 `console.log` 输出，以及可能出现的错误信息。

理解 `model_execution_responder.cc` 的功能及其与 JavaScript 的联系，有助于理解 Chromium 中 AI 功能的实现方式，并能更好地调试与 AI 功能相关的网页应用。

### 提示词
```
这是目录为blink/renderer/modules/ai/model_execution_responder.cc的chromium blink引擎源代码文件， 请列举一下它的功能, 
如果它与javascript, html, css的功能有关系，请做出对应的举例说明，
如果做了逻辑推理，请给出假设输入与输出,
如果涉及用户或者编程常见的使用错误，请举例说明,
说明用户操作是如何一步步的到达这里，作为调试线索。
```

### 源代码
```cpp
// Copyright 2024 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "third_party/blink/renderer/modules/ai/model_execution_responder.h"

#include <optional>

#include "base/functional/callback_forward.h"
#include "base/metrics/histogram_functions.h"
#include "third_party/blink/public/mojom/ai/model_streaming_responder.mojom-blink.h"
#include "third_party/blink/renderer/bindings/core/v8/script_promise_resolver.h"
#include "third_party/blink/renderer/core/dom/abort_signal.h"
#include "third_party/blink/renderer/core/execution_context/execution_context.h"
#include "third_party/blink/renderer/core/streams/readable_stream.h"
#include "third_party/blink/renderer/core/streams/readable_stream_default_controller_with_script_scope.h"
#include "third_party/blink/renderer/core/streams/underlying_source_base.h"
#include "third_party/blink/renderer/modules/ai/exception_helpers.h"
#include "third_party/blink/renderer/platform/bindings/script_state.h"
#include "third_party/blink/renderer/platform/context_lifecycle_observer.h"
#include "third_party/blink/renderer/platform/heap/garbage_collected.h"
#include "third_party/blink/renderer/platform/heap/persistent.h"
#include "third_party/blink/renderer/platform/heap/self_keep_alive.h"
#include "third_party/blink/renderer/platform/mojo/heap_mojo_receiver.h"
#include "third_party/blink/renderer/platform/wtf/functional.h"
#include "third_party/blink/renderer/platform/wtf/text/wtf_string.h"

namespace blink {

namespace {

// Implementation of blink::mojom::blink::ModelStreamingResponder that
// handles the streaming output of the model execution, and returns the full
// result through a promise.
class Responder final : public GarbageCollected<Responder>,
                        public mojom::blink::ModelStreamingResponder,
                        public ContextLifecycleObserver {
 public:
  Responder(ScriptState* script_state,
            AbortSignal* signal,
            ScriptPromiseResolver<IDLString>* resolver,
            AIMetrics::AISessionType session_type,
            base::OnceCallback<void(mojom::blink::ModelExecutionContextInfoPtr)>
                complete_callback)
      : script_state_(script_state),
        resolver_(resolver),
        receiver_(this, ExecutionContext::From(script_state)),
        abort_signal_(signal),
        session_type_(session_type),
        complete_callback_(std::move(complete_callback)) {
    SetContextLifecycleNotifier(ExecutionContext::From(script_state));
    if (abort_signal_) {
      CHECK(!abort_signal_->aborted());
      abort_handle_ = abort_signal_->AddAlgorithm(
          WTF::BindOnce(&Responder::OnAborted, WrapWeakPersistent(this)));
    }
  }
  ~Responder() override = default;
  Responder(const Responder&) = delete;
  Responder& operator=(const Responder&) = delete;

  void Trace(Visitor* visitor) const override {
    ContextLifecycleObserver::Trace(visitor);
    visitor->Trace(script_state_);
    visitor->Trace(resolver_);
    visitor->Trace(receiver_);
    visitor->Trace(abort_signal_);
    visitor->Trace(abort_handle_);
  }

  ScriptPromise<IDLString> GetPromise() { return resolver_->Promise(); }

  mojo::PendingRemote<blink::mojom::blink::ModelStreamingResponder>
  BindNewPipeAndPassRemote(
      scoped_refptr<base::SequencedTaskRunner> task_runner) {
    return receiver_.BindNewPipeAndPassRemote(task_runner);
  }

  // `mojom::blink::ModelStreamingResponder` implementation.
  void OnStreaming(const String& text) override {
    RecordResponseStatusMetrics(
        mojom::blink::ModelStreamingResponseStatus::kOngoing);
    response_callback_count_++;
    // Update the response with the latest value.
    response_ = text;
  }

  void OnCompletion(
      mojom::blink::ModelExecutionContextInfoPtr context_info) override {
    RecordResponseStatusMetrics(
        mojom::blink::ModelStreamingResponseStatus::kComplete);
    response_callback_count_++;

    resolver_->Resolve(response_);
    if (context_info && complete_callback_) {
      std::move(complete_callback_).Run(std::move(context_info));
    }
    RecordResponseMetrics();
    Cleanup();
  }

  void OnError(mojom::blink::ModelStreamingResponseStatus status) override {
    RecordResponseStatusMetrics(status);
    response_callback_count_++;
    resolver_->Reject(ConvertModelStreamingResponseErrorToDOMException(status));
    RecordResponseMetrics();
    Cleanup();
  }

  // ContextLifecycleObserver implementation.
  void ContextDestroyed() override { Cleanup(); }

 private:
  void OnAborted() {
    if (!resolver_) {
      return;
    }
    resolver_->Reject(abort_signal_->reason(script_state_));
    Cleanup();
  }

  void RecordResponseStatusMetrics(
      mojom::blink::ModelStreamingResponseStatus status) {
    base::UmaHistogramEnumeration(
        AIMetrics::GetAISessionResponseStatusMetricName(session_type_), status);
  }

  void RecordResponseMetrics() {
    base::UmaHistogramCounts1M(
        AIMetrics::GetAISessionResponseSizeMetricName(session_type_),
        int(response_.CharactersSizeInBytes()));
    base::UmaHistogramCounts1M(
        AIMetrics::GetAISessionResponseCallbackCountMetricName(session_type_),
        response_callback_count_);
  }

  void Cleanup() {
    resolver_ = nullptr;
    receiver_.reset();
    keep_alive_.Clear();
    if (abort_handle_) {
      abort_signal_->RemoveAlgorithm(abort_handle_);
      abort_handle_ = nullptr;
    }
  }

  Member<ScriptState> script_state_;
  Member<ScriptPromiseResolver<IDLString>> resolver_;
  String response_;
  int response_callback_count_ = 0;
  HeapMojoReceiver<blink::mojom::blink::ModelStreamingResponder, Responder>
      receiver_;
  SelfKeepAlive<Responder> keep_alive_{this};
  Member<AbortSignal> abort_signal_;
  Member<AbortSignal::AlgorithmHandle> abort_handle_;
  const AIMetrics::AISessionType session_type_;
  // The callback will be invoked once when the responder receive the first
  // `kComplete`.
  base::OnceCallback<void(
      mojom::blink::ModelExecutionContextInfoPtr context_info)>
      complete_callback_;
};

// Implementation of blink::mojom::blink::ModelStreamingResponder that
// handles the streaming output of the model execution, and returns the full
// result through a ReadableStream.
class StreamingResponder final
    : public UnderlyingSourceBase,
      public blink::mojom::blink::ModelStreamingResponder {
 public:
  StreamingResponder(
      ScriptState* script_state,
      AbortSignal* signal,
      AIMetrics::AISessionType session_type,
      base::OnceCallback<void(mojom::blink::ModelExecutionContextInfoPtr)>
          complete_callback)
      : UnderlyingSourceBase(script_state),
        script_state_(script_state),
        receiver_(this, ExecutionContext::From(script_state)),
        abort_signal_(signal),
        session_type_(session_type),
        complete_callback_(std::move(complete_callback)) {
    if (abort_signal_) {
      CHECK(!abort_signal_->aborted());
      abort_handle_ = abort_signal_->AddAlgorithm(WTF::BindOnce(
          &StreamingResponder::OnAborted, WrapWeakPersistent(this)));
    }
  }
  ~StreamingResponder() override = default;

  StreamingResponder(const StreamingResponder&) = delete;
  StreamingResponder& operator=(const StreamingResponder&) = delete;

  void Trace(Visitor* visitor) const override {
    UnderlyingSourceBase::Trace(visitor);
    visitor->Trace(script_state_);
    visitor->Trace(receiver_);
    visitor->Trace(abort_signal_);
    visitor->Trace(abort_handle_);
  }

  mojo::PendingRemote<blink::mojom::blink::ModelStreamingResponder>
  BindNewPipeAndPassRemote(
      scoped_refptr<base::SequencedTaskRunner> task_runner) {
    return receiver_.BindNewPipeAndPassRemote(task_runner);
  }

  ReadableStream* CreateReadableStream() {
    // Set the high water mark to 1 so the backpressure will be applied on every
    // enqueue.
    return ReadableStream::CreateWithCountQueueingStrategy(script_state_, this,
                                                           1);
  }

  // `UnderlyingSourceBase` implementation.
  ScriptPromise<IDLUndefined> Pull(ScriptState* script_state,
                                   ExceptionState& exception_state) override {
    return ToResolvedUndefinedPromise(script_state);
  }

  ScriptPromise<IDLUndefined> Cancel(ScriptState* script_state,
                                     ScriptValue reason,
                                     ExceptionState& exception_state) override {
    return ToResolvedUndefinedPromise(script_state);
  }

  // `blink::mojom::blink::ModelStreamingResponder` implementation.
  void OnStreaming(

      const String& text) override {
    RecordResponseStatusMetrics(
        mojom::blink::ModelStreamingResponseStatus::kOngoing);
    // Update the response info and enqueue the latest response.
    response_callback_count_++;
    response_size_ = int(text.CharactersSizeInBytes());
    v8::HandleScope handle_scope(script_state_->GetIsolate());
    Controller()->Enqueue(V8String(script_state_->GetIsolate(), text));
  }

  void OnCompletion(
      mojom::blink::ModelExecutionContextInfoPtr context_info) override {
    RecordResponseStatusMetrics(
        mojom::blink::ModelStreamingResponseStatus::kComplete);
    response_callback_count_++;
    Controller()->Close();
    if (context_info && complete_callback_) {
      std::move(complete_callback_).Run(std::move(context_info));
    }
    RecordResponseMetrics();
    Cleanup();
    return;
  }

  void OnError(ModelStreamingResponseStatus status) override {
    RecordResponseStatusMetrics(status);
    response_callback_count_++;
    Controller()->Error(
        ConvertModelStreamingResponseErrorToDOMException(status));
    RecordResponseMetrics();
    Cleanup();
  }

 private:
  void OnAborted() {
    // TODO(crbug.com/374879795): fix the abort handling for streaming
    // responder.
    Controller()->Error(DOMException::Create(
        kExceptionMessageRequestAborted,
        DOMException::GetErrorName(DOMExceptionCode::kAbortError)));
    Cleanup();
  }

  void RecordResponseStatusMetrics(
      mojom::blink::ModelStreamingResponseStatus status) {
    base::UmaHistogramEnumeration(
        AIMetrics::GetAISessionResponseStatusMetricName(session_type_), status);
  }

  void RecordResponseMetrics() {
    base::UmaHistogramCounts1M(
        AIMetrics::GetAISessionResponseSizeMetricName(session_type_),
        response_size_);
    base::UmaHistogramCounts1M(
        AIMetrics::GetAISessionResponseCallbackCountMetricName(session_type_),
        response_callback_count_);
  }

  void Cleanup() {
    script_state_ = nullptr;
    receiver_.reset();
    if (abort_handle_) {
      abort_signal_->RemoveAlgorithm(abort_handle_);
      abort_handle_ = nullptr;
    }
  }

  int response_size_ = 0;
  int response_callback_count_ = 0;
  Member<ScriptState> script_state_;
  HeapMojoReceiver<blink::mojom::blink::ModelStreamingResponder,
                   StreamingResponder>
      receiver_;
  Member<AbortSignal> abort_signal_;
  Member<AbortSignal::AlgorithmHandle> abort_handle_;
  const AIMetrics::AISessionType session_type_;
  // The callback will be invoked once when the responder receive the first
  // `kComplete`.
  base::OnceCallback<void(mojom::blink::ModelExecutionContextInfoPtr)>
      complete_callback_;
};

}  // namespace

mojo::PendingRemote<blink::mojom::blink::ModelStreamingResponder>
CreateModelExecutionResponder(
    ScriptState* script_state,
    AbortSignal* signal,
    ScriptPromiseResolver<IDLString>* resolver,
    scoped_refptr<base::SequencedTaskRunner> task_runner,
    AIMetrics::AISessionType session_type,
    base::OnceCallback<void(mojom::blink::ModelExecutionContextInfoPtr)>
        complete_callback) {
  Responder* responder = MakeGarbageCollected<Responder>(
      script_state, signal, resolver, session_type,
      std::move(complete_callback));
  return responder->BindNewPipeAndPassRemote(task_runner);
}

std::tuple<ReadableStream*,
           mojo::PendingRemote<blink::mojom::blink::ModelStreamingResponder>>
CreateModelExecutionStreamingResponder(
    ScriptState* script_state,
    AbortSignal* signal,
    scoped_refptr<base::SequencedTaskRunner> task_runner,
    AIMetrics::AISessionType session_type,
    base::OnceCallback<void(mojom::blink::ModelExecutionContextInfoPtr)>
        complete_callback) {
  StreamingResponder* streaming_responder =
      MakeGarbageCollected<StreamingResponder>(
          script_state, signal, session_type, std::move(complete_callback));
  return std::make_tuple(
      streaming_responder->CreateReadableStream(),
      streaming_responder->BindNewPipeAndPassRemote(task_runner));
}

}  // namespace blink
```