Response:
The user is asking for an analysis of a C++ source code file. The file `darkmode_classifier.cc` seems to be auto-generated code for a TensorFlow Lite model.

Here's a breakdown of the thought process to answer the request:

1. **Identify the Core Function:** The file name `darkmode_classifier.cc` and the comment "This file is automatically generated using tfNative from a neural network" strongly suggest this code implements a machine learning model designed to classify content for dark mode adaptation.

2. **Analyze the Code Structure:**
    * **Includes:**  The included headers like `<algorithm>`, `<cmath>`, `<cstring>`, `<limits>`, and the conditional inclusion of `<third_party/eigen3/Eigen/Core>` point to numerical computations and data manipulation.
    * **Namespace:** The code is within `darkmode_tfnative_model`, reinforcing the idea of a specific model implementation.
    * **OP LIBRARY:**  The large section labeled "OP LIBRARY" is crucial. It contains implementations of various neural network operations (e.g., `Conv2DAsGemm`, `MaxPool`, `FullyConnected`, `Softmax`). This confirms it's a model inference engine.
    * **Macros:** The use of macros like `SIMPLE_UNARY_OP` suggests a way to generate code for similar operations efficiently.

3. **Infer the Functionality:** Based on the "OP LIBRARY," the primary function is to perform inference on a pre-trained neural network model. This model likely takes some form of input (likely image data) and outputs a classification result related to dark mode suitability.

4. **Relate to Web Technologies (JavaScript, HTML, CSS):**
    * **Relevance:**  A dark mode classifier in the rendering engine (Blink) is directly related to how web content is displayed. It likely informs decisions about applying dark mode adjustments.
    * **Examples:**
        * **Input:** The model might receive pixel data from an HTML element (e.g., an `<img>` tag or a `<canvas>`).
        * **Processing:** The C++ code performs the neural network calculations on this pixel data.
        * **Output:** The model outputs a probability or classification score indicating if the content is "light" or "dark".
        * **CSS Interaction:**  This score could influence CSS rules. For example, if the classifier determines an image is already dark, the browser might avoid applying further dark mode filters to it. It could also trigger specific CSS styles for elements based on the overall page "darkness."

5. **Logic and Reasoning (Hypothetical Input/Output):**
    * **Input:**  Consider a simplified scenario. The input to the model could be a flattened array of grayscale pixel values from a small image. For instance, a 4x4 grayscale image would have 16 input values (0-255).
    * **Processing:** The code would perform matrix multiplications, additions (with biases), and activation functions (like ReLU or Sigmoid) as defined by the model's architecture.
    * **Output:** The final output could be a single value between 0 and 1, where a value close to 0 indicates a "light" image and a value close to 1 indicates a "dark" image. Alternatively, it could be a vector of probabilities for different classes (e.g., "light," "dark," "neutral").

6. **User/Programming Errors:**
    * **Incorrect Input Shape/Format:** A common error is providing input data that doesn't match the model's expected input shape (e.g., wrong dimensions or data type). The code has `assert` statements in some operations, which could trigger in debug builds if shapes are mismatched.
    * **Memory Management (less common with generated code, but possible in related areas):**  While less likely in *this* generated code, general C++ programming errors related to memory allocation or deallocation could cause crashes.
    * **Model Mismatch:** If the input data was intended for a different model, the results would be meaningless.

7. **Summarize the Functionality (Part 1):**  The core function of this code is to implement the inference logic of a neural network model specifically designed for dark mode classification. It takes numerical input, performs a series of mathematical operations defined by the model's architecture, and produces an output that indicates whether the input is likely "dark" or "light."  This output is intended to be used by the browser's rendering engine to make decisions about applying dark mode adjustments to web content.
```cpp
// Copyright 2017 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifdef UNSAFE_BUFFERS_BUILD
// TODO(crbug.com/351564777): Remove this and convert code to safer constructs.
#pragma allow_unsafe_buffers
#endif

// This file is automatically generated using tfNative from a neural network,
// trained by TensorFlow. Please do not edit.

#include "darkmode_classifier.h"
#include <algorithm>
#include <cassert>
#include <cmath>
#include <cstdint>
#include <cstring>
#include <limits>
#include <tuple>
#if USE_EIGEN
#include "third_party/eigen3/Eigen/Core"
#endif
namespace darkmode_tfnative_model {
namespace {

// -----------------------------------------------------------------------------
// OP LIBRARY
// Copied here to make sure that the inferece code always stays in sync with the
// lib that it was generated for.
// -----------------------------------------------------------------------------

// Default to using std::copy and std::fill over memcpy and memset as they
// are usually faster, thanks to the compiler getting stricter alignment
// guarantees.
#ifndef USE_TYPED_MEMSETMEMCPY
#define USE_TYPED_MEMSETMEMCPY 1
#endif
#define USE_EIGEN 0
#ifndef USE_EIGEN
#error Please define USE_EIGEN to either 0 or 1
#endif

// Helper to reinterpret memory as Eigen matrices.
#if USE_EIGEN
template <typename Scalar>
using ConstMatrixMap = typename Eigen::Map<
    const Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>;
template <typename Scalar>
using ConstRowVectorMap =
    typename Eigen::Map<const Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>;
template <typename Scalar>
using RowVectorMap =
    typename Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>;
template <typename Scalar>
using MatrixMap =
    typename Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>;
#endif

#define BENCHMARK_TIMER(...)

// The size of a shape in terms of number of coefficients.
inline int ShapeSize(const int32_t rank, const int32_t* shape) {
  int size = 1;
  for (int i = 0; i < rank; ++i)
    size *= shape[i];
  return size;
}

// Helper to compute the size of the inner loop for an op that uses indices to
// specify which axes are reduced.
template <typename Tidx>
int32_t GetReduceInnerSize(int32_t input_tensor_rank,
                           const int32_t* __restrict input_shape,
                           int32_t index_tensor_rank,
                           const int32_t* __restrict index_shape,
                           const Tidx* __restrict index_values) {
  assert(index_tensor_rank <= 1);
  const int32_t num_indices = index_tensor_rank > 0 ? index_shape[0] : 1;
  int32_t inner_size = 1;
  for (int32_t i = 0; i < num_indices; ++i) {
    inner_size *= input_shape[index_values[i]];
  }
  return inner_size;
}

template <typename T>
void ConcatV2Args2(int32_t arg0_rank,
                   const int32_t* __restrict arg0_shape,
                   const T* __restrict arg0_values,
                   int32_t arg1_rank,
                   const int32_t* __restrict arg1_shape,
                   const T* __restrict arg1_values,
                   const int32_t* __restrict axis_value,
                   T* __restrict output_values) {
  BENCHMARK_TIMER("ConcatV2Args2");
  const int axis = axis_value[0];
  const int num_lines = ShapeSize(axis, arg0_shape);
  const int arg0_line_size = ShapeSize(arg0_rank - axis, arg0_shape + axis);
  const int arg1_line_size = ShapeSize(arg1_rank - axis, arg1_shape + axis);
  for (int line = 0; line < num_lines; ++line) {
    std::copy(arg0_values, arg0_values + arg0_line_size, output_values);
    arg0_values += arg0_line_size;
    output_values += arg0_line_size;
    std::copy(arg1_values, arg1_values + arg1_line_size, output_values);
    arg1_values += arg1_line_size;
    output_values += arg1_line_size;
  }
}

template <typename T>
void Conv2DAsGemm(const int32_t* __restrict in_shape,
                  const T* __restrict in_values,
                  const int32_t* __restrict filter_shape,
                  const T* __restrict filter_values,
                  T* __restrict output_values) {
  BENCHMARK_TIMER("Conv2DAsGemm");
#if USE_EIGEN
  const auto in = ConstMatrixMap<T>(in_values, in_shape[0], in_shape[1]);
  const auto filter =
      ConstMatrixMap<T>(filter_values, filter_shape[3],
                        filter_shape[0] * filter_shape[1] * filter_shape[2]);
  auto result = MatrixMap<T>(output_values, filter_shape[3], in_shape[1]);
  result.noalias() = filter * in;
#else
  const int32_t out_rows = in_shape[1];
  const int32_t out_cols = filter_shape[3];
  const int32_t dot_len = in_shape[0];
  for (int row = 0; row < out_rows; ++row) {
    for (int col = 0; col < out_cols; ++col) {
      T value = 0;
      for (int i = 0; i < dot_len; ++i) {
        value +=
            in_values[row * dot_len + i] * filter_values[i * out_cols + col];
      }
      *output_values++ = value;
    }
  }
#endif
}

template <typename T>
void DepthwiseConv2dNative(const int32_t* __restrict input_shape,
                           const T* __restrict input_values,
                           const int32_t* __restrict kernel_shape,
                           const T* __restrict kernel_values,
                           int32_t stride_y,
                           int32_t stride_x,
                           int32_t out_height,
                           int32_t out_width,
                           T* __restrict output_values) {
  BENCHMARK_TIMER("DepthwiseConv2dNative");
  // Give the shape values nicer names.
  assert(input_shape[3] == kernel_shape[2]);
  const int batch_size = input_shape[0];
  const int kernel_height = kernel_shape[0];
  const int kernel_width = kernel_shape[1];
  const int in_depth = kernel_shape[2];
  const int depth_mul = kernel_shape[3];
  const int in_height = input_shape[1];
  const int in_width = input_shape[2];

  // Compute the amount of padding needed to get the desired output size.
  const int pad_height =
      ((out_height - 1) * stride_y + kernel_height - in_height) / 2;
  const int pad_width =
      ((out_width - 1) * stride_x + kernel_width - in_width) / 2;

  // Cache the strides for address computations.
  const int in_strides[4] = {
      input_shape[1] * input_shape[2] * input_shape[3],  // batch
      input_shape[2] * input_shape[3],                   // y
      input_shape[3],                                    // x
      1,                                                 // channel
  };
  const int kernel_strides[4] = {
      kernel_shape[1] * kernel_shape[2] * kernel_shape[3],  // y
      kernel_shape[2] * kernel_shape[3],                    // x
      kernel_shape[3],                                      // in channels
      1,                                                    // channel mult
  };

  T* out_write_ptr = output_values;
  for (int batch = 0; batch < batch_size; ++batch) {
    for (int out_y = 0; out_y < out_height; ++out_y) {
      for (int out_x = 0; out_x < out_width; ++out_x) {
        // Compute the input read offsets.
        const int in_y_origin = (out_y * stride_y) - pad_height;
        const int in_x_origin = (out_x * stride_x) - pad_width;

        // Compute the range of the kernel to be applied (we may need to clip
        // when we'd read outside of the valid input region - for SAME).
        const int kernel_y_start = std::max(0, -in_y_origin);
        const int kernel_y_end =
            std::min(kernel_height, in_height - in_y_origin);
        const int kernel_x_start = std::max(0, -in_x_origin);
        const int kernel_x_end = std::min(kernel_width, in_width - in_x_origin);

        for (int in_c = 0; in_c < in_depth; ++in_c) {
          for (int mul_c = 0; mul_c < depth_mul; ++mul_c, ++out_write_ptr) {
            // Convolve.
            T sum = 0;
            for (int k_y = kernel_y_start; k_y < kernel_y_end; ++k_y) {
              const int in_y = in_y_origin + k_y;
              assert(in_y >= 0 && in_y < in_height);
              for (int k_x = kernel_x_start; k_x < kernel_x_end; ++k_x) {
                const int in_x = in_x_origin + k_x;
                assert(in_x >= 0 && in_x < in_width);
                const T input_value =
                    input_values[batch * in_strides[0] +  // batch
                                 in_y * in_strides[1] +   // y
                                 in_x * in_strides[2] +   // x
                                 in_c];                   // in chan
                const T kernel_value =
                    kernel_values[k_y * kernel_strides[0] +   // y
                                  k_x * kernel_strides[1] +   // x
                                  in_c * kernel_strides[2] +  // in chan
                                  mul_c];                     // chan mult
                sum += input_value * kernel_value;
              }
            }
            *out_write_ptr = sum;
          }  // mul_c
        }    // in_c
      }      // out_x
    }        // out_y
  }          // batch
}

template <typename T>
void FullyConnected(const int32_t* __restrict input_shape,
                    const T* __restrict input_values,
                    const int32_t* __restrict weight_shape,
                    const T* __restrict weight_values,
                    const int32_t* __restrict bias_shape,
                    const T* __restrict bias_values,
                    T* __restrict output_values) {
  BENCHMARK_TIMER("FullyConnected");
#if USE_EIGEN
  const auto in =
      ConstMatrixMap<T>(input_values, input_shape[1], input_shape[0]);
  const auto weight =
      ConstMatrixMap<T>(weight_values, weight_shape[1], weight_shape[0]);
  const auto bias = ConstRowVectorMap<T>(bias_values, bias_shape[0]);
  auto result = MatrixMap<T>(output_values, weight_shape[1], input_shape[0]);
  result.noalias() = (weight * in).colwise() + bias;
#else
  const int batch_size = input_shape[0];
  const int num_inputs = weight_shape[0];
  const int num_outputs = weight_shape[1];
  assert(input_shape[1] == num_inputs);
  assert(bias_shape[0] == num_outputs);
  for (int batch = 0; batch < batch_size; ++batch) {
    for (int out_i = 0; out_i < num_outputs; ++out_i) {
      T value = 0;
      for (int in_i = 0; in_i < num_inputs; ++in_i) {
        value += input_values[batch * num_inputs + in_i] *
                 weight_values[in_i * num_outputs + out_i];
      }
      value += bias_values[out_i];
      output_values[batch * num_outputs + out_i] = value;
    }
  }
#endif
}

template <typename T, typename TIndex>
void Gather(int params_rank,
            const int32_t* __restrict params_shape,
            const T* __restrict params_values,
            int indices_rank,
            const int32_t* __restrict indices_shape,
            const TIndex* __restrict indices_values,
            T* __restrict output_values) {
  BENCHMARK_TIMER("Gather");
  const int num_indices = ShapeSize(indices_rank, indices_shape);
  const int num_params = params_shape[0];
  const int slice_size = ShapeSize(params_rank - 1, params_shape + 1);
  for (int i = 0; i < num_indices; ++i) {
    const int index = indices_values[i];
    if (index < 0 || index >= num_params) {
      std::fill(output_values, output_values + slice_size, 0);
    } else {
      std::copy(params_values + index * slice_size,
                params_values + index * slice_size + slice_size, output_values);
    }
    output_values += slice_size;
  }
}

template <typename T, typename TIndex>
void Im2Col(const int32_t* __restrict input_shape,
            const T* __restrict input_values,
            const int32_t* __restrict kernel_shape,
            int32_t stride_y,
            int32_t stride_x,
            int32_t out_height,
            int32_t out_width,
            TIndex* output_shape,
            T* __restrict output_values) {
  BENCHMARK_TIMER("Im2Col");
  // Give the shape values nicer names.
  assert(input_shape[3] == kernel_shape[2]);
  const int batch_size = input_shape[0];
  const int kernel_height = kernel_shape[0];
  const int kernel_width = kernel_shape[1];
  const int in_depth = kernel_shape[2];
  const int in_height = input_shape[1];
  const int in_width = input_shape[2];

  // Compute the amount of padding needed to get the desired output size.
  const int pad_height =
      ((out_height - 1) * stride_y + kernel_height - in_height) / 2;
  const int pad_width =
      ((out_width - 1) * stride_x + kernel_width - in_width) / 2;

  // Cache the strides for address computations.
  const int x_stride = input_shape[3];
  const int y_stride = input_shape[2] * x_stride;
  const int batch_stride = input_shape[1] * y_stride;

  // Write the output shape.
  output_shape[0] = kernel_height * kernel_width * in_depth;
  output_shape[1] = input_shape[0] * out_width * out_height;

  for (int batch = 0; batch < batch_size; ++batch) {
    for (int out_y = 0; out_y < out_height; ++out_y) {
      for (int out_x = 0; out_x < out_width; ++out_x) {
        // Compute the input read offsets.
        const int in_y_origin = (out_y * stride_y) - pad_height;
        const int in_x_origin = (out_x * stride_x) - pad_width;

        // Compute the range of the kernel to be applied (we may need to clip
        // when we'd read outside of the valid input region - for SAME).
        const int kernel_y_start = std::max(0, -in_y_origin);
        const int kernel_y_end =
            std::min(kernel_height, in_height - in_y_origin);
        const int kernel_x_start = std::max(0, -in_x_origin);
        const int kernel_x_end = std::min(kernel_width, in_width - in_x_origin);

        // Padding top.
        if (kernel_y_start != 0) {
          const int num_lines = kernel_y_start;
          const int num_coeffs = num_lines * kernel_width * in_depth;
#if USE_TYPED_MEMSETMEMCPY
          std::fill(output_values, output_values + num_coeffs, 0);
#else
          std::memset(output_values, 0, num_coeffs * sizeof(T));
#endif
          output_values += num_coeffs;
        }
        for (int k_y = kernel_y_start; k_y < kernel_y_end; ++k_y) {
          // Padding left.
          if (kernel_x_start != 0) {
            const int num_coeffs = kernel_x_start * in_depth;
#if USE_TYPED_MEMSETMEMCPY
            std::fill(output_values, output_values + num_coeffs, 0);
#else
            std::memset(output_values, 0, num_coeffs * sizeof(T));
#endif
            output_values += num_coeffs;
          }
          // Valid values.
          {
            const int in_y = in_y_origin + k_y;
            const int in_x = in_x_origin + kernel_x_start;
            const int num_coeffs = (kernel_x_end - kernel_x_start) * in_depth;
#if USE_TYPED_MEMSETMEMCPY
            const int offset =
                batch * batch_stride + in_y * y_stride + in_x * x_stride;
            std::copy(input_values + offset, input_values + offset + num_coeffs,
                      output_values);
#else
            std::memcpy(output_values,
                        input_values  // Reusing the restricted pointer.
                            + batch * batch_stride  // batch
                            + in_y * y_stride       // y
                            + in_x * x_stride,      // x
                        num_coeffs * sizeof(T));
#endif
            output_values += num_coeffs;
          }
          // Padding right.
          if (kernel_x_end != kernel_width) {
            const int num_coeffs = (kernel_width - kernel_x_end) * in_depth;
#if USE_TYPED_MEMSETMEMCPY
            std::fill(output_values, output_values + num_coeffs, 0);
#else
            std::memset(output_values, 0, num_coeffs * sizeof(T));
#endif
            output_values += num_coeffs;
          }
        }
        // Padding bottom.
        if (kernel_y_end != kernel_height) {
          const int num_lines = kernel_height - kernel_y_end;
          const int num_coeffs = num_lines * kernel_width * in_depth;
#if USE_TYPED_MEMSETMEMCPY
          std::fill(output_values, output_values + num_coeffs, 0);
#else
          std::memset(output_values, 0, num_coeffs * sizeof(T));
#endif
          output_values += num_coeffs;
        }
      }
    }
  }
}

template <typename T>
void MaxPool(const int32_t* __restrict input_shape,
             const T* __restrict input_values,
             int32_t stride_y,
             int32_t stride_x,
             int32_t kernel_height,
             int32_t kernel_width,
             int32_t out_height,
             int32_t out_width,
             T* __restrict output_values) {
  BENCHMARK_TIMER("MaxPool");
  // Give the shape values nicer names.
  const int batch_size = input_shape[0];
  const int in_height = input_shape[1];
  const int in_width = input_shape[2];
  const int depth = input_shape[3];

  // Compute the amount of padding needed to get the desired output size.
  const int pad_height =
      ((out_height - 1) * stride_y + kernel_height - in_height) / 2;
  const int pad_width =
      ((out_width - 1) * stride_x + kernel_width - in_width) / 2;

  // Cache the strides for address computations.
  const int in_strides[4] = {
      input_shape[1] * input_shape[2] * input_shape[3],  // batch
      input_shape[2] * input_shape[3],                   // y
      input_shape[3],                                    // x
      1,                                                 // channel
  };

  T* out_write_ptr = output_values;
  for (int batch = 0; batch < batch_size; ++batch) {
    for (int out_y = 0; out_y < out_height; ++out_y) {
      for (int out_x = 0; out_x < out_width; ++out_x) {
        // Compute the input read offsets.
        const int in_y_origin = (out_y * stride_y) - pad_height;
        const int in_x_origin = (out_x * stride_x) - pad_width;

        // Compute the range of the kernel to be applied (we may need to clip
        // when we'd read outside of the valid input region - for SAME).
        const int kernel_y_start = std::max(0, -in_y_origin);
        const int kernel_y_end =
            std::min(kernel_height, in_height - in_y_origin);
        const int kernel_x_start = std::max(0, -in_x_origin);
        const int kernel_x_end = std::min(kernel_width, in_width - in_x_origin);

        for (int chan = 0; chan < depth; ++chan, ++out_write_ptr) {
          // Convolve.
          T max_value = std::numeric_limits<T>::lowest();
          for (int k_y = kernel_y_start; k_y < kernel_y_end; ++k_y) {
            const int in_y = in_y_origin + k_y;
            assert(in_y >= 0 && in_y < in_height);
            for (int k_x = kernel_x_start; k_x < kernel_x_end; ++k_x) {
              const int in_x = in_x_origin + k_x;
              assert(in_x >= 0 && in_x < in_width);
              const T input_value =
                  input_values[batch * in_strides[0] +  // batch
                               in_y * in_strides[1] +   // y
                               in_x * in_strides[2] +   // x
                               chan];                   // channel
              max_value = std::max(max_value, input_value);
            }  // kernel_x
          }    // kernel_y
          *out_write_ptr = max_value;
        }  // chan
      }    // out_x
    }      // out_y
  }        // batch
}

template <typename T>
void Memcpy(const int32_t rank,
            const int32_t* __restrict input_shape,
            const T* __restrict input_values,
            T* __restrict output_values) {
  BENCHMARK_TIMER("Memcpy");
  const int size = ShapeSize(rank, input_shape);
  for (int i = 0; i < size; ++i) {
    output_values[i] = input_values[i];
  }
}

template <typename T>
void Softmax(const int32_t rank,
             const int32_t* __restrict input_shape,
             const T* __restrict input_values,
             const int32_t reduce_dim,
             T* __restrict output_values,
             T* __restrict scratch_values) {
  BENCHMARK_TIMER("Softmax");
  const int size = ShapeSize(rank, input_shape);
  if (rank == 2 && reduce_dim == 1) {
    T logits_max = std::numeric_limits<T>::lowest();

    // Max.
    for (int i = 0; i < size; ++i) {
      logits_max = std::max(logits_max, input_values[i]);
    }

    // Pre-compute exp.
    for (int i = 0; i < size; ++i) {
      scratch_values[i] = std::exp(input_values[i] - logits_max);
    }

    // Sum over the last dimension, then divide the exps and write out.
    for (int offset = 0; offset < size; offset += input_shape[1]) {
      T sum = 0;
      const int end_offset = offset + input_shape[1];
      for (int i = offset; i < end_offset; ++i)
        sum += scratch_values[i];
      const T rcp_denom = static_cast<T>(1) / sum;
      for (int i = 0; i < input_shape[1]; ++i) {
        output_values[offset + i] = scratch_values[offset + i] * rcp_denom;
      }
    }
  } else {
    assert(false && "Generic Softmax not yet supported.");
  }
}

// Returns the start position for a slice in a single dimension.
template <typename T>
int StridedSliceBegin(int range_mask,
                      const T* __restrict range_values,
                      const T* __restrict strides,
                      const int32_t* __restrict input_shape,
                      int dim) {
  const bool is_explicit = 0 == (range_mask & (1 << dim));
  if (is_explicit) {
    return range_values[dim];
  } else {
    const bool is_reverse = strides[dim] < 0;
    return is_reverse ? input_shape[dim] - 1 : 0;
  }
}

// Returns the end position for a slice in a single dimension.
template <typename T>
int StridedSliceEnd(int range_mask,
                    const T* __restrict range_values,
                    const T* __restrict strides,
                    const int32_t* __restrict input_shape,
                    int dim) {
  const bool is_explicit = 0 == (range_mask & (1 << dim));
  if (is_explicit) {
    return range_values[dim];
  } else {
    const bool is_reverse = strides[dim] < 0;
    return is_reverse ? -1 : input_shape[dim];
  }
}

template <typename T, typename TIdx>
void StridedSlice(const int32_t input_rank,
                  const int32_t* __restrict input_shape,
                  const T* __restrict input_values,
                  const TIdx* __restrict begin,
                  const TIdx* __restrict end,
                  const TIdx* __restrict strides,
                  int32_t begin_mask,
                  int32_t end_mask,
                  T* __restrict output_values) {
  BENCHMARK_TIMER("StridedSlice");
  const int MAX_RANK = 8;
  assert(input_rank < MAX_RANK);

  // Compute the address strides for each dimension.
  int dim_addr_strides[MAX_RANK] = {0};
  dim_addr_strides[input_rank - 1] = 1;
  for (int dim = input_rank - 2; dim >= 0; --dim) {
    dim_addr_strides[dim] = dim_addr_strides[dim + 1] * input_shape[dim + 1];
  }

  // Resolve the masks and get explicit ranges for each dimension.
  int dim_begin[MAX_RANK];
  int dim_end[MAX_RANK];
  bool dim_is_full_range[MAX_RANK];
  for (int dim = 0; dim < input_rank; ++dim) {
    const int stride
### 提示词
```
这是目录为blink/renderer/platform/graphics/darkmode/darkmode_classifier.cc的chromium blink引擎源代码文件， 请列举一下它的功能, 
如果它与javascript, html, css的功能有关系，请做出对应的举例说明，
如果做了逻辑推理，请给出假设输入与输出,
如果涉及用户或者编程常见的使用错误，请举例说明
这是第1部分，共2部分，请归纳一下它的功能
```

### 源代码
```cpp
// Copyright 2017 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifdef UNSAFE_BUFFERS_BUILD
// TODO(crbug.com/351564777): Remove this and convert code to safer constructs.
#pragma allow_unsafe_buffers
#endif

// This file is automatically generated using tfNative from a neural network,
// trained by TensorFlow. Please do not edit.

#include "darkmode_classifier.h"
#include <algorithm>
#include <cassert>
#include <cmath>
#include <cstdint>
#include <cstring>
#include <limits>
#include <tuple>
#if USE_EIGEN
#include "third_party/eigen3/Eigen/Core"
#endif
namespace darkmode_tfnative_model {
namespace {

// -----------------------------------------------------------------------------
// OP LIBRARY
// Copied here to make sure that the inferece code always stays in sync with the
// lib that it was generated for.
// -----------------------------------------------------------------------------

// Default to using std::copy and std::fill over memcpy and memset as they
// are usually faster, thanks to the compiler getting stricter alignment
// guarantees.
#ifndef USE_TYPED_MEMSETMEMCPY
#define USE_TYPED_MEMSETMEMCPY 1
#endif
#define USE_EIGEN 0
#ifndef USE_EIGEN
#error Please define USE_EIGEN to either 0 or 1
#endif

// Helper to reinterpret memory as Eigen matrices.
#if USE_EIGEN
template <typename Scalar>
using ConstMatrixMap = typename Eigen::Map<
    const Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>;
template <typename Scalar>
using ConstRowVectorMap =
    typename Eigen::Map<const Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>;
template <typename Scalar>
using RowVectorMap =
    typename Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>;
template <typename Scalar>
using MatrixMap =
    typename Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>;
#endif

#define BENCHMARK_TIMER(...)

// The size of a shape in terms of number of coefficients.
inline int ShapeSize(const int32_t rank, const int32_t* shape) {
  int size = 1;
  for (int i = 0; i < rank; ++i)
    size *= shape[i];
  return size;
}

// Helper to compute the size of the inner loop for an op that uses indices to
// specify which axes are reduced.
template <typename Tidx>
int32_t GetReduceInnerSize(int32_t input_tensor_rank,
                           const int32_t* __restrict input_shape,
                           int32_t index_tensor_rank,
                           const int32_t* __restrict index_shape,
                           const Tidx* __restrict index_values) {
  assert(index_tensor_rank <= 1);
  const int32_t num_indices = index_tensor_rank > 0 ? index_shape[0] : 1;
  int32_t inner_size = 1;
  for (int32_t i = 0; i < num_indices; ++i) {
    inner_size *= input_shape[index_values[i]];
  }
  return inner_size;
}

template <typename T>
void ConcatV2Args2(int32_t arg0_rank,
                   const int32_t* __restrict arg0_shape,
                   const T* __restrict arg0_values,
                   int32_t arg1_rank,
                   const int32_t* __restrict arg1_shape,
                   const T* __restrict arg1_values,
                   const int32_t* __restrict axis_value,
                   T* __restrict output_values) {
  BENCHMARK_TIMER("ConcatV2Args2");
  const int axis = axis_value[0];
  const int num_lines = ShapeSize(axis, arg0_shape);
  const int arg0_line_size = ShapeSize(arg0_rank - axis, arg0_shape + axis);
  const int arg1_line_size = ShapeSize(arg1_rank - axis, arg1_shape + axis);
  for (int line = 0; line < num_lines; ++line) {
    std::copy(arg0_values, arg0_values + arg0_line_size, output_values);
    arg0_values += arg0_line_size;
    output_values += arg0_line_size;
    std::copy(arg1_values, arg1_values + arg1_line_size, output_values);
    arg1_values += arg1_line_size;
    output_values += arg1_line_size;
  }
}

template <typename T>
void Conv2DAsGemm(const int32_t* __restrict in_shape,
                  const T* __restrict in_values,
                  const int32_t* __restrict filter_shape,
                  const T* __restrict filter_values,
                  T* __restrict output_values) {
  BENCHMARK_TIMER("Conv2DAsGemm");
#if USE_EIGEN
  const auto in = ConstMatrixMap<T>(in_values, in_shape[0], in_shape[1]);
  const auto filter =
      ConstMatrixMap<T>(filter_values, filter_shape[3],
                        filter_shape[0] * filter_shape[1] * filter_shape[2]);
  auto result = MatrixMap<T>(output_values, filter_shape[3], in_shape[1]);
  result.noalias() = filter * in;
#else
  const int32_t out_rows = in_shape[1];
  const int32_t out_cols = filter_shape[3];
  const int32_t dot_len = in_shape[0];
  for (int row = 0; row < out_rows; ++row) {
    for (int col = 0; col < out_cols; ++col) {
      T value = 0;
      for (int i = 0; i < dot_len; ++i) {
        value +=
            in_values[row * dot_len + i] * filter_values[i * out_cols + col];
      }
      *output_values++ = value;
    }
  }
#endif
}

template <typename T>
void DepthwiseConv2dNative(const int32_t* __restrict input_shape,
                           const T* __restrict input_values,
                           const int32_t* __restrict kernel_shape,
                           const T* __restrict kernel_values,
                           int32_t stride_y,
                           int32_t stride_x,
                           int32_t out_height,
                           int32_t out_width,
                           T* __restrict output_values) {
  BENCHMARK_TIMER("DepthwiseConv2dNative");
  // Give the shape values nicer names.
  assert(input_shape[3] == kernel_shape[2]);
  const int batch_size = input_shape[0];
  const int kernel_height = kernel_shape[0];
  const int kernel_width = kernel_shape[1];
  const int in_depth = kernel_shape[2];
  const int depth_mul = kernel_shape[3];
  const int in_height = input_shape[1];
  const int in_width = input_shape[2];

  // Compute the amount of padding needed to get the desired output size.
  const int pad_height =
      ((out_height - 1) * stride_y + kernel_height - in_height) / 2;
  const int pad_width =
      ((out_width - 1) * stride_x + kernel_width - in_width) / 2;

  // Cache the strides for address computations.
  const int in_strides[4] = {
      input_shape[1] * input_shape[2] * input_shape[3],  // batch
      input_shape[2] * input_shape[3],                   // y
      input_shape[3],                                    // x
      1,                                                 // channel
  };
  const int kernel_strides[4] = {
      kernel_shape[1] * kernel_shape[2] * kernel_shape[3],  // y
      kernel_shape[2] * kernel_shape[3],                    // x
      kernel_shape[3],                                      // in channels
      1,                                                    // channel mult
  };

  T* out_write_ptr = output_values;
  for (int batch = 0; batch < batch_size; ++batch) {
    for (int out_y = 0; out_y < out_height; ++out_y) {
      for (int out_x = 0; out_x < out_width; ++out_x) {
        // Compute the input read offsets.
        const int in_y_origin = (out_y * stride_y) - pad_height;
        const int in_x_origin = (out_x * stride_x) - pad_width;

        // Compute the range of the kernel to be applied (we may need to clip
        // when we'd read outside of the valid input region - for SAME).
        const int kernel_y_start = std::max(0, -in_y_origin);
        const int kernel_y_end =
            std::min(kernel_height, in_height - in_y_origin);
        const int kernel_x_start = std::max(0, -in_x_origin);
        const int kernel_x_end = std::min(kernel_width, in_width - in_x_origin);

        for (int in_c = 0; in_c < in_depth; ++in_c) {
          for (int mul_c = 0; mul_c < depth_mul; ++mul_c, ++out_write_ptr) {
            // Convolve.
            T sum = 0;
            for (int k_y = kernel_y_start; k_y < kernel_y_end; ++k_y) {
              const int in_y = in_y_origin + k_y;
              assert(in_y >= 0 && in_y < in_height);
              for (int k_x = kernel_x_start; k_x < kernel_x_end; ++k_x) {
                const int in_x = in_x_origin + k_x;
                assert(in_x >= 0 && in_x < in_width);
                const T input_value =
                    input_values[batch * in_strides[0] +  // batch
                                 in_y * in_strides[1] +   // y
                                 in_x * in_strides[2] +   // x
                                 in_c];                   // in chan
                const T kernel_value =
                    kernel_values[k_y * kernel_strides[0] +   // y
                                  k_x * kernel_strides[1] +   // x
                                  in_c * kernel_strides[2] +  // in chan
                                  mul_c];                     // chan mult
                sum += input_value * kernel_value;
              }
            }
            *out_write_ptr = sum;
          }  // mul_c
        }    // in_c
      }      // out_x
    }        // out_y
  }          // batch
}

template <typename T>
void FullyConnected(const int32_t* __restrict input_shape,
                    const T* __restrict input_values,
                    const int32_t* __restrict weight_shape,
                    const T* __restrict weight_values,
                    const int32_t* __restrict bias_shape,
                    const T* __restrict bias_values,
                    T* __restrict output_values) {
  BENCHMARK_TIMER("FullyConnected");
#if USE_EIGEN
  const auto in =
      ConstMatrixMap<T>(input_values, input_shape[1], input_shape[0]);
  const auto weight =
      ConstMatrixMap<T>(weight_values, weight_shape[1], weight_shape[0]);
  const auto bias = ConstRowVectorMap<T>(bias_values, bias_shape[0]);
  auto result = MatrixMap<T>(output_values, weight_shape[1], input_shape[0]);
  result.noalias() = (weight * in).colwise() + bias;
#else
  const int batch_size = input_shape[0];
  const int num_inputs = weight_shape[0];
  const int num_outputs = weight_shape[1];
  assert(input_shape[1] == num_inputs);
  assert(bias_shape[0] == num_outputs);
  for (int batch = 0; batch < batch_size; ++batch) {
    for (int out_i = 0; out_i < num_outputs; ++out_i) {
      T value = 0;
      for (int in_i = 0; in_i < num_inputs; ++in_i) {
        value += input_values[batch * num_inputs + in_i] *
                 weight_values[in_i * num_outputs + out_i];
      }
      value += bias_values[out_i];
      output_values[batch * num_outputs + out_i] = value;
    }
  }
#endif
}

template <typename T, typename TIndex>
void Gather(int params_rank,
            const int32_t* __restrict params_shape,
            const T* __restrict params_values,
            int indices_rank,
            const int32_t* __restrict indices_shape,
            const TIndex* __restrict indices_values,
            T* __restrict output_values) {
  BENCHMARK_TIMER("Gather");
  const int num_indices = ShapeSize(indices_rank, indices_shape);
  const int num_params = params_shape[0];
  const int slice_size = ShapeSize(params_rank - 1, params_shape + 1);
  for (int i = 0; i < num_indices; ++i) {
    const int index = indices_values[i];
    if (index < 0 || index >= num_params) {
      std::fill(output_values, output_values + slice_size, 0);
    } else {
      std::copy(params_values + index * slice_size,
                params_values + index * slice_size + slice_size, output_values);
    }
    output_values += slice_size;
  }
}

template <typename T, typename TIndex>
void Im2Col(const int32_t* __restrict input_shape,
            const T* __restrict input_values,
            const int32_t* __restrict kernel_shape,
            int32_t stride_y,
            int32_t stride_x,
            int32_t out_height,
            int32_t out_width,
            TIndex* output_shape,
            T* __restrict output_values) {
  BENCHMARK_TIMER("Im2Col");
  // Give the shape values nicer names.
  assert(input_shape[3] == kernel_shape[2]);
  const int batch_size = input_shape[0];
  const int kernel_height = kernel_shape[0];
  const int kernel_width = kernel_shape[1];
  const int in_depth = kernel_shape[2];
  const int in_height = input_shape[1];
  const int in_width = input_shape[2];

  // Compute the amount of padding needed to get the desired output size.
  const int pad_height =
      ((out_height - 1) * stride_y + kernel_height - in_height) / 2;
  const int pad_width =
      ((out_width - 1) * stride_x + kernel_width - in_width) / 2;

  // Cache the strides for address computations.
  const int x_stride = input_shape[3];
  const int y_stride = input_shape[2] * x_stride;
  const int batch_stride = input_shape[1] * y_stride;

  // Write the output shape.
  output_shape[0] = kernel_height * kernel_width * in_depth;
  output_shape[1] = input_shape[0] * out_width * out_height;

  for (int batch = 0; batch < batch_size; ++batch) {
    for (int out_y = 0; out_y < out_height; ++out_y) {
      for (int out_x = 0; out_x < out_width; ++out_x) {
        // Compute the input read offsets.
        const int in_y_origin = (out_y * stride_y) - pad_height;
        const int in_x_origin = (out_x * stride_x) - pad_width;

        // Compute the range of the kernel to be applied (we may need to clip
        // when we'd read outside of the valid input region - for SAME).
        const int kernel_y_start = std::max(0, -in_y_origin);
        const int kernel_y_end =
            std::min(kernel_height, in_height - in_y_origin);
        const int kernel_x_start = std::max(0, -in_x_origin);
        const int kernel_x_end = std::min(kernel_width, in_width - in_x_origin);

        // Padding top.
        if (kernel_y_start != 0) {
          const int num_lines = kernel_y_start;
          const int num_coeffs = num_lines * kernel_width * in_depth;
#if USE_TYPED_MEMSETMEMCPY
          std::fill(output_values, output_values + num_coeffs, 0);
#else
          std::memset(output_values, 0, num_coeffs * sizeof(T));
#endif
          output_values += num_coeffs;
        }
        for (int k_y = kernel_y_start; k_y < kernel_y_end; ++k_y) {
          // Padding left.
          if (kernel_x_start != 0) {
            const int num_coeffs = kernel_x_start * in_depth;
#if USE_TYPED_MEMSETMEMCPY
            std::fill(output_values, output_values + num_coeffs, 0);
#else
            std::memset(output_values, 0, num_coeffs * sizeof(T));
#endif
            output_values += num_coeffs;
          }
          // Valid values.
          {
            const int in_y = in_y_origin + k_y;
            const int in_x = in_x_origin + kernel_x_start;
            const int num_coeffs = (kernel_x_end - kernel_x_start) * in_depth;
#if USE_TYPED_MEMSETMEMCPY
            const int offset =
                batch * batch_stride + in_y * y_stride + in_x * x_stride;
            std::copy(input_values + offset, input_values + offset + num_coeffs,
                      output_values);
#else
            std::memcpy(output_values,
                        input_values  // Reusing the restricted pointer.
                            + batch * batch_stride  // batch
                            + in_y * y_stride       // y
                            + in_x * x_stride,      // x
                        num_coeffs * sizeof(T));
#endif
            output_values += num_coeffs;
          }
          // Padding right.
          if (kernel_x_end != kernel_width) {
            const int num_coeffs = (kernel_width - kernel_x_end) * in_depth;
#if USE_TYPED_MEMSETMEMCPY
            std::fill(output_values, output_values + num_coeffs, 0);
#else
            std::memset(output_values, 0, num_coeffs * sizeof(T));
#endif
            output_values += num_coeffs;
          }
        }
        // Padding bottom.
        if (kernel_y_end != kernel_height) {
          const int num_lines = kernel_height - kernel_y_end;
          const int num_coeffs = num_lines * kernel_width * in_depth;
#if USE_TYPED_MEMSETMEMCPY
          std::fill(output_values, output_values + num_coeffs, 0);
#else
          std::memset(output_values, 0, num_coeffs * sizeof(T));
#endif
          output_values += num_coeffs;
        }
      }
    }
  }
}

template <typename T>
void MaxPool(const int32_t* __restrict input_shape,
             const T* __restrict input_values,
             int32_t stride_y,
             int32_t stride_x,
             int32_t kernel_height,
             int32_t kernel_width,
             int32_t out_height,
             int32_t out_width,
             T* __restrict output_values) {
  BENCHMARK_TIMER("MaxPool");
  // Give the shape values nicer names.
  const int batch_size = input_shape[0];
  const int in_height = input_shape[1];
  const int in_width = input_shape[2];
  const int depth = input_shape[3];

  // Compute the amount of padding needed to get the desired output size.
  const int pad_height =
      ((out_height - 1) * stride_y + kernel_height - in_height) / 2;
  const int pad_width =
      ((out_width - 1) * stride_x + kernel_width - in_width) / 2;

  // Cache the strides for address computations.
  const int in_strides[4] = {
      input_shape[1] * input_shape[2] * input_shape[3],  // batch
      input_shape[2] * input_shape[3],                   // y
      input_shape[3],                                    // x
      1,                                                 // channel
  };

  T* out_write_ptr = output_values;
  for (int batch = 0; batch < batch_size; ++batch) {
    for (int out_y = 0; out_y < out_height; ++out_y) {
      for (int out_x = 0; out_x < out_width; ++out_x) {
        // Compute the input read offsets.
        const int in_y_origin = (out_y * stride_y) - pad_height;
        const int in_x_origin = (out_x * stride_x) - pad_width;

        // Compute the range of the kernel to be applied (we may need to clip
        // when we'd read outside of the valid input region - for SAME).
        const int kernel_y_start = std::max(0, -in_y_origin);
        const int kernel_y_end =
            std::min(kernel_height, in_height - in_y_origin);
        const int kernel_x_start = std::max(0, -in_x_origin);
        const int kernel_x_end = std::min(kernel_width, in_width - in_x_origin);

        for (int chan = 0; chan < depth; ++chan, ++out_write_ptr) {
          // Convolve.
          T max_value = std::numeric_limits<T>::lowest();
          for (int k_y = kernel_y_start; k_y < kernel_y_end; ++k_y) {
            const int in_y = in_y_origin + k_y;
            assert(in_y >= 0 && in_y < in_height);
            for (int k_x = kernel_x_start; k_x < kernel_x_end; ++k_x) {
              const int in_x = in_x_origin + k_x;
              assert(in_x >= 0 && in_x < in_width);
              const T input_value =
                  input_values[batch * in_strides[0] +  // batch
                               in_y * in_strides[1] +   // y
                               in_x * in_strides[2] +   // x
                               chan];                   // channel
              max_value = std::max(max_value, input_value);
            }  // kernel_x
          }    // kernel_y
          *out_write_ptr = max_value;
        }  // chan
      }    // out_x
    }      // out_y
  }        // batch
}

template <typename T>
void Memcpy(const int32_t rank,
            const int32_t* __restrict input_shape,
            const T* __restrict input_values,
            T* __restrict output_values) {
  BENCHMARK_TIMER("Memcpy");
  const int size = ShapeSize(rank, input_shape);
  for (int i = 0; i < size; ++i) {
    output_values[i] = input_values[i];
  }
}

template <typename T>
void Softmax(const int32_t rank,
             const int32_t* __restrict input_shape,
             const T* __restrict input_values,
             const int32_t reduce_dim,
             T* __restrict output_values,
             T* __restrict scratch_values) {
  BENCHMARK_TIMER("Softmax");
  const int size = ShapeSize(rank, input_shape);
  if (rank == 2 && reduce_dim == 1) {
    T logits_max = std::numeric_limits<T>::lowest();

    // Max.
    for (int i = 0; i < size; ++i) {
      logits_max = std::max(logits_max, input_values[i]);
    }

    // Pre-compute exp.
    for (int i = 0; i < size; ++i) {
      scratch_values[i] = std::exp(input_values[i] - logits_max);
    }

    // Sum over the last dimension, then divide the exps and write out.
    for (int offset = 0; offset < size; offset += input_shape[1]) {
      T sum = 0;
      const int end_offset = offset + input_shape[1];
      for (int i = offset; i < end_offset; ++i)
        sum += scratch_values[i];
      const T rcp_denom = static_cast<T>(1) / sum;
      for (int i = 0; i < input_shape[1]; ++i) {
        output_values[offset + i] = scratch_values[offset + i] * rcp_denom;
      }
    }
  } else {
    assert(false && "Generic Softmax not yet supported.");
  }
}

// Returns the start position for a slice in a single dimension.
template <typename T>
int StridedSliceBegin(int range_mask,
                      const T* __restrict range_values,
                      const T* __restrict strides,
                      const int32_t* __restrict input_shape,
                      int dim) {
  const bool is_explicit = 0 == (range_mask & (1 << dim));
  if (is_explicit) {
    return range_values[dim];
  } else {
    const bool is_reverse = strides[dim] < 0;
    return is_reverse ? input_shape[dim] - 1 : 0;
  }
}

// Returns the end position for a slice in a single dimension.
template <typename T>
int StridedSliceEnd(int range_mask,
                    const T* __restrict range_values,
                    const T* __restrict strides,
                    const int32_t* __restrict input_shape,
                    int dim) {
  const bool is_explicit = 0 == (range_mask & (1 << dim));
  if (is_explicit) {
    return range_values[dim];
  } else {
    const bool is_reverse = strides[dim] < 0;
    return is_reverse ? -1 : input_shape[dim];
  }
}

template <typename T, typename TIdx>
void StridedSlice(const int32_t input_rank,
                  const int32_t* __restrict input_shape,
                  const T* __restrict input_values,
                  const TIdx* __restrict begin,
                  const TIdx* __restrict end,
                  const TIdx* __restrict strides,
                  int32_t begin_mask,
                  int32_t end_mask,
                  T* __restrict output_values) {
  BENCHMARK_TIMER("StridedSlice");
  const int MAX_RANK = 8;
  assert(input_rank < MAX_RANK);

  // Compute the address strides for each dimension.
  int dim_addr_strides[MAX_RANK] = {0};
  dim_addr_strides[input_rank - 1] = 1;
  for (int dim = input_rank - 2; dim >= 0; --dim) {
    dim_addr_strides[dim] = dim_addr_strides[dim + 1] * input_shape[dim + 1];
  }

  // Resolve the masks and get explicit ranges for each dimension.
  int dim_begin[MAX_RANK];
  int dim_end[MAX_RANK];
  bool dim_is_full_range[MAX_RANK];
  for (int dim = 0; dim < input_rank; ++dim) {
    const int stride = strides[dim];
    dim_begin[dim] =
        StridedSliceBegin(begin_mask, begin, strides, input_shape, dim);
    dim_end[dim] = StridedSliceEnd(end_mask, end, strides, input_shape, dim);
    dim_is_full_range[dim] =
        dim_begin[dim] == 0 && dim_end[dim] == input_shape[dim] && stride == 1;

    // Our termination criteria for loops is that we hit the end exactly, so
    // we need to ensure that we don't step over the end with stride != 1.
    const int length_mod = (dim_end[dim] - dim_begin[dim]) % stride;
    if (length_mod != 0) {
      dim_end[dim] += stride - length_mod;
    }
  }

  // Find out how large the blocks are that we can copy contiguously. (All
  // dimensions on the right for which we fetch the full range)
  int last_sliced_dim = input_rank - 1;
  int block_size = 1;
  for (int dim = input_rank - 1; dim >= 0 && dim_is_full_range[dim]; --dim) {
    block_size *= input_shape[dim];
    last_sliced_dim--;
  }

  // Initialize the read pos for each dimension according to the begin offsets.
  int read_pos[MAX_RANK] = {0};
  for (int dim = 0; dim < input_rank; ++dim) {
    read_pos[dim] = dim_begin[dim];
  }

  while (read_pos[0] != dim_end[0]) {
    // Compute the read offset for the current position.
    int32_t read_offset = 0;
    for (int dim = 0; dim <= last_sliced_dim; ++dim) {
      const int addr_stride = dim_addr_strides[dim];
      if (read_pos[dim] < 0) {
        read_offset += (input_shape[dim] + read_pos[dim]) * addr_stride;
      } else {
        read_offset += read_pos[dim] * addr_stride;
      }
    }

#if USE_TYPED_MEMSETMEMCPY
    std::copy(input_values + read_offset,
              input_values + read_offset + block_size, output_values);
#else
    std::memcpy(output_values, input_values + read_offset,
                block_size * sizeof(T));
#endif
    output_values += block_size;

    // Advance the read position.
    for (int dim = last_sliced_dim; dim >= 0; --dim) {
      read_pos[dim] += strides[dim];
      if (dim == 0 || read_pos[dim] != dim_end[dim])
        break;
      read_pos[dim] = dim_begin[dim];
    }
  }
}

template <typename T>
void TransposeRank3(const int32_t* __restrict input_shape,
                    const T* __restrict input_values,
                    const int32_t* __restrict perm,
                    T* __restrict output_values) {
  BENCHMARK_TIMER("TransposeRank3");
  const int32_t in_strides[3] = {
      input_shape[1] * input_shape[2],
      input_shape[2],
      1,
  };
  const int32_t out_strides[3] = {in_strides[perm[0]], in_strides[perm[1]],
                                  in_strides[perm[2]]};
  const int32_t out_shape[3] = {input_shape[perm[0]], input_shape[perm[1]],
                                input_shape[perm[2]]};

  int32_t write_offset = 0;
  for (int32_t it0 = 0; it0 < out_shape[0]; ++it0) {
    const int32_t read_offset0 = it0 * out_strides[0];
    for (int32_t it1 = 0; it1 < out_shape[1]; ++it1) {
      const int32_t read_offset01 = read_offset0 + it1 * out_strides[1];
      for (int32_t it2 = 0; it2 < out_shape[2]; ++it2, ++write_offset) {
        const int32_t read_offset = read_offset01 + it2 * out_strides[2];
        output_values[write_offset] = input_values[read_offset];
      }
    }
  }
}

template <typename T>
void TransposeRank4(const int32_t* __restrict input_shape,
                    const T* __restrict input_values,
                    const int32_t* __restrict perm,
                    T* __restrict output_values) {
  BENCHMARK_TIMER("TransposeRank4");
  const int32_t in_strides[4] = {
      input_shape[1] * input_shape[2] * input_shape[3],
      input_shape[2] * input_shape[3],
      input_shape[3],
      1,
  };
  const int32_t out_strides[4] = {in_strides[perm[0]], in_strides[perm[1]],
                                  in_strides[perm[2]], in_strides[perm[3]]};
  const int32_t out_shape[4] = {input_shape[perm[0]], input_shape[perm[1]],
                                input_shape[perm[2]], input_shape[perm[3]]};

  int32_t write_offset = 0;
  for (int32_t it0 = 0; it0 < out_shape[0]; ++it0) {
    const int32_t read_offset0 = it0 * out_strides[0];
    for (int32_t it1 = 0; it1 < out_shape[1]; ++it1) {
      const int32_t read_offset01 = read_offset0 + it1 * out_strides[1];
      for (int32_t it2 = 0; it2 < out_shape[2]; ++it2) {
        const int32_t read_offset012 = read_offset01 + it2 * out_strides[2];
        for (int32_t it3 = 0; it3 < out_shape[3]; ++it3, ++write_offset) {
          const int32_t read_offset = read_offset012 + it3 * out_strides[3];
          output_values[write_offset] = input_values[read_offset];
        }
      }
    }
  }
}

template <typename T, typename TIdx, typename TDepth>
void OneHot(const int32_t input_rank,
            const int32_t* __restrict input_shape,
            const TIdx* __restrict input_values,
            const TDepth* __restrict depth,
            const T* __restrict on_value,
            const T* __restrict off_value,
            const int32_t axis,
            T* __restrict output_values) {
  BENCHMARK_TIMER("OneHot");
  const int32_t num_elements = ShapeSize(input_rank, input_shape);
  // We can assume axis >= 0 in this implementation.
  const int32_t prefix_dim_size = ShapeSize(axis, input_shape);
  const int32_t suffix_dim_size = num_elements / prefix_dim_size;
  int32_t write_offset = 0;
  for (int32_t i = 0; i < prefix_dim_size; i++) {
    int32_t read_offset_pre = i * suffix_dim_size;
    for (TDepth d = 0; d < *depth; d++) {
      for (int32_t j = 0; j < suffix_dim_size; j++, write_offset++) {
        const int32_t read_offset = read_offset_pre + j;
        output_values[write_offset] =
            (input_values[read_offset] == d) ? *on_value : *off_value;
      }
    }
  }
}

template <typename T, typename TIdx, typename TDepth>
void OneHotLastDim(const int32_t input_rank,
                   const int32_t* __restrict input_shape,
                   const TIdx* __restrict input_values,
                   const TDepth* __restrict depth,
                   const T* __restrict on_value,
                   const T* __restrict off_value,
                   T* __restrict output_values) {
  BENCHMARK_TIMER("OneHotLastDim");
  const int32_t num_elements = ShapeSize(input_rank, input_shape);
  int32_t write_offset = 0;
  for (int32_t i = 0; i < num_elements; i++) {
    for (TDepth d = 0; d < *depth; d++, write_offset++) {
      output_values[write_offset] =
          (input_values[i] == d) ? *on_value : *off_value;
    }
  }
}

// -----------------------------------------------------------------------------
// Simple unary ops
// -----------------------------------------------------------------------------

// We use macros instead of template functions with templated functors here
// because it's a lot less verbose and easier for the compiler to optimize.

#if USE_EIGEN

#define SIMPLE_UNARY_OP(OP_NAME, _, EXPR_EIGEN)                           \
  template <typename T>                                                   \
  void OP_NAME(const int32_t rank, const int32_t* __restrict input_shape, \
               const T* __restrict input_values,                          \
               T* __restrict output_values) {                             \
    BENCHMARK_TIMER(#OP_NAME);                                            \
    const int size = ShapeSize(rank, input_shape);                        \
    auto values = ConstRowVectorMap<T>(input_values, size).array();       \
    auto output = RowVectorMap<T>(output_values, size).array();           \
    output = EXPR_EIGEN;                                                  \
  }

#else

#define SIMPLE_UNARY_OP(OP_NAME, EXPR, _)                                 \
  template <typename T>                                                   \
  void OP_NAME(const int32_t rank, const int32_t* __restrict input_shape, \
               const T* __restrict input_values,                          \
               T* __restrict output_values) {                             \
    BENCHMARK_TIMER(#OP_NAME);                                            \
    const int size = ShapeSize(rank, input_shape);                        \
    for (int i = 0; i < size; ++i) {                                      \
      const T value = input_values[i];                                    \
      output_values[i] = EXPR;                                            \
    }                                                                     \
  }

#endif

// Second macro param is value expression, third entry is Eigen vector
// expression.
SIMPLE_UNARY_OP(Abs, std::abs(value), values.abs())
SIMPLE_UNARY_OP(Acos, std::acos(value), values.acos())
SIMPLE_UNARY_OP(Asin, std::asin(value), values.asin())
SIMPLE_UNARY_OP(Atan, std::atan(value), values.atan())
SIMPLE_UNARY_OP(Cos, std::cos(value), values.cos())
SIMPLE_UNARY_OP(Cosh, std::cosh(value), values.cosh())
SIMPLE_UNARY_OP(Exp, std::exp(value), values.exp())
SIMPLE_UNARY_OP(Elu,
                value < 0 ? std::expm1(value) : value,
                // Use branchless version of Elu: min(ReLU, e^x - 1)
                values.max(0).min(values.exp() - 1))
SIMPLE_UNARY_OP(Log, std::log(value), values.log())
SIMPLE_UNARY_OP(Log1p, std::log1p(value), values.log1p())
SIMPLE_UNARY_OP(Neg, -value, -values)
SIMPLE_UNARY_OP(Reciprocal, static_cast<T>(1) / value, values.cwiseInverse())
SIMPLE_UNARY_OP(Relu, std::max(value, static_cast<T>(0)), values.max(0))
SIMPLE_UNARY_OP(Relu6,
                std::min(std::max(value, static_cast<T>(0)), static_cast<T>(6)),
                values.max(0).min(6))
SIMPLE_UNARY_OP(Rsqrt, static_cast<T>(1) / std::sqrt(value), values.rsqrt())
SIMPLE_UNARY_OP(Sigmoid,
                static_cast<T>(1) / (1 + std::exp(-value)),
                ((-values).exp() + 1).cwiseInverse())
SIMPLE_UNARY_OP(Sin, std::sin(value), values.sin())
SIMPLE_UN
```