Response:
Let's break down the thought process for analyzing the provided Python script `deps.py`.

**1. Initial Understanding: What is the script's purpose?**

The script name `deps.py` and the context "fridaDynamic instrumentation tool" immediately suggest it deals with dependencies. The presence of functions like `sync`, `roll`, and `build` reinforces this idea. It seems responsible for managing pre-built dependencies required by Frida.

**2. Deconstructing the Code: Identifying Key Functions and Concepts**

I'll go through the code section by section, noting important functions, data structures, and modules used.

* **Imports:**  Standard Python libraries like `argparse`, `os`, `pathlib`, `subprocess`, `urllib`, and more specialized ones like `tomlkit` and custom modules (`releng`). These imports hint at the script's functionality (command-line parsing, file system operations, external process execution, network operations, and configuration management).
* **`main()` function:** This is the entry point. It uses `argparse` to define command-line arguments and subcommands: `sync`, `roll`, `build`, `wait`, `bump`. This confirms the script is designed to be run from the command line with different actions.
* **`sync()`:** This function handles downloading and extracting pre-built dependency bundles. Keywords like "downloading", "extracting", and handling local files are important. The mention of "VERSION.txt" suggests versioning is managed.
* **`roll()`:** This function seems to automate the process of building and uploading dependencies. It checks for the existence of bundles on a remote storage (S3) and triggers a build if necessary. The use of `aws` CLI commands is significant.
* **`build()`:** This is the core function for compiling dependencies. It utilizes a `Builder` class.
* **`Builder` class:**  This class encapsulates the complex build logic. Key aspects are:
    * Handling different `Bundle` types (SDK, Toolchain).
    * Supporting cross-compilation (build machine vs. host machine).
    * Using Meson as the build system.
    * Managing source code (cloning Git repositories).
    * Packaging the built artifacts.
    * The concept of "manifest" files to track installed components.
* **Helper Functions:** Functions like `parse_bundle_option_value`, `parse_set_option_value`, `query_toolchain_prefix`, `ensure_toolchain`, `detect_cache_dir`, `compute_bundle_parameters` provide supporting functionality for parsing arguments, locating resources, and generating URLs.
* **`Bundle` Enum:** Defines the different types of dependency bundles (SDK, Toolchain).
* **`MachineSpec` Class:**  Represents the operating system and architecture. This is crucial for handling platform-specific dependencies.
* **`SourceState` Enum:** Tracks whether the local copy of a dependency is pristine or modified.
* **Configuration:** The script interacts with TOML files (through `tomlkit`) for dependency parameters.
* **Environment Variables:** The script uses `FRIDA_DEPS` to determine the cache directory.

**3. Relating to Reverse Engineering, Binary Analysis, and System Knowledge**

Now, I connect the identified functionalities to the requested areas:

* **Reverse Engineering:**
    * Frida is a reverse engineering tool, and this script manages *its* dependencies. Thus, it indirectly supports reverse engineering by ensuring Frida has the necessary components.
    * The `sync` and `roll` commands ensure up-to-date dependency versions, which might be crucial for specific reverse engineering tasks that rely on certain Frida features.
    * The `build` command allows building specific components, which a developer or advanced user might need for custom Frida setups or debugging.
* **Binary Analysis:**
    * The script deals with pre-built binaries (downloading them) and the process of building them. Understanding the build process can be valuable in binary analysis, especially when encountering issues or needing to understand how a particular component is compiled.
    * The toolchain dependencies managed by this script include compilers and linkers, which are fundamental to understanding how binaries are created.
* **Linux/Android Kernel and Framework:**
    * The script explicitly handles different operating systems (Windows, Linux, macOS) and architectures.
    * The "SDK" bundle likely includes libraries and headers needed to interact with the target system, including potentially Android frameworks.
    * The toolchain includes compilers and other tools necessary to build software for these platforms.
    * The script's logic for handling machine specifications (`MachineSpec`) and conditional compilation based on the target OS and architecture directly relates to kernel and framework differences.

**4. Logical Reasoning (Hypothetical Input/Output)**

I consider a few scenarios:

* **`sync sdk linux/x86_64 ./my_deps`:**  The script would download the SDK bundle for Linux 64-bit, extract it to the `./my_deps` directory, and potentially update a "VERSION.txt" file. Output would be progress messages.
* **`build --bundle toolchain --build windows/x86_64 --host linux/x86_64`:**  This would initiate a cross-compilation build of the toolchain for Windows 64-bit, running on a Linux 64-bit host. Output would be detailed build logs from Meson and Git.

**5. User/Programming Errors**

I think about common mistakes:

* **Incorrect Arguments:**  Providing an invalid bundle name (`sync invalid_bundle ...`). The `parse_bundle_option_value` function explicitly handles this with an `argparse.ArgumentTypeError`.
* **Missing Dependencies:** If the `aws` CLI is not installed when running `roll`, the script would fail.
* **Network Issues:**  If there are network problems during downloads, the script would raise exceptions.
* **Incorrect Environment:**  If `FRIDA_DEPS` points to a non-existent or incorrect directory.

**6. User Operation to Reach the Script**

I imagine the steps:

1. A user needs to build Frida from source or ensure its dependencies are up-to-date.
2. They navigate to the Frida source directory (`frida/`).
3. They then go to the `frida-gum/releng/` directory.
4. They execute the `deps.py` script from the command line with specific arguments (e.g., `python deps.py sync sdk linux/x86_64 deps`).

**7. Summarizing Functionality (Part 1)**

Based on the analysis so far, I would summarize the functionality as:

> The `deps.py` script is a command-line tool for managing pre-built dependencies required by the Frida dynamic instrumentation framework. It provides functionalities to:
>
> * **Synchronize (sync):** Download and extract pre-built dependency bundles for a specified target operating system and architecture. It manages versions and handles local copies.
> * **Roll (roll):**  Automate the process of building and uploading dependency bundles to a remote storage (likely S3). It checks if a bundle needs to be built before triggering the build process.
> * **Build (build):**  Compile dependency packages for specified build and host machines using the Meson build system. It supports building specific packages or excluding others.
> * **Wait (wait):**  (Although not fully analyzed yet, the name suggests it waits for dependencies to become available).
> * **Bump (bump):** (Likely for updating dependency versions).
>
> The script uses configuration files (TOML), environment variables, and interacts with external tools like Git and the AWS CLI. It supports cross-compilation and handles platform-specific build requirements.

This detailed breakdown and thought process allows for a comprehensive understanding of the script's purpose and its connections to the broader context of Frida and system-level software development.
å¥½çš„ï¼Œè®©æˆ‘ä»¬å¯¹ `frida/subprojects/frida-gum/releng/deps.py` æ–‡ä»¶çš„ç¬¬ä¸€éƒ¨åˆ†ä»£ç è¿›è¡ŒåŠŸèƒ½å½’çº³å’Œåˆ†æã€‚

**åŠŸèƒ½å½’çº³ï¼š**

`deps.py` è„šæœ¬çš„ä¸»è¦åŠŸèƒ½æ˜¯ç®¡ç† Frida å·¥å…·çš„é¢„æ„å»ºä¾èµ–é¡¹ã€‚å®ƒæä¾›äº†ä¸€ç»„å‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºï¼š

1. **åŒæ­¥ (sync):**  ä¸‹è½½å¹¶æå–æŒ‡å®šæ“ä½œç³»ç»Ÿå’Œæ¶æ„çš„é¢„æ„å»ºä¾èµ–é¡¹åŒ…åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿã€‚
2. **æ„å»º (build):**  ä»æºä»£ç æ„å»ºé¢„æ„å»ºçš„ä¾èµ–é¡¹åŒ…ã€‚è¿™å…è®¸åœ¨æ²¡æœ‰é¢„æ„å»ºåŒ…å¯ç”¨æ—¶æˆ–è€…éœ€è¦è‡ªå®šä¹‰æ„å»ºæ—¶ä½¿ç”¨ã€‚
3. **æ»šåŠ¨ (roll):**  è‡ªåŠ¨åŒ–æ„å»ºå’Œä¸Šä¼ é¢„æ„å»ºä¾èµ–é¡¹çš„è¿‡ç¨‹ã€‚å®ƒä¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ„å»ºï¼Œå¦‚æœéœ€è¦åˆ™è¿›è¡Œæ„å»ºï¼Œç„¶åä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆS3ï¼‰ï¼Œå¹¶æ¸…é™¤ CDN ç¼“å­˜ã€‚
4. **ç­‰å¾… (wait):**  ç­‰å¾…æŒ‡å®šæ“ä½œç³»ç»Ÿå’Œæ¶æ„çš„é¢„æ„å»ºä¾èµ–é¡¹å¯ç”¨ã€‚
5. **æ›´æ–° (bump):**  æ›´æ–°ä¾èµ–é¡¹çš„ç‰ˆæœ¬ä¿¡æ¯ï¼ˆå…·ä½“å®ç°æœªåœ¨æä¾›çš„ä»£ç ä¸­ä½“ç°ï¼‰ã€‚

**è¯¦ç»†åŠŸèƒ½åˆ†æä¸ä¸¾ä¾‹è¯´æ˜ï¼š**

**1. é€†å‘æ–¹æ³•çš„å…³ç³»ï¼š**

* **åŠŸèƒ½ï¼šåŒæ­¥ (sync)**
    * **è¯´æ˜ï¼š** Frida æ˜¯ä¸€ä¸ªåŠ¨æ€æ’æ¡©å·¥å…·ï¼Œå¸¸ç”¨äºé€†å‘å·¥ç¨‹ã€‚`sync` å‘½ä»¤ç¡®ä¿ Frida è¿è¡Œæ—¶æ‰€éœ€çš„ä¾èµ–é¡¹ï¼ˆä¾‹å¦‚ï¼Œç‰¹å®šå¹³å°çš„åº“ã€å·¥å…·é“¾ç­‰ï¼‰æ˜¯æœ€æ–°çš„ã€‚
    * **ä¸¾ä¾‹ï¼š** å‡è®¾ä½ è¦åœ¨ Android è®¾å¤‡ä¸Šä½¿ç”¨ Frida è¿›è¡Œé€†å‘åˆ†æã€‚ä½ éœ€è¦åŒæ­¥ Android å¹³å°çš„ SDK ä¾èµ–é¡¹ã€‚ä½ å¯ä»¥è¿è¡Œç±»ä¼¼ `python deps.py sync sdk android/arm64 ./frida_deps` çš„å‘½ä»¤ï¼Œå°† Android ARM64 çš„ SDK ä¸‹è½½åˆ° `frida_deps` ç›®å½•ã€‚

* **åŠŸèƒ½ï¼šæ„å»º (build)**
    * **è¯´æ˜ï¼š** å½“æ²¡æœ‰é¢„æ„å»ºçš„ä¾èµ–é¡¹å¯ç”¨ï¼Œæˆ–è€…ä½ éœ€è¦ä¿®æ”¹æŸäº›ä¾èµ–é¡¹çš„æ„å»ºé€‰é¡¹æ—¶ï¼Œ`build` å‘½ä»¤å°±æ´¾ä¸Šç”¨åœºã€‚è¿™ä¸é€†å‘ä¸­è‡ªå®šä¹‰å·¥å…·æˆ–åº“çš„è¡Œä¸ºæœ‰ç›¸ä¼¼ä¹‹å¤„ã€‚
    * **ä¸¾ä¾‹ï¼š**  å¦‚æœä½ å‘ç°æŸä¸ª Frida çš„ä¾èµ–åº“å­˜åœ¨ bugï¼Œå¹¶ä¸”ä½ å·²ç»ä¿®æ”¹äº†å®ƒçš„æºä»£ç ã€‚ä½ å¯ä»¥ä½¿ç”¨ `build` å‘½ä»¤é’ˆå¯¹ä½ çš„ä¿®æ”¹é‡æ–°æ„å»ºè¿™ä¸ªä¾èµ–é¡¹ï¼Œä¾‹å¦‚ `python deps.py build --bundle sdk --build linux/x86_64 --host linux/x86_64 --only some_problematic_lib`ã€‚

* **åŠŸèƒ½ï¼šæ»šåŠ¨ (roll)**
    * **è¯´æ˜ï¼š**  å¯¹äº Frida çš„å¼€å‘è€…æˆ–ç»´æŠ¤è€…æ¥è¯´ï¼Œ`roll` å‘½ä»¤ç”¨äºå‘å¸ƒæ–°çš„ Frida ç‰ˆæœ¬æˆ–æ›´æ–°ä¾èµ–é¡¹ã€‚è¿™æ¶‰åŠåˆ°æ„å»ºã€æµ‹è¯•å’Œéƒ¨ç½²çš„è¿‡ç¨‹ï¼Œç±»ä¼¼äºå‘å¸ƒä¸€ä¸ªé€†å‘åˆ†æå·¥å…·çš„æ›´æ–°ã€‚
    * **ä¸¾ä¾‹ï¼š**  Frida çš„å¼€å‘è€…åœ¨æ›´æ–°äº†æŸä¸ªæ ¸å¿ƒä¾èµ–åº“åï¼Œå¯ä»¥ä½¿ç”¨ `roll` å‘½ä»¤æ„å»ºé’ˆå¯¹æ‰€æœ‰æ”¯æŒå¹³å°çš„ä¾èµ–åŒ…ï¼Œå¹¶å°†å®ƒä»¬ä¸Šä¼ åˆ°æœåŠ¡å™¨ä¾›ç”¨æˆ·ä¸‹è½½ã€‚

**2. äºŒè¿›åˆ¶åº•å±‚ã€Linuxã€Android å†…æ ¸åŠæ¡†æ¶çš„çŸ¥è¯†ï¼š**

* **æ“ä½œç³»ç»Ÿå’Œæ¶æ„ (MachineSpec):**
    * **è¯´æ˜ï¼š** è„šæœ¬å¤§é‡ä½¿ç”¨äº† `MachineSpec` ç±»æ¥è¡¨ç¤ºä¸åŒçš„æ“ä½œç³»ç»Ÿå’Œæ¶æ„ï¼ˆä¾‹å¦‚ `linux/x86_64`, `android/arm`ï¼‰ã€‚è¿™ç›´æ¥å…³ç³»åˆ°äºŒè¿›åˆ¶ç¨‹åºçš„å…¼å®¹æ€§å’Œè¿è¡Œç¯å¢ƒã€‚
    * **ä¸¾ä¾‹ï¼š**  åœ¨ `sync` æˆ– `build` å‘½ä»¤ä¸­ï¼Œä½ éœ€è¦æŒ‡å®šç›®æ ‡ `host` æœºå™¨çš„ `MachineSpec`ï¼Œä¾‹å¦‚ `android/arm64`ï¼Œè¡¨æ˜ä½ éœ€è¦ä¸‹è½½æˆ–æ„å»ºé€‚ç”¨äº Android 64 ä½ ARM æ¶æ„çš„ä¾èµ–é¡¹ã€‚è¿™æ¶‰åŠåˆ°å¯¹ Android ç³»ç»Ÿæ¶æ„çš„ç†è§£ã€‚

* **å·¥å…·é“¾ (Toolchain):**
    * **è¯´æ˜ï¼š**  è„šæœ¬ä¸­æåˆ°äº† `Bundle.TOOLCHAIN`ï¼ŒæŒ‡çš„æ˜¯æ„å»ºç‰¹å®šå¹³å°äºŒè¿›åˆ¶æ–‡ä»¶æ‰€éœ€çš„ç¼–è¯‘å™¨ã€é“¾æ¥å™¨ç­‰å·¥å…·çš„é›†åˆã€‚è¿™ä¸æ“ä½œç³»ç»Ÿåº•å±‚å’Œå†…æ ¸å¼€å‘å¯†åˆ‡ç›¸å…³ã€‚
    * **ä¸¾ä¾‹ï¼š**  æ„å»º Android å¹³å°çš„ Frida ä¾èµ–é¡¹éœ€è¦ Android NDK ä¸­çš„å·¥å…·é“¾ã€‚è„šæœ¬å¯èƒ½ä¼šä¸‹è½½æˆ–ä½¿ç”¨é¢„å…ˆé…ç½®å¥½çš„ Android å·¥å…·é“¾ã€‚

* **SDK (SDK):**
    * **è¯´æ˜ï¼š**  `Bundle.SDK` é€šå¸¸åŒ…å«ç‰¹å®šå¹³å°çš„å¼€å‘åº“ã€å¤´æ–‡ä»¶ç­‰ã€‚å¯¹äº Android é€†å‘ï¼Œè¿™å¯èƒ½åŒ…æ‹¬ Android SDK ä¸­çš„ä¸€äº›ç»„ä»¶æˆ– Frida å®šåˆ¶çš„ SDKã€‚
    * **ä¸¾ä¾‹ï¼š**  åŒæ­¥ Android SDK ä¾èµ–é¡¹ä¼šè·å– Frida åœ¨ Android ä¸Šè¿è¡Œæ—¶éœ€è¦çš„åº“æ–‡ä»¶ï¼Œè¿™äº›åº“å¯èƒ½ä¸ Android æ¡†æ¶å±‚äº¤äº’ã€‚

* **æ–‡ä»¶ç³»ç»Ÿè·¯å¾„å’Œæ“ä½œ:**
    * **è¯´æ˜ï¼š**  è„šæœ¬ä½¿ç”¨ `pathlib` æ¨¡å—è¿›è¡Œæ–‡ä»¶å’Œç›®å½•æ“ä½œï¼Œä¾‹å¦‚åˆ›å»ºã€åˆ é™¤ã€é‡å‘½åã€è¯»å–æ–‡ä»¶ç­‰ã€‚è¿™æ¶‰åŠåˆ°å¯¹ä¸åŒæ“ä½œç³»ç»Ÿæ–‡ä»¶ç³»ç»Ÿç»“æ„çš„ç†è§£ã€‚
    * **ä¸¾ä¾‹ï¼š**  `sync` å‘½ä»¤ä¼šå°†ä¸‹è½½çš„å‹ç¼©åŒ…è§£å‹åˆ°æŒ‡å®šçš„ `location` ç›®å½•ã€‚è„šæœ¬éœ€è¦å¤„ç†ä¸åŒæ“ä½œç³»ç»Ÿä¸‹è·¯å¾„çš„è¡¨ç¤ºæ–¹å¼ã€‚

* **è¿›ç¨‹æ‰§è¡Œ (subprocess):**
    * **è¯´æ˜ï¼š**  `roll` å’Œ `build` å‘½ä»¤ä¼šä½¿ç”¨ `subprocess` æ¨¡å—æ‰§è¡Œå¤–éƒ¨å‘½ä»¤ï¼Œä¾‹å¦‚ `aws s3 cp` ç”¨äºä¸Šä¼ æ–‡ä»¶ï¼Œä»¥åŠ Meson æ„å»ºç³»ç»Ÿã€‚
    * **ä¸¾ä¾‹ï¼š**  åœ¨æ„å»ºä¾èµ–é¡¹æ—¶ï¼Œè„šæœ¬ä¼šè°ƒç”¨ Meson å‘½ä»¤è¡Œå·¥å…·æ¥é…ç½®å’Œç¼–è¯‘æºä»£ç ã€‚è¿™éœ€è¦ç†è§£æ„å»ºç³»ç»Ÿçš„ä½¿ç”¨ã€‚

**3. é€»è¾‘æ¨ç† (å‡è®¾è¾“å…¥ä¸è¾“å‡º):**

* **å‡è®¾è¾“å…¥:** `python deps.py sync sdk windows/x86_64 ./win_deps`
* **è¾“å‡º:**  è„šæœ¬ä¼šå°è¯•ä»é¢„å®šä¹‰çš„ URL ä¸‹è½½é€‚ç”¨äº Windows 64 ä½çš„ SDK ä¾èµ–åŒ…ï¼Œå¹¶å°†å…¶è§£å‹åˆ°å½“å‰ç›®å½•ä¸‹çš„ `win_deps` æ–‡ä»¶å¤¹ä¸­ã€‚å±å¹•ä¸Šä¼šæ˜¾ç¤ºä¸‹è½½å’Œè§£å‹çš„è¿›åº¦ä¿¡æ¯ã€‚å¦‚æœåœ¨ `win_deps` ç›®å½•ä¸‹å­˜åœ¨æ—§ç‰ˆæœ¬çš„ SDKï¼Œåˆ™ä¼šè¢«åˆ é™¤ã€‚

* **å‡è®¾è¾“å…¥:** `python deps.py build --bundle toolchain --build linux/arm --host linux/arm`
* **è¾“å‡º:** è„šæœ¬ä¼šå°è¯•æ„å»º Linux ARM æ¶æ„çš„å·¥å…·é“¾ä¾èµ–é¡¹ã€‚è¿™ä¼šæ¶‰åŠåˆ°å…‹éš†ç›¸å…³çš„æºä»£ç ä»“åº“ï¼Œé…ç½®æ„å»ºç¯å¢ƒï¼Œç„¶åä½¿ç”¨ç¼–è¯‘å™¨è¿›è¡Œç¼–è¯‘ã€‚è¾“å‡ºä¼šåŒ…å«ç¼–è¯‘è¿‡ç¨‹ä¸­çš„è¯¦ç»†æ—¥å¿—ä¿¡æ¯ï¼Œæœ€ç»ˆå°†æ„å»ºäº§ç‰©å®‰è£…åˆ°æŒ‡å®šç›®å½•ã€‚

**4. ç”¨æˆ·æˆ–ç¼–ç¨‹å¸¸è§çš„ä½¿ç”¨é”™è¯¯ï¼š**

* **é”™è¯¯çš„ Bundle ç±»å‹:** ç”¨æˆ·å¯èƒ½è¾“å…¥äº†ä¸å­˜åœ¨çš„ bundle ç±»å‹ï¼Œä¾‹å¦‚ `python deps.py sync invalid_bundle ...`ï¼Œ`parse_bundle_option_value` å‡½æ•°ä¼šæ•è·è¿™ä¸ªé”™è¯¯å¹¶ç»™å‡ºæç¤ºä¿¡æ¯ã€‚
* **é”™è¯¯çš„æ“ä½œç³»ç»Ÿ/æ¶æ„:** ç”¨æˆ·å¯èƒ½è¾“å…¥äº†ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿå’Œæ¶æ„ç»„åˆï¼Œ`MachineSpec.parse` å¯èƒ½ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚
* **ç½‘ç»œé—®é¢˜:**  åœ¨ `sync` æˆ– `roll` è¿‡ç¨‹ä¸­ï¼Œå¦‚æœç½‘ç»œè¿æ¥å‡ºç°é—®é¢˜ï¼Œä¸‹è½½æ–‡ä»¶å¯èƒ½ä¼šå¤±è´¥ï¼Œå¯¼è‡´è„šæœ¬æŠ¥é”™ã€‚
* **æƒé™é—®é¢˜:**  è„šæœ¬åœ¨åˆ›å»ºæˆ–å†™å…¥æ–‡ä»¶æ—¶ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰è¶³å¤Ÿçš„æƒé™ï¼Œä¼šå¯¼è‡´æ“ä½œå¤±è´¥ã€‚
* **ç¼ºå°‘ä¾èµ–å·¥å…·:**  `roll` å‘½ä»¤ä¾èµ–äº `aws` å‘½ä»¤è¡Œå·¥å…·ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰å®‰è£…ï¼Œè„šæœ¬ä¼šæŠ¥é”™ã€‚
* **`location` è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸å¯å†™:**  åœ¨ `sync` å‘½ä»¤ä¸­ï¼Œå¦‚æœæŒ‡å®šçš„ `location` è·¯å¾„ä¸å­˜åœ¨æˆ–è€…ç”¨æˆ·æ²¡æœ‰å†™å…¥æƒé™ï¼Œè„šæœ¬ä¼šå‡ºé”™ã€‚

**5. ç”¨æˆ·æ“ä½œæ˜¯å¦‚ä½•ä¸€æ­¥æ­¥çš„åˆ°è¾¾è¿™é‡Œï¼Œä½œä¸ºè°ƒè¯•çº¿ç´¢ï¼š**

å‡è®¾ç”¨æˆ·åœ¨ä½¿ç”¨ Frida æ—¶é‡åˆ°äº†ä¾èµ–é¡¹ç›¸å…³çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

1. **Frida è¿è¡Œæ—¶æŠ¥é”™ï¼Œæç¤ºç¼ºå°‘æŸäº›åº“æ–‡ä»¶ã€‚** è¿™å¯èƒ½æ„å‘³ç€æœ¬åœ°çš„ä¾èµ–é¡¹ä¸å®Œæ•´æˆ–ç‰ˆæœ¬ä¸æ­£ç¡®ã€‚
2. **ç”¨æˆ·æƒ³è¦ä¸ºæ–°çš„å¹³å°æ„å»º Fridaã€‚** è¿™éœ€è¦æ„å»ºé’ˆå¯¹è¯¥å¹³å°çš„ä¾èµ–é¡¹ã€‚
3. **ç”¨æˆ·æ˜¯ Frida çš„å¼€å‘è€…ï¼Œæ­£åœ¨æ›´æ–°æˆ–ä¿®æ”¹æŸä¸ªä¾èµ–åº“ã€‚** ä»–éœ€è¦é‡æ–°æ„å»ºå¹¶å‘å¸ƒæ›´æ–°çš„ä¾èµ–é¡¹ã€‚

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç”¨æˆ·å¯èƒ½ä¼šæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1. **æŸ¥é˜… Frida çš„æ–‡æ¡£æˆ–å¼€å‘æŒ‡å—ï¼Œäº†è§£ä¾èµ–é¡¹çš„ç®¡ç†æ–¹å¼ã€‚**  æ–‡æ¡£å¯èƒ½ä¼šæŒ‡å‘ `deps.py` è„šæœ¬ã€‚
2. **å¯¼èˆªåˆ° Frida æºä»£ç çš„ `frida/subprojects/frida-gum/releng/` ç›®å½•ã€‚**
3. **æ ¹æ®å…·ä½“éœ€æ±‚ï¼Œæ‰§è¡Œ `deps.py` è„šæœ¬çš„ç›¸åº”å‘½ä»¤ã€‚** ä¾‹å¦‚ï¼Œä½¿ç”¨ `sync` å‘½ä»¤åŒæ­¥ä¾èµ–é¡¹ï¼Œæˆ–ä½¿ç”¨ `build` å‘½ä»¤æ„å»ºä¾èµ–é¡¹ã€‚
4. **å¦‚æœå‡ºç°é”™è¯¯ï¼ŒæŸ¥çœ‹è„šæœ¬çš„è¾“å‡ºä¿¡æ¯ï¼Œæ ¹æ®é”™è¯¯æç¤ºè¿›è¡Œè°ƒè¯•ã€‚** ä¾‹å¦‚ï¼Œæ£€æŸ¥ç½‘ç»œè¿æ¥ã€ç¡®è®¤æŒ‡å®šçš„æ“ä½œç³»ç»Ÿå’Œæ¶æ„æ˜¯å¦æ­£ç¡®ã€æ£€æŸ¥æ˜¯å¦å®‰è£…äº†å¿…è¦çš„å·¥å…·ç­‰ã€‚

**æ€»ç»“ï¼ˆé’ˆå¯¹ç¬¬ 1 éƒ¨åˆ†ä»£ç ï¼‰ï¼š**

`deps.py` çš„ç¬¬ä¸€éƒ¨åˆ†ä»£ç ä¸»è¦å®šä¹‰äº†è„šæœ¬çš„å…¥å£ (`main` å‡½æ•°)ï¼Œä»¥åŠå¤„ç†å‘½ä»¤è¡Œå‚æ•°å’Œåˆ†å‘åˆ°ä¸åŒå­å‘½ä»¤çš„é€»è¾‘ã€‚å®ƒè¿˜å®šä¹‰äº†ä¸€äº›æ ¸å¿ƒçš„è¾…åŠ©å‡½æ•°ï¼Œä¾‹å¦‚è§£æ bundle ç±»å‹ã€è§£ææ“ä½œç³»ç»Ÿå’Œæ¶æ„ã€æŸ¥è¯¢å·¥å…·é“¾å’Œ SDK çš„è·¯å¾„ï¼Œä»¥åŠæ‰§è¡ŒåŒæ­¥æ“ä½œçš„ `sync` å‡½æ•°ã€‚ æ ¸å¿ƒæ¦‚å¿µå¦‚ `Bundle` æšä¸¾å’Œ `MachineSpec` ç±»ä¹Ÿåœ¨è¿™ä¸€éƒ¨åˆ†è¢«å®šä¹‰ï¼Œä¸ºåç»­çš„ä¾èµ–é¡¹ç®¡ç†æ“ä½œæä¾›äº†åŸºç¡€ã€‚  æ€»è€Œè¨€ä¹‹ï¼Œè¿™éƒ¨åˆ†ä»£ç æ„å»ºäº†è„šæœ¬çš„éª¨æ¶ï¼Œå¹¶å®ç°äº†æœ€åŸºç¡€çš„ä¾èµ–é¡¹åŒæ­¥åŠŸèƒ½ã€‚

### æç¤ºè¯
```
è¿™æ˜¯ç›®å½•ä¸ºfrida/subprojects/frida-gum/releng/deps.pyçš„fridaDynamic instrumentation toolçš„æºä»£ç æ–‡ä»¶ï¼Œ è¯·åˆ—ä¸¾ä¸€ä¸‹å®ƒçš„åŠŸèƒ½, 
å¦‚æœå®ƒä¸é€†å‘çš„æ–¹æ³•æœ‰å…³ç³»ï¼Œè¯·åšå‡ºå¯¹åº”çš„ä¸¾ä¾‹è¯´æ˜ï¼Œ
å¦‚æœæ¶‰åŠåˆ°äºŒè¿›åˆ¶åº•å±‚ï¼Œlinux, androidå†…æ ¸åŠæ¡†æ¶çš„çŸ¥è¯†ï¼Œè¯·åšå‡ºå¯¹åº”çš„ä¸¾ä¾‹è¯´æ˜ï¼Œ
å¦‚æœåšäº†é€»è¾‘æ¨ç†ï¼Œè¯·ç»™å‡ºå‡è®¾è¾“å…¥ä¸è¾“å‡º,
å¦‚æœæ¶‰åŠç”¨æˆ·æˆ–è€…ç¼–ç¨‹å¸¸è§çš„ä½¿ç”¨é”™è¯¯ï¼Œè¯·ä¸¾ä¾‹è¯´æ˜,
è¯´æ˜ç”¨æˆ·æ“ä½œæ˜¯å¦‚ä½•ä¸€æ­¥æ­¥çš„åˆ°è¾¾è¿™é‡Œï¼Œä½œä¸ºè°ƒè¯•çº¿ç´¢ã€‚
è¿™æ˜¯ç¬¬1éƒ¨åˆ†ï¼Œå…±2éƒ¨åˆ†ï¼Œè¯·å½’çº³ä¸€ä¸‹å®ƒçš„åŠŸèƒ½
```

### æºä»£ç 
```python
#!/usr/bin/env python3
from __future__ import annotations
import argparse
import base64
from configparser import ConfigParser
import dataclasses
from dataclasses import dataclass, field
from enum import Enum
import graphlib
import itertools
import json
import os
from pathlib import Path
import re
import shlex
import shutil
import subprocess
import sys
import tarfile
import tempfile
import time
from typing import Callable, Iterator, Optional, Mapping, Sequence, Union
import urllib.request

RELENG_DIR = Path(__file__).resolve().parent
ROOT_DIR = RELENG_DIR.parent

if __name__ == "__main__":
    # TODO: Refactor
    sys.path.insert(0, str(ROOT_DIR))
sys.path.insert(0, str(RELENG_DIR / "tomlkit"))

from tomlkit.toml_file import TOMLFile

from releng import env
from releng.progress import Progress, ProgressCallback, print_progress
from releng.machine_spec import MachineSpec


def main():
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers()

    default_machine = MachineSpec.make_from_local_system().identifier

    bundle_opt_kwargs = {
        "help": "bundle (default: sdk)",
        "type": parse_bundle_option_value,
    }
    machine_opt_kwargs = {
        "help": f"os/arch (default: {default_machine})",
        "type": MachineSpec.parse,
    }

    command = subparsers.add_parser("sync", help="ensure prebuilt dependencies are up-to-date")
    command.add_argument("bundle", **bundle_opt_kwargs)
    command.add_argument("host", **machine_opt_kwargs)
    command.add_argument("location", help="filesystem location", type=Path)
    command.set_defaults(func=lambda args: sync(args.bundle, args.host, args.location.resolve()))

    command = subparsers.add_parser("roll", help="build and upload prebuilt dependencies if needed")
    command.add_argument("bundle", **bundle_opt_kwargs)
    command.add_argument("host", **machine_opt_kwargs)
    command.add_argument("--build", default=default_machine, **machine_opt_kwargs)
    command.add_argument("--activate", default=False, action='store_true')
    command.add_argument("--post", help="post-processing script")
    command.set_defaults(func=lambda args: roll(args.bundle, args.build, args.host, args.activate,
                                                Path(args.post) if args.post is not None else None))

    command = subparsers.add_parser("build", help="build prebuilt dependencies")
    command.add_argument("--bundle", default=Bundle.SDK, **bundle_opt_kwargs)
    command.add_argument("--build", default=default_machine, **machine_opt_kwargs)
    command.add_argument("--host", default=default_machine, **machine_opt_kwargs)
    command.add_argument("--only", help="only build packages A, B, and C", metavar="A,B,C",
                         type=parse_set_option_value)
    command.add_argument("--exclude", help="exclude packages A, B, and C", metavar="A,B,C",
                         type=parse_set_option_value, default=set())
    command.add_argument("-v", "--verbose", help="be verbose", action="store_true")
    command.set_defaults(func=lambda args: build(args.bundle, args.build, args.host,
                                                 args.only, args.exclude, args.verbose))

    command = subparsers.add_parser("wait", help="wait for prebuilt dependencies if needed")
    command.add_argument("bundle", **bundle_opt_kwargs)
    command.add_argument("host", **machine_opt_kwargs)
    command.set_defaults(func=lambda args: wait(args.bundle, args.host))

    command = subparsers.add_parser("bump", help="bump dependency versions")
    command.set_defaults(func=lambda args: bump())

    args = parser.parse_args()
    if 'func' in args:
        try:
            args.func(args)
        except CommandError as e:
            print(e, file=sys.stderr)
            sys.exit(1)
    else:
        parser.print_usage(file=sys.stderr)
        sys.exit(1)


def parse_bundle_option_value(raw_bundle: str) -> Bundle:
    try:
        return Bundle[raw_bundle.upper()]
    except KeyError:
        choices = "', '".join([e.name.lower() for e in Bundle])
        raise argparse.ArgumentTypeError(f"invalid choice: {raw_bundle} (choose from '{choices}')")


def parse_set_option_value(v: str) -> set[str]:
    return set([v.strip() for v in v.split(",")])


def query_toolchain_prefix(machine: MachineSpec,
                           cache_dir: Path) -> Path:
    if machine.os == "windows":
        identifier = "windows-x86" if machine.arch in {"x86", "x86_64"} else machine.os_dash_arch
    else:
        identifier = machine.identifier
    return cache_dir / f"toolchain-{identifier}"


def ensure_toolchain(machine: MachineSpec,
                     cache_dir: Path,
                     version: Optional[str] = None,
                     on_progress: ProgressCallback = print_progress) -> tuple[Path, SourceState]:
    toolchain_prefix = query_toolchain_prefix(machine, cache_dir)
    state = sync(Bundle.TOOLCHAIN, machine, toolchain_prefix, version, on_progress)
    return (toolchain_prefix, state)


def query_sdk_prefix(machine: MachineSpec,
                     cache_dir: Path) -> Path:
    return cache_dir / f"sdk-{machine.identifier}"


def ensure_sdk(machine: MachineSpec,
               cache_dir: Path,
               version: Optional[str] = None,
               on_progress: ProgressCallback = print_progress) -> tuple[Path, SourceState]:
    sdk_prefix = query_sdk_prefix(machine, cache_dir)
    state = sync(Bundle.SDK, machine, sdk_prefix, version, on_progress)
    return (sdk_prefix, state)


def detect_cache_dir(sourcedir: Path) -> Path:
    raw_location = os.environ.get("FRIDA_DEPS", None)
    if raw_location is not None:
        location = Path(raw_location)
    else:
        location = sourcedir / "deps"
    return location


def sync(bundle: Bundle,
         machine: MachineSpec,
         location: Path,
         version: Optional[str] = None,
         on_progress: ProgressCallback = print_progress) -> SourceState:
    state = SourceState.PRISTINE

    if version is None:
        version = load_dependency_parameters().deps_version

    bundle_nick = bundle.name.lower() if bundle != Bundle.SDK else bundle.name

    if location.exists():
        try:
            cached_version = (location / "VERSION.txt").read_text(encoding="utf-8").strip()
            if cached_version == version:
                return state
        except:
            pass
        shutil.rmtree(location)
        state = SourceState.MODIFIED

    (url, filename) = compute_bundle_parameters(bundle, machine, version)

    local_bundle = location.parent / filename
    if local_bundle.exists():
        on_progress(Progress("Deploying local {}".format(bundle_nick)))
        archive_path = local_bundle
        archive_is_temporary = False
    else:
        if bundle == Bundle.SDK:
            on_progress(Progress(f"Downloading SDK {version} for {machine.identifier}"))
        else:
            on_progress(Progress(f"Downloading {bundle_nick} {version}"))
        try:
            with urllib.request.urlopen(url) as response, \
                    tempfile.NamedTemporaryFile(delete=False) as archive:
                shutil.copyfileobj(response, archive)
                archive_path = Path(archive.name)
                archive_is_temporary = True
            on_progress(Progress(f"Extracting {bundle_nick}"))
        except urllib.error.HTTPError as e:
            if e.code == 404:
                raise BundleNotFoundError(f"missing bundle at {url}") from e
            raise e

    try:
        staging_dir = location.parent / f"_{location.name}"
        if staging_dir.exists():
            shutil.rmtree(staging_dir)
        staging_dir.mkdir(parents=True)

        with tarfile.open(archive_path, "r:xz") as tar:
            tar.extractall(staging_dir)

        suffix_len = len(".frida.in")
        raw_location = location.as_posix()
        for f in staging_dir.rglob("*.frida.in"):
            target = f.parent / f.name[:-suffix_len]
            f.write_text(f.read_text(encoding="utf-8").replace("@FRIDA_TOOLROOT@", raw_location),
                         encoding="utf-8")
            f.rename(target)

        staging_dir.rename(location)
    finally:
        if archive_is_temporary:
            archive_path.unlink()

    return state


def roll(bundle: Bundle,
         build_machine: MachineSpec,
         host_machine: MachineSpec,
         activate: bool,
         post: Optional[Path]):
    params = load_dependency_parameters()
    version = params.deps_version

    if activate and bundle == Bundle.SDK:
        configure_bootstrap_version(version)

    (public_url, filename) = compute_bundle_parameters(bundle, host_machine, version)

    # First do a quick check to avoid hitting S3 in most cases.
    request = urllib.request.Request(public_url)
    request.get_method = lambda: "HEAD"
    try:
        with urllib.request.urlopen(request) as r:
            return
    except urllib.request.HTTPError as e:
        if e.code != 404:
            raise CommandError("network error") from e

    s3_url = "s3://build.frida.re/deps/{version}/{filename}".format(version=version, filename=filename)

    # We will most likely need to build, but let's check S3 to be certain.
    r = subprocess.run(["aws", "s3", "ls", s3_url], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, encoding="utf-8")
    if r.returncode == 0:
        return
    if r.returncode != 1:
        raise CommandError(f"unable to access S3: {r.stdout.strip()}")

    artifact = build(bundle, build_machine, host_machine)

    if post is not None:
        post_script = RELENG_DIR / post
        if not post_script.exists():
            raise CommandError("post-processing script not found")

        subprocess.run([
                           sys.executable, post_script,
                           "--bundle=" + bundle.name.lower(),
                           "--host=" + host_machine.identifier,
                           "--artifact=" + str(artifact),
                           "--version=" + version,
                       ],
                       check=True)

    subprocess.run(["aws", "s3", "cp", artifact, s3_url], check=True)

    # Use the shell for Windows compatibility, where npm generates a .bat script.
    subprocess.run("cfcli purge " + public_url, shell=True, check=True)

    if activate and bundle == Bundle.TOOLCHAIN:
        configure_bootstrap_version(version)


def build(bundle: Bundle,
          build_machine: MachineSpec,
          host_machine: MachineSpec,
          only_packages: Optional[set[str]] = None,
          excluded_packages: set[str] = set(),
          verbose: bool = False) -> Path:
    builder = Builder(bundle, build_machine, host_machine, verbose)
    try:
        return builder.build(only_packages, excluded_packages)
    except subprocess.CalledProcessError as e:
        print(e, file=sys.stderr)
        if e.stdout is not None:
            print("\n=== stdout ===\n" + e.stdout, file=sys.stderr)
        if e.stderr is not None:
            print("\n=== stderr ===\n" + e.stderr, file=sys.stderr)
        sys.exit(1)


class Builder:
    def __init__(self,
                 bundle: Bundle,
                 build_machine: MachineSpec,
                 host_machine: MachineSpec,
                 verbose: bool):
        self._bundle = bundle
        self._host_machine = host_machine.default_missing()
        self._build_machine = build_machine.default_missing().maybe_adapt_to_host(self._host_machine)
        self._verbose = verbose
        self._default_library = "static"

        self._params = load_dependency_parameters()
        self._cachedir = detect_cache_dir(ROOT_DIR)
        self._workdir = self._cachedir / "src"

        self._toolchain_prefix: Optional[Path] = None
        self._build_config: Optional[env.MachineConfig] = None
        self._host_config: Optional[env.MachineConfig] = None
        self._build_env: dict[str, str] = {}
        self._host_env: dict[str, str] = {}

        self._ansi_supported = os.environ.get("TERM") != "dumb" \
                    and (self._build_machine.os != "windows" or "WT_SESSION" in os.environ)

    def build(self,
              only_packages: Optional[list[str]],
              excluded_packages: set[str]) -> Path:
        started_at = time.time()
        prepare_ended_at = None
        clone_time_elapsed = None
        build_time_elapsed = None
        build_ended_at = None
        packaging_ended_at = None
        try:
            all_packages = {i: self._resolve_package(p) for i, p in self._params.packages.items() \
                    if self._can_build(p)}
            if only_packages is not None:
                toplevel_packages = [all_packages[identifier] for identifier in only_packages]
                selected_packages = self._resolve_dependencies(toplevel_packages, all_packages)
            elif self._bundle is Bundle.TOOLCHAIN:
                toplevel_packages = [p for p in all_packages.values() if p.scope == "toolchain"]
                selected_packages = self._resolve_dependencies(toplevel_packages, all_packages)
            else:
                selected_packages = {i: p for i, p, in all_packages.items() if p.scope is None}
            selected_packages = {i: p for i, p in selected_packages.items() if i not in excluded_packages}

            packages = [selected_packages[i] for i in iterate_package_ids_in_dependency_order(selected_packages.values())]
            all_deps = itertools.chain.from_iterable([pkg.dependencies for pkg in packages])
            deps_for_build_machine = {dep.identifier for dep in all_deps if dep.for_machine == "build"}

            self._prepare()
            prepare_ended_at = time.time()

            clone_time_elapsed = 0
            build_time_elapsed = 0
            for pkg in packages:
                self._print_package_banner(pkg)

                t1 = time.time()
                self._clone_repo_if_needed(pkg)
                t2 = time.time()
                clone_time_elapsed += t2 - t1

                machines = [self._host_machine]
                if pkg.identifier in deps_for_build_machine:
                    machines += [self._build_machine]
                self._build_package(pkg, machines)
                t3 = time.time()
                build_time_elapsed += t3 - t2
            build_ended_at = time.time()

            artifact_file = self._package()
            packaging_ended_at = time.time()
        finally:
            ended_at = time.time()

            if prepare_ended_at is not None:
                self._print_summary_banner()
                print("      Total: {}".format(format_duration(ended_at - started_at)))

            if prepare_ended_at is not None:
                print("    Prepare: {}".format(format_duration(prepare_ended_at - started_at)))

            if clone_time_elapsed is not None:
                print("      Clone: {}".format(format_duration(clone_time_elapsed)))

            if build_time_elapsed is not None:
                print("      Build: {}".format(format_duration(build_time_elapsed)))

            if packaging_ended_at is not None:
                print("  Packaging: {}".format(format_duration(packaging_ended_at - build_ended_at)))

            print("", flush=True)

        return artifact_file

    def _can_build(self, pkg: PackageSpec) -> bool:
        return self._evaluate_condition(pkg.when)

    def _resolve_package(self, pkg: PackageSpec) -> bool:
        resolved_opts = [opt for opt in pkg.options if self._evaluate_condition(opt.when)]
        resolved_deps = [dep for dep in pkg.dependencies if self._evaluate_condition(dep.when)]
        return dataclasses.replace(pkg,
                                   options=resolved_opts,
                                   dependencies=resolved_deps)

    def _resolve_dependencies(self,
                              packages: Sequence[PackageSpec],
                              all_packages: Mapping[str, PackageSpec]) -> dict[str, PackageSpec]:
        result = {p.identifier: p for p in packages}
        for p in packages:
            self._resolve_package_dependencies(p, all_packages, result)
        return result

    def _resolve_package_dependencies(self,
                                      package: PackageSpec,
                                      all_packages: Mapping[str, PackageSpec],
                                      resolved_packages: Mapping[str, PackageSpec]):
        for dep in package.dependencies:
            identifier = dep.identifier
            if identifier in resolved_packages:
                continue
            p = all_packages[identifier]
            resolved_packages[identifier] = p
            self._resolve_package_dependencies(p, all_packages, resolved_packages)

    def _evaluate_condition(self, cond: Optional[str]) -> bool:
        if cond is None:
            return True
        global_vars = {
            "Bundle": Bundle,
            "bundle": self._bundle,
            "machine": self._host_machine,
        }
        return eval(cond, global_vars)

    def _prepare(self):
        self._toolchain_prefix, toolchain_state = \
                ensure_toolchain(self._build_machine,
                                 self._cachedir,
                                 version=self._params.bootstrap_version)
        if toolchain_state == SourceState.MODIFIED:
            self._wipe_build_state()

        envdir = self._get_builddir_container()
        envdir.mkdir(parents=True, exist_ok=True)

        menv = {**os.environ}

        if self._bundle is Bundle.TOOLCHAIN:
            extra_ldflags = []
            if self._host_machine.is_apple:
                symfile = envdir / "toolchain-executable.symbols"
                symfile.write_text("# No exported symbols.\n", encoding="utf-8")
                extra_ldflags += [f"-Wl,-exported_symbols_list,{symfile}"]
            elif self._host_machine.os != "windows":
                verfile = envdir / "toolchain-executable.version"
                verfile.write_text("\n".join([
                                                 "{",
                                                 "  global:",
                                                 "    # FreeBSD needs these two:",
                                                 "    __progname;",
                                                 "    environ;",
                                                 "",
                                                 "  local:",
                                                 "    *;",
                                                 "};",
                                                 ""
                                             ]),
                                   encoding="utf-8")
                extra_ldflags += [f"-Wl,--version-script,{verfile}"]
            if extra_ldflags:
                menv["LDFLAGS"] = shlex.join(extra_ldflags + shlex.split(menv.get("LDFLAGS", "")))

        build_sdk_prefix = None
        host_sdk_prefix = None

        self._build_config, self._host_config = \
                env.generate_machine_configs(self._build_machine,
                                             self._host_machine,
                                             menv,
                                             self._toolchain_prefix,
                                             build_sdk_prefix,
                                             host_sdk_prefix,
                                             self._call_meson,
                                             self._default_library,
                                             envdir)
        self._build_env = self._build_config.make_merged_environment(os.environ)
        self._host_env = self._host_config.make_merged_environment(os.environ)

    def _clone_repo_if_needed(self, pkg: PackageSpec):
        sourcedir = self._get_sourcedir(pkg)

        git = lambda *args, **kwargs: subprocess.run(["git", *args],
                                                     **kwargs,
                                                     capture_output=True,
                                                     encoding="utf-8")

        if sourcedir.exists():
            self._print_status(pkg.name, "Reusing existing checkout")
            current_rev = git("rev-parse", "FETCH_HEAD", cwd=sourcedir, check=True).stdout.strip()
            if current_rev != pkg.version:
                self._print_status(pkg.name, "WARNING: Checkout does not match version in deps.toml")
        else:
            self._print_status(pkg.name, "Cloning")
            clone_shallow(pkg, sourcedir, git)

    def _wipe_build_state(self):
        for path in (self._get_outdir(), self._get_builddir_container()):
            if path.exists():
                self._print_status(path.relative_to(self._workdir).as_posix(), "Wiping")
                shutil.rmtree(path)

    def _build_package(self, pkg: PackageSpec, machines: Sequence[MachineSpec]):
        for machine in machines:
            manifest_path = self._get_manifest_path(pkg, machine)
            action = "skip" if manifest_path.exists() else "build"

            message = "Building" if action == "build" else "Already built"
            message += f" for {machine.identifier}"
            self._print_status(pkg.name, message)

            if action == "build":
                self._build_package_for_machine(pkg, machine)
                assert manifest_path.exists()

    def _build_package_for_machine(self, pkg: PackageSpec, machine: MachineSpec):
        sourcedir = self._get_sourcedir(pkg)
        builddir = self._get_builddir(pkg, machine)

        prefix = self._get_prefix(machine)
        libdir = prefix / "lib"

        strip = "true" if machine.toolchain_can_strip else "false"

        if builddir.exists():
            shutil.rmtree(builddir)

        machine_file_opts = [f"--native-file={self._build_config.machine_file}"]
        pc_opts = [f"-Dpkg_config_path={prefix / machine.libdatadir / 'pkgconfig'}"]
        if self._host_config is not self._build_config and machine is self._host_machine:
            machine_file_opts += [f"--cross-file={self._host_config.machine_file}"]
            pc_path_for_build = self._get_prefix(self._build_machine) / self._build_machine.libdatadir / "pkgconfig"
            pc_opts += [f"-Dbuild.pkg_config_path={pc_path_for_build}"]

        menv = self._host_env if machine is self._host_machine else self._build_env

        meson_kwargs = {
            "env": menv,
            "check": True,
        }
        if not self._verbose:
            meson_kwargs["capture_output"] = True
            meson_kwargs["encoding"] = "utf-8"

        self._call_meson([
                             "setup",
                             builddir,
                             *machine_file_opts,
                             f"-Dprefix={prefix}",
                             f"-Dlibdir={libdir}",
                             *pc_opts,
                             f"-Ddefault_library={self._default_library}",
                             f"-Dbackend=ninja",
                             *machine.meson_optimization_options,
                             f"-Dstrip={strip}",
                             *[opt.value for opt in pkg.options],
                         ],
                         cwd=sourcedir,
                         **meson_kwargs)

        self._call_meson(["install"],
                         cwd=builddir,
                         **meson_kwargs)

        manifest_lines = []
        install_locations = json.loads(self._call_meson(["introspect", "--installed"],
                                                        cwd=builddir,
                                                        capture_output=True,
                                                        encoding="utf-8",
                                                        env=menv).stdout)
        for installed_path in install_locations.values():
            manifest_lines.append(Path(installed_path).relative_to(prefix).as_posix())
        manifest_lines.sort()
        manifest_path = self._get_manifest_path(pkg, machine)
        manifest_path.parent.mkdir(parents=True, exist_ok=True)
        manifest_path.write_text("\n".join(manifest_lines) + "\n", encoding="utf-8")

    def _call_meson(self, argv, *args, **kwargs):
        if self._verbose and argv[0] in {"setup", "install"}:
            vanilla_env = os.environ
            meson_env = kwargs["env"]
            changed_env = {k: v for k, v in meson_env.items() if k not in vanilla_env or v != vanilla_env[k]}

            indent = "  "
            env_summary = f" \\\n{indent}".join([f"{k}={shlex.quote(v)}" for k, v in changed_env.items()])
            argv_summary = f" \\\n{3 * indent}".join([str(arg) for arg in argv])

            print(f"> {env_summary} \\\n{indent}meson {argv_summary}", flush=True)

        return env.call_meson(argv, use_submodule=True, *args, **kwargs)

    def _package(self):
        outfile = self._cachedir / f"{self._bundle.name.lower()}-{self._host_machine.identifier}.tar.xz"

        self._print_packaging_banner()
        with tempfile.TemporaryDirectory(prefix="frida-deps") as raw_tempdir:
            tempdir = Path(raw_tempdir)

            self._print_status(outfile.name, "Staging files")
            if self._bundle is Bundle.TOOLCHAIN:
                self._stage_toolchain_files(tempdir)
            else:
                self._stage_sdk_files(tempdir)

            self._adjust_manifests(tempdir)
            self._adjust_files_containing_hardcoded_paths(tempdir)

            (tempdir / "VERSION.txt").write_text(self._params.deps_version + "\n", encoding="utf-8")

            self._print_status(outfile.name, "Assembling")
            with tarfile.open(outfile, "w:xz") as tar:
                tar.add(tempdir, ".")

            self._print_status(outfile.name, "All done")

        return outfile

    def _stage_toolchain_files(self, location: Path) -> list[Path]:
        if self._host_machine.os == "windows":
            toolchain_prefix = self._toolchain_prefix
            mixin_files = [f for f in self._walk_plain_files(toolchain_prefix)
                           if self._file_should_be_mixed_into_toolchain(f)]
            copy_files(toolchain_prefix, mixin_files, location)

        prefix = self._get_prefix(self._host_machine)
        files = [f for f in self._walk_plain_files(prefix)
                 if self._file_is_toolchain_related(f)]
        copy_files(prefix, files, location)

    def _stage_sdk_files(self, location: Path) -> list[Path]:
        prefix = self._get_prefix(self._host_machine)
        files = [f for f in self._walk_plain_files(prefix)
                 if self._file_is_sdk_related(f)]
        copy_files(prefix, files, location)

    def _adjust_files_containing_hardcoded_paths(self, bundledir: Path):
        prefix = self._get_prefix(self._host_machine)

        raw_prefixes = [str(prefix)]
        if self._host_machine.os == "windows":
            raw_prefixes.append(prefix.as_posix())

        for f in self._walk_plain_files(bundledir):
            filepath = bundledir / f
            try:
                text = filepath.read_text(encoding="utf-8")

                new_text = text
                is_pcfile = filepath.suffix == ".pc"
                replacement = "${frida_sdk_prefix}" if is_pcfile else "@FRIDA_TOOLROOT@"
                for p in raw_prefixes:
                    new_text = new_text.replace(p, replacement)

                if new_text != text:
                    filepath.write_text(new_text, encoding="utf-8")
                    if not is_pcfile:
                        filepath.rename(filepath.parent / f"{f.name}.frida.in")
            except UnicodeDecodeError:
                pass

    @staticmethod
    def _walk_plain_files(rootdir: Path) -> Iterator[Path]:
        for dirpath, dirnames, filenames in os.walk(rootdir):
            for filename in filenames:
                f = Path(dirpath) / filename
                if f.is_symlink():
                    continue
                yield f.relative_to(rootdir)

    @staticmethod
    def _adjust_manifests(bundledir: Path):
        for manifest_path in (bundledir / "manifest").glob("*.pkg"):
            lines = []

            prefix = manifest_path.parent.parent
            for entry in manifest_path.read_text(encoding="utf-8").strip().split("\n"):
                if prefix.joinpath(entry).exists():
                    lines.append(entry)

            if lines:
                lines.sort()
                manifest_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
            else:
                manifest_path.unlink()

    def _file_should_be_mixed_into_toolchain(self, f: Path) -> bool:
        parts = f.parts
        if parts[0] == "VERSION.txt":
            return False
        if parts[0] == "bin":
            stem = f.stem
            return stem in {"bison", "flex", "m4", "nasm", "vswhere"} or stem.startswith("msys-")
        if parts[0] == "manifest":
            return False

        if self._file_is_vala_toolchain_related(f):
            return False

        return True

    def _file_is_toolchain_related(self, f: Path) -> bool:
        if self._file_is_vala_toolchain_related(f):
            return True

        parts = f.parts
        if parts[0] == "bin":
            if f.suffix == ".pdb":
                return False
            stem = f.stem
            if stem in {"gdbus", "gio", "gobject-query", "gsettings"}:
                return False
            if stem.startswith("gspawn-"):
                return False
            return True
        if parts[0] == "manifest":
            return True

        return False

    def _file_is_vala_toolchain_related(self, f: Path) -> bool:
        if f.suffix in {".vapi", ".deps"}:
            return True

        name = f.name
        if f.suffix == self._host_machine.executable_suffix:
            return name.startswith("vala") or name.startswith("vapi") or name.startswith("gen-introspect")
        if f.parts[0] == "bin" and name.startswith("vala-gen-introspect"):
            return True

        return False

    def _file_is_sdk_related(self, f: Path) -> bool:
        suffix = f.suffix
        if suffix == ".pdb":
            return False
        if suffix in [".vapi", ".deps"]:
            return True

        parts = f.parts
        if parts[0] == "bin":
            return f.name.startswith("v8-mksnapshot-")

        return "share" not in parts

    def _get_outdir(self) -> Path:
        return self._workdir / f"_{self._bundle.name.lower()}.out"

    def _get_sourcedir(self, pkg: PackageSpec) -> Path:
        return self._workdir / pkg.identifier

    def _get_builddir(self, pkg: PackageSpec, machine: MachineSpec) -> Path:
        return self._get_builddir_container() / machine.identifier / pkg.identifier

    def _get_builddir_container(self) -> Path:
        return self._workdir / f"_{self._bundle.name.lower()}.tmp"

    def _get_prefix(self, machine: MachineSpec) -> Path:
        return self._get_outdir() / machine.identifier

    def _get_manifest_path(self, pkg: PackageSpec, machine: MachineSpec) -> Path:
        return self._get_prefix(machine) / "manifest" / f"{pkg.identifier}.pkg"

    def _print_package_banner(self, pkg: PackageSpec):
        if self._ansi_supported:
            print("\n".join([
                "",
                "â•­â”€â”€â”€â”€",
                f"â”‚ ğŸ“¦ \033[1m{pkg.name}\033[0m",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®",
                f"â”‚ URL: {pkg.url}",
                f"â”‚ CID: {pkg.version}",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯",
            ]), flush=True)
        else:
            print("\n".join([
                "",
                f"# {pkg.name}",
                f"- URL: {pkg.url}",
                f"- CID: {pkg.version}",
            ]), flush=True)

    def _print_packaging_banner(self):
        if self._ansi_supported:
            print("\n".join([
                "",
                "â•­â”€â”€â”€â”€",
                f"â”‚ ğŸ—ï¸  \033[1mPackaging\033[0m",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®",
            ]), flush=True)
        else:
            print("\n".join([
                "",
                f"# Packaging",
            ]), flush=True)

    def _print_summary_banner(self):
        if self._ansi_supported:
            print("\n".join([
                "",
                "â•­â”€â”€â”€â”€",
                f"â”‚ ğŸ‰ \033[1mDone\033[0m",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®",
            ]), flush=True)
        else:
            print("\n".join([
                "",
                f"# Done",
```