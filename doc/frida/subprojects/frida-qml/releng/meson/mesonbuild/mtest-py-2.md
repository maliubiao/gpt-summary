Response:
The user wants a summary of the functionalities of the Python code provided. I need to identify the main tasks performed by the script and explain them in the context of a dynamic instrumentation tool like Frida.

Here's a breakdown of the code's functionality:

1. **Test Discovery and Loading:** The code is responsible for discovering and loading test definitions from a Meson build directory. This involves reading metadata files (`meson_test_setup.dat`, `meson_benchmark_setup.dat`) generated by the Meson build system.
2. **Test Execution:** It provides the framework for executing these tests, either sequentially or in parallel. This includes handling test timeouts, setting up the test environment, and running the test executables.
3. **Test Result Processing and Reporting:**  The code captures the results of each test run (success, failure, skip, timeout, etc.) and formats them for display on the console and optionally writes them to log files (JUnit XML, JSON, plain text).
4. **Integration with Build System (Meson):** It interacts with the Meson build system to ensure the tests are run in an up-to-date build environment. This includes triggering rebuilds if necessary.
5. **Support for Test Suites and Filtering:** The script allows users to specify which tests to run based on suite names or individual test names. It also supports excluding specific suites.
6. **Support for Debugging:**  It provides options for running tests under a debugger (GDB) or with custom wrappers.
7. **Handling Cross-Compilation:** The code can handle tests that are cross-compiled and require execution on a different architecture, potentially using an execution wrapper.
8. **Error Handling:** It includes mechanisms for catching errors during test execution and reporting them.

Now, let's connect these functionalities to the concepts requested by the user:

*   **Relationship to Reverse Engineering:** Testing plays a crucial role in reverse engineering. After instrumenting an application with Frida, developers write tests to verify the behavior of their instrumentation code and ensure it's working as expected. These tests might check if hooks are being called correctly, if data is being modified as intended, or if specific functions are behaving in a certain way.
*   **Involvement of Binary Underpinnings, Linux/Android Kernel, and Frameworks:** When testing Frida instrumentation, the tests often interact with the underlying binary code, potentially touching operating system APIs or framework functionalities (especially on Android). For example, tests might verify the interception of system calls or the manipulation of objects within the Android runtime.
*   **Logical Reasoning:** The test selection and execution logic involves logical reasoning. The script needs to determine which tests to run based on user-provided arguments, suite inclusions/exclusions, and the structure of the test definitions.
*   **Common User Errors:** Users might make mistakes like specifying incorrect test names, forgetting to build the tests, or having environment conflicts.
*   **User Operation and Debugging:**  The script is invoked from the command line, and understanding the command-line options and the build process is essential for debugging test failures.

Finally, I will summarize the main functions of the script based on this analysis.
这是frida动态 instrumentation tool的测试执行框架代码的第三部分，主要负责实际的测试运行和结果处理，以及一些辅助功能。结合前两部分的功能，我们可以归纳一下 `mtest.py` 的主要功能如下：

**核心功能：测试执行与结果处理**

1. **执行测试用例 (`run_tests`, `_run_tests`, `run_test`)**:
    *   接收由 `get_tests` 函数筛选出的待执行测试用例列表 (`runners`)。
    *   使用 `asyncio` 异步并发地执行这些测试用例，通过 `asyncio.Semaphore` 控制并发数量 (`options.num_processes`)。
    *   支持测试用例的重复执行 (`options.repeat`)。
    *   在执行前可以设置每个测试用例的运行环境 (`get_test_runner`)，包括环境变量、调试器 (`gdb`) 或自定义执行包装器 (`wrapper`)。
    *   支持串行和并行测试用例的混合执行，通过 `runner.is_parallel` 区分。
    *   如果设置了最大失败次数 (`options.maxfail`)，并在执行过程中达到该阈值，会中断后续测试的执行。

2. **处理测试结果 (`process_test_result`)**:
    *   接收每个测试用例的运行结果 (`TestRun` 对象)。
    *   根据测试结果的状态（成功、失败、跳过、超时等）更新计数器 (`success_count`, `fail_count`, `skip_count`, `timeout_count`, `expectedfail_count`, `unexpectedpass_count`)。
    *   将失败的测试结果添加到 `collected_failures` 列表中。
    *   将测试结果传递给配置的日志记录器 (`loggers`) 进行记录。

3. **日志记录 (`open_logfiles`, `close_logfiles`, `flush_logfiles`, `log_subtest`, `log_start_test`)**:
    *   根据配置打开不同的日志文件记录器（JUnit XML, JSON, 文本）。
    *   在测试开始、结束以及子测试执行时，调用日志记录器的方法记录相关信息。

4. **信号处理 (`sigint_handler`, `sigterm_handler`)**:
    *   监听 `SIGINT` (Ctrl+C) 和 `SIGTERM` 信号。
    *   在接收到信号时，可以优雅地中断当前正在运行的测试，或者直接退出。
    *   支持连续多次按下 Ctrl+C 直接退出的行为。

**辅助功能：**

5. **获取测试执行的包装器 (`get_wrapper`)**:
    *   根据命令行选项 (`gdb`, `wrapper`) 决定测试执行时是否需要使用调试器或自定义包装器。
    *   如果指定了 `gdb`，则会构建启动 `gdb` 的命令。

6. **获取格式化的测试套件名称 (`get_pretty_suite`)**:
    *   生成更易读的测试套件名称，包含了项目名称和具体的测试名称。

7. **重构建依赖 (`rebuild_deps`)**:
    *   在运行测试前，检查测试用例的依赖项是否需要重新构建。
    *   调用 `ninja` 命令来构建这些依赖项。

8. **列出测试用例 (`list_tests`)**:
    *   如果命令行指定了 `--list` 选项，则列出所有符合条件的测试用例名称，而不实际执行测试。

9. **处理测试名称参数 (`tests_from_args`)**:
    *   允许用户通过命令行参数指定要运行的测试用例名称，支持通配符匹配。
    *   如果指定的测试名称找不到匹配的测试用例，会抛出错误。
    *   如果指定的测试名称匹配到多个测试用例，但由于之前的匹配已经被选中，会发出警告。

10. **计算显示相关的长度 (`numlen`, `max_left_width`, `duration_max_len`)**:
    *   计算用于在控制台输出测试进度和结果时需要的字符串长度，以保持格式的整齐。

11. **格式化测试结果输出 (`format`, `get_test_num_prefix`)**:
    *   将测试结果格式化为易于阅读的字符串，包含测试编号、名称、结果和执行时间。

12. **生成测试结果摘要 (`summary`, `total_failure_count`)**:
    *   在测试执行完成后，生成一个包含各种测试结果数量的摘要信息。

**与逆向方法的关联及举例说明:**

*   **验证 Instrumentation 代码:** 在使用 Frida 进行逆向工程时，经常需要编写 Instrumentation 脚本来修改程序的行为。`mtest.py` 负责运行测试用例，这些测试用例可以用来验证 Instrumentation 代码是否按预期工作。

    *   **举例:** 假设我们编写了一个 Frida 脚本来 Hook `open` 系统调用，记录所有被打开的文件路径。我们可以编写一个测试用例，该用例会触发程序调用 `open` 函数，然后测试用例会检查日志文件，确认我们的 Hook 是否成功记录了预期的文件路径。

**涉及二进制底层，Linux, Android 内核及框架的知识的举例说明:**

*   **执行依赖二进制文件的测试:** 测试用例通常会依赖于被 Instrumentation 的目标程序，这是一个二进制文件。`mtest.py` 需要能够执行这个二进制文件。
*   **测试环境设置:** 测试环境的设置可能涉及到对环境变量的修改，这与 Linux 或 Android 系统的环境配置有关。
*   **跨平台测试:** Frida 可以用于 Android 和 Linux 等平台，`mtest.py` 需要能够处理不同平台下的测试执行。
*   **执行包装器 (`wrapper`)**:  在某些情况下，可能需要在特定的环境下运行测试，例如使用 `chroot` 或 `namespaces` 隔离环境。`mtest.py` 支持使用自定义的执行包装器来实现这些需求。

    *   **举例:**  在测试 Android 平台的 Instrumentation 代码时，可能需要在一个模拟的 Android 环境中运行测试。可以编写一个执行包装器，该包装器负责启动 Android 模拟器，并将测试程序部署到模拟器中执行。

**逻辑推理的假设输入与输出:**

*   **假设输入:**
    *   `self.tests`:  一个包含多个 `TestSerialisation` 对象的列表，每个对象代表一个测试用例，包含测试名称、依赖项、所属套件等信息。
    *   `self.options.include_suites`:  一个包含要包含的测试套件名称的列表，例如 `['network']`.
    *   `self.options.exclude_suites`:  一个包含要排除的测试套件名称的列表，例如 `['gui']`.

*   **逻辑推理过程 (在 `get_tests` 和 `test_suitable` 中):**
    1. 遍历 `self.tests` 中的每个测试用例 `t`。
    2. 检查 `t` 的套件是否在 `self.options.exclude_suites` 中，如果在，则排除该测试用例。
    3. 如果 `self.options.include_suites` 不为空，则检查 `t` 的套件是否在 `self.options.include_suites` 中，如果不在，则排除该测试用例。
    4. 如果测试用例通过了上述过滤，则将其添加到最终的测试用例列表中。

*   **假设输出:**  一个包含所有套件名称包含 'network' 但不包含 'gui' 的测试用例的列表。

**用户或编程常见的使用错误及举例说明:**

*   **测试名称拼写错误:**  用户在命令行中使用 `--args` 指定要运行的测试用例名称时，可能会拼写错误。

    *   **举例:**  假设有一个测试用例名为 `test_api_call`，用户在命令行中输入 `meson test test_ap_call`。`mtest.py` 会在 `tests_from_args` 中检测到 `test_ap_call` 找不到匹配的测试用例，并抛出 `MesonException` 异常。

*   **忘记构建测试用例:**  在修改了测试代码后，用户可能忘记重新构建测试用例就直接运行 `meson test`。

    *   **举例:**  `mtest.py` 在 `prepare_build` 和 `load_metadata` 中会检查构建系统是否需要重新构建。如果检测到需要重新构建，但由于某种原因（例如找不到 `ninja`），构建失败，程序会退出并提示错误信息。

*   **环境变量冲突:**  测试用例可能依赖特定的环境变量，如果用户的系统环境变量与测试用例所需的冲突，可能导致测试失败。

    *   **举例:**  某个测试用例依赖于环境变量 `API_KEY` 的设置。如果用户没有设置该环境变量或者设置了错误的值，测试用例可能会因为无法正确连接 API 而失败。

**用户操作如何一步步到达这里，作为调试线索:**

1. **用户编写或修改了 Frida Instrumentation 脚本和对应的测试代码。**
2. **用户在项目的构建目录下打开终端。**
3. **用户输入命令 `meson test` 或 `meson test <test_name>` 或 `meson test --suite <suite_name>` 等命令来运行测试。**
4. **Meson 构建系统解析用户的命令，并调用相应的测试执行脚本，即 `frida/subprojects/frida-qml/releng/meson/mesonbuild/mtest.py`。**
5. **`mtest.py` 脚本开始执行，按照其内部逻辑进行测试用例的加载、筛选、环境设置和执行。**
6. **如果测试过程中出现错误，例如找不到测试用例、构建失败、测试执行失败等，`mtest.py` 会打印相应的错误信息到终端或日志文件。**

作为调试线索，了解 `mtest.py` 的功能可以帮助用户：

*   **理解测试执行流程:**  当测试失败时，可以分析 `mtest.py` 的执行流程，了解是在哪个阶段出错，例如是测试加载失败，还是测试执行超时。
*   **排查测试配置问题:**  检查命令行参数、测试套件配置、环境变量设置等是否正确。
*   **分析日志信息:**  查看 `mtest.py` 生成的日志文件，获取更详细的测试执行信息和错误报告。
*   **调试测试代码:**  如果怀疑是测试代码本身的问题，可以使用 `gdb` 等调试器来调试测试执行过程。

总而言之，`mtest.py` 是 Frida 测试框架的核心组件，负责管理和执行测试用例，并提供丰富的选项和功能来满足不同的测试需求。理解其功能对于开发、测试和调试 Frida Instrumentation 代码至关重要。

Prompt: 
```
这是目录为frida/subprojects/frida-qml/releng/meson/mesonbuild/mtest.py的fridaDynamic instrumentation tool的源代码文件， 请列举一下它的功能, 
如果它与逆向的方法有关系，请做出对应的举例说明，
如果涉及到二进制底层，linux, android内核及框架的知识，请做出对应的举例说明，
如果做了逻辑推理，请给出假设输入与输出,
如果涉及用户或者编程常见的使用错误，请举例说明,
说明用户操作是如何一步步的到达这里，作为调试线索。
这是第3部分，共3部分，请归纳一下它的功能

"""
metadata()

        ss = set()
        for t in self.tests:
            for s in t.suite:
                ss.add(s)
        self.suites = list(ss)

    def get_console_logger(self) -> 'ConsoleLogger':
        assert self.console_logger
        return self.console_logger

    def prepare_build(self) -> None:
        if self.options.no_rebuild:
            return

        self.ninja = environment.detect_ninja()
        if not self.ninja:
            print("Can't find ninja, can't rebuild test.")
            # If ninja can't be found return exit code 127, indicating command
            # not found for shell, which seems appropriate here. This works
            # nicely for `git bisect run`, telling it to abort - no point in
            # continuing if there's no ninja.
            sys.exit(127)

    def load_metadata(self) -> None:
        startdir = os.getcwd()
        try:
            os.chdir(self.options.wd)

            # Before loading build / test data, make sure that the build
            # configuration does not need to be regenerated. This needs to
            # happen before rebuild_deps(), because we need the correct list of
            # tests and their dependencies to compute
            if not self.options.no_rebuild:
                teststdo = subprocess.run(self.ninja + ['-n', 'build.ninja'], capture_output=True).stdout
                if b'ninja: no work to do.' not in teststdo and b'samu: nothing to do' not in teststdo:
                    stdo = sys.stderr if self.options.list else sys.stdout
                    ret = subprocess.run(self.ninja + ['build.ninja'], stdout=stdo.fileno())
                    if ret.returncode != 0:
                        raise TestException(f'Could not configure {self.options.wd!r}')

            self.build_data = build.load(os.getcwd())
            if not self.options.setup:
                self.options.setup = self.build_data.test_setup_default_name
            if self.options.benchmark:
                self.tests = self.load_tests('meson_benchmark_setup.dat')
            else:
                self.tests = self.load_tests('meson_test_setup.dat')
        finally:
            os.chdir(startdir)

    def load_tests(self, file_name: str) -> T.List[TestSerialisation]:
        datafile = Path('meson-private') / file_name
        if not datafile.is_file():
            raise TestException(f'Directory {self.options.wd!r} does not seem to be a Meson build directory.')
        with datafile.open('rb') as f:
            objs = check_testdata(pickle.load(f))
        return objs

    def __enter__(self) -> 'TestHarness':
        return self

    def __exit__(self, exc_type: T.Any, exc_value: T.Any, traceback: T.Any) -> None:
        self.close_logfiles()

    def close_logfiles(self) -> None:
        for l in self.loggers:
            l.close()
        self.console_logger = None

    def get_test_setup(self, test: T.Optional[TestSerialisation]) -> build.TestSetup:
        if ':' in self.options.setup:
            if self.options.setup not in self.build_data.test_setups:
                sys.exit(f"Unknown test setup '{self.options.setup}'.")
            return self.build_data.test_setups[self.options.setup]
        else:
            full_name = test.project_name + ":" + self.options.setup
            if full_name not in self.build_data.test_setups:
                sys.exit(f"Test setup '{self.options.setup}' not found from project '{test.project_name}'.")
            return self.build_data.test_setups[full_name]

    def merge_setup_options(self, options: argparse.Namespace, test: TestSerialisation) -> T.Dict[str, str]:
        current = self.get_test_setup(test)
        if not options.gdb:
            options.gdb = current.gdb
        if options.gdb:
            options.verbose = True
        if options.timeout_multiplier is None:
            options.timeout_multiplier = current.timeout_multiplier
    #    if options.env is None:
    #        options.env = current.env # FIXME, should probably merge options here.
        if options.wrapper is None:
            options.wrapper = current.exe_wrapper
        elif current.exe_wrapper:
            sys.exit('Conflict: both test setup and command line specify an exe wrapper.')
        return current.env.get_env(os.environ.copy())

    def get_test_runner(self, test: TestSerialisation) -> SingleTestRunner:
        name = self.get_pretty_suite(test)
        options = deepcopy(self.options)
        if self.options.setup:
            env = self.merge_setup_options(options, test)
        else:
            env = os.environ.copy()
        test_env = test.env.get_env(env)
        env.update(test_env)
        if (test.is_cross_built and test.needs_exe_wrapper and
                test.exe_wrapper and test.exe_wrapper.found()):
            env['MESON_EXE_WRAPPER'] = join_args(test.exe_wrapper.get_command())
        return SingleTestRunner(test, env, name, options)

    def process_test_result(self, result: TestRun) -> None:
        if result.res is TestResult.TIMEOUT:
            self.timeout_count += 1
        elif result.res is TestResult.SKIP:
            self.skip_count += 1
        elif result.res is TestResult.OK:
            self.success_count += 1
        elif result.res in {TestResult.FAIL, TestResult.ERROR, TestResult.INTERRUPT}:
            self.fail_count += 1
        elif result.res is TestResult.EXPECTEDFAIL:
            self.expectedfail_count += 1
        elif result.res is TestResult.UNEXPECTEDPASS:
            self.unexpectedpass_count += 1
        else:
            sys.exit(f'Unknown test result encountered: {result.res}')

        if result.res.is_bad():
            self.collected_failures.append(result)
        for l in self.loggers:
            l.log(self, result)

    @property
    def numlen(self) -> int:
        return len(str(self.test_count))

    @property
    def max_left_width(self) -> int:
        return 2 * self.numlen + 2

    def get_test_num_prefix(self, num: int) -> str:
        return '{num:{numlen}}/{testcount} '.format(numlen=self.numlen,
                                                    num=num,
                                                    testcount=self.test_count)

    def format(self, result: TestRun, colorize: bool,
               max_left_width: int = 0,
               prefix: str = '',
               left: T.Optional[str] = None,
               middle: T.Optional[str] = None,
               right: T.Optional[str] = None) -> str:
        if left is None:
            left = self.get_test_num_prefix(result.num)

        # A non-default max_left_width lets the logger print more stuff before the
        # name, while ensuring that the rightmost columns remain aligned.
        max_left_width = max(max_left_width, self.max_left_width)

        if middle is None:
            middle = result.name
        extra_mid_width = max_left_width + self.name_max_len + 1 - uniwidth(middle) - uniwidth(left) - uniwidth(prefix)
        middle += ' ' * max(1, extra_mid_width)

        if right is None:
            right = '{res} {dur:{durlen}.2f}s'.format(
                res=result.res.get_text(colorize),
                dur=result.duration,
                durlen=self.duration_max_len + 3)
            details = result.get_details()
            if details:
                right += '   ' + details
        return prefix + left + middle + right

    def summary(self) -> str:
        return textwrap.dedent('''
            Ok:                 {:<4}
            Expected Fail:      {:<4}
            Fail:               {:<4}
            Unexpected Pass:    {:<4}
            Skipped:            {:<4}
            Timeout:            {:<4}
            ''').format(self.success_count, self.expectedfail_count, self.fail_count,
                        self.unexpectedpass_count, self.skip_count, self.timeout_count)

    def total_failure_count(self) -> int:
        return self.fail_count + self.unexpectedpass_count + self.timeout_count

    def doit(self) -> int:
        if self.is_run:
            raise RuntimeError('Test harness object can only be used once.')
        self.is_run = True
        tests = self.get_tests()
        if not tests:
            return 0
        if not self.options.no_rebuild and not rebuild_deps(self.ninja, self.options.wd, tests):
            # We return 125 here in case the build failed.
            # The reason is that exit code 125 tells `git bisect run` that the current
            # commit should be skipped.  Thus users can directly use `meson test` to
            # bisect without needing to handle the does-not-build case separately in a
            # wrapper script.
            sys.exit(125)

        self.name_max_len = max(uniwidth(self.get_pretty_suite(test)) for test in tests)
        self.options.num_processes = min(self.options.num_processes,
                                         len(tests) * self.options.repeat)
        startdir = os.getcwd()
        try:
            os.chdir(self.options.wd)
            runners: T.List[SingleTestRunner] = []
            for i in range(self.options.repeat):
                runners.extend(self.get_test_runner(test) for test in tests)
                if i == 0:
                    self.duration_max_len = max(len(str(int(runner.timeout or 99)))
                                                for runner in runners)
                    # Disable the progress report if it gets in the way
                    self.need_console = any(runner.console_mode is not ConsoleUser.LOGGER
                                            for runner in runners)

            self.test_count = len(runners)
            self.run_tests(runners)
        finally:
            os.chdir(startdir)
        return self.total_failure_count()

    @staticmethod
    def split_suite_string(suite: str) -> T.Tuple[str, str]:
        if ':' in suite:
            split = suite.split(':', 1)
            assert len(split) == 2
            return split[0], split[1]
        else:
            return suite, ""

    @staticmethod
    def test_in_suites(test: TestSerialisation, suites: T.List[str]) -> bool:
        for suite in suites:
            (prj_match, st_match) = TestHarness.split_suite_string(suite)
            for prjst in test.suite:
                (prj, st) = TestHarness.split_suite_string(prjst)

                # the SUITE can be passed as
                #     suite_name
                # or
                #     project_name:suite_name
                # so we need to select only the test belonging to project_name

                # this if handle the first case (i.e., SUITE == suite_name)

                # in this way we can run tests belonging to different
                # (sub)projects which share the same suite_name
                if not st_match and st == prj_match:
                    return True

                # these two conditions are needed to handle the second option
                # i.e., SUITE == project_name:suite_name

                # in this way we select the only the tests of
                # project_name with suite_name
                if prj_match and prj != prj_match:
                    continue
                if st_match and st != st_match:
                    continue
                return True
        return False

    def test_suitable(self, test: TestSerialisation) -> bool:
        if TestHarness.test_in_suites(test, self.options.exclude_suites):
            return False

        if self.options.include_suites:
            # Both force inclusion (overriding add_test_setup) and exclude
            # everything else
            return TestHarness.test_in_suites(test, self.options.include_suites)

        if self.options.setup:
            setup = self.get_test_setup(test)
            if TestHarness.test_in_suites(test, setup.exclude_suites):
                return False

        return True

    def tests_from_args(self, tests: T.List[TestSerialisation]) -> T.Generator[TestSerialisation, None, None]:
        '''
        Allow specifying test names like "meson test foo1 foo2", where test('foo1', ...)

        Also support specifying the subproject to run tests from like
        "meson test subproj:" (all tests inside subproj) or "meson test subproj:foo1"
        to run foo1 inside subproj. Coincidentally also "meson test :foo1" to
        run all tests with that name across all subprojects, which is
        identical to "meson test foo1"
        '''
        patterns: T.Dict[T.Tuple[str, str], bool] = {}
        for arg in self.options.args:
            # Replace empty components by wildcards:
            # '' -> '*:*'
            # 'name' -> '*:name'
            # ':name' -> '*:name'
            # 'proj:' -> 'proj:*'
            if ':' in arg:
                subproj, name = arg.split(':', maxsplit=1)
                if name == '':
                    name = '*'
                if subproj == '':  # in case arg was ':'
                    subproj = '*'
            else:
                subproj, name = '*', arg
            patterns[(subproj, name)] = False

        for t in tests:
            # For each test, find the first matching pattern
            # and mark it as used. yield the matching tests.
            for subproj, name in list(patterns):
                if fnmatch(t.project_name, subproj) and fnmatch(t.name, name):
                    patterns[(subproj, name)] = True
                    yield t
                    break

        for (subproj, name), was_used in patterns.items():
            if not was_used:
                # For each unused pattern...
                arg = f'{subproj}:{name}'
                for t in tests:
                    # ... if it matches a test, then it wasn't used because another
                    # pattern matched the same test before.
                    # Report it as a warning.
                    if fnmatch(t.project_name, subproj) and fnmatch(t.name, name):
                        mlog.warning(f'{arg} test name is redundant and was not used')
                        break
                else:
                    # If the pattern doesn't match any test,
                    # report it as an error. We don't want the `test` command to
                    # succeed on an invalid pattern.
                    raise MesonException(f'{arg} test name does not match any test')

    def get_tests(self, errorfile: T.Optional[T.IO] = None) -> T.List[TestSerialisation]:
        if not self.tests:
            print('No tests defined.', file=errorfile)
            return []

        tests = [t for t in self.tests if self.test_suitable(t)]
        if self.options.args:
            tests = list(self.tests_from_args(tests))

        if not tests:
            print('No suitable tests defined.', file=errorfile)
            return []

        return tests

    def flush_logfiles(self) -> None:
        for l in self.loggers:
            l.flush()

    def open_logfiles(self) -> None:
        if not self.logfile_base:
            return

        self.loggers.append(JunitBuilder(self.logfile_base + '.junit.xml'))
        self.loggers.append(JsonLogfileBuilder(self.logfile_base + '.json'))
        self.loggers.append(TextLogfileBuilder(self.logfile_base + '.txt', errors='surrogateescape'))

    @staticmethod
    def get_wrapper(options: argparse.Namespace) -> T.List[str]:
        wrap: T.List[str] = []
        if options.gdb:
            wrap = [options.gdb_path, '--quiet']
            if options.repeat > 1:
                wrap += ['-ex', 'run', '-ex', 'quit']
            # Signal the end of arguments to gdb
            wrap += ['--args']
        if options.wrapper:
            wrap += options.wrapper
        return wrap

    def get_pretty_suite(self, test: TestSerialisation) -> str:
        if len(self.suites) > 1 and test.suite:
            rv = TestHarness.split_suite_string(test.suite[0])[0]
            s = "+".join(TestHarness.split_suite_string(s)[1] for s in test.suite)
            if s:
                rv += ":"
            return rv + s + " / " + test.name
        else:
            return test.name

    def run_tests(self, runners: T.List[SingleTestRunner]) -> None:
        try:
            self.open_logfiles()

            # TODO: this is the default for python 3.8
            if sys.platform == 'win32':
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

            asyncio.run(self._run_tests(runners))
        finally:
            self.close_logfiles()

    def log_subtest(self, test: TestRun, s: str, res: TestResult) -> None:
        for l in self.loggers:
            l.log_subtest(self, test, s, res)

    def log_start_test(self, test: TestRun) -> None:
        for l in self.loggers:
            l.start_test(self, test)

    async def _run_tests(self, runners: T.List[SingleTestRunner]) -> None:
        semaphore = asyncio.Semaphore(self.options.num_processes)
        futures: T.Deque[asyncio.Future] = deque()
        running_tests: T.Dict[asyncio.Future, str] = {}
        interrupted = False
        ctrlc_times: T.Deque[float] = deque(maxlen=MAX_CTRLC)
        loop = asyncio.get_running_loop()

        async def run_test(test: SingleTestRunner) -> None:
            async with semaphore:
                if interrupted or (self.options.repeat > 1 and self.fail_count):
                    return
                res = await test.run(self)
                self.process_test_result(res)
                maxfail = self.options.maxfail
                if maxfail and self.fail_count >= maxfail and res.res.is_bad():
                    cancel_all_tests()

        def test_done(f: asyncio.Future) -> None:
            if not f.cancelled():
                f.result()
            futures.remove(f)
            try:
                del running_tests[f]
            except KeyError:
                pass

        def cancel_one_test(warn: bool) -> None:
            future = futures.popleft()
            futures.append(future)
            if warn:
                self.flush_logfiles()
                mlog.warning('CTRL-C detected, interrupting {}'.format(running_tests[future]))
            del running_tests[future]
            future.cancel()

        def cancel_all_tests() -> None:
            nonlocal interrupted
            interrupted = True
            while running_tests:
                cancel_one_test(False)

        def sigterm_handler() -> None:
            if interrupted:
                return
            self.flush_logfiles()
            mlog.warning('Received SIGTERM, exiting')
            cancel_all_tests()

        def sigint_handler() -> None:
            # We always pick the longest-running future that has not been cancelled
            # If all the tests have been CTRL-C'ed, just stop
            nonlocal interrupted
            if interrupted:
                return
            ctrlc_times.append(loop.time())
            if len(ctrlc_times) == MAX_CTRLC and ctrlc_times[-1] - ctrlc_times[0] < 1:
                self.flush_logfiles()
                mlog.warning('CTRL-C detected, exiting')
                cancel_all_tests()
            elif running_tests:
                cancel_one_test(True)
            else:
                self.flush_logfiles()
                mlog.warning('CTRL-C detected, exiting')
                interrupted = True

        for l in self.loggers:
            l.start(self)

        if sys.platform != 'win32':
            if os.getpgid(0) == os.getpid():
                loop.add_signal_handler(signal.SIGINT, sigint_handler)
            else:
                loop.add_signal_handler(signal.SIGINT, sigterm_handler)
            loop.add_signal_handler(signal.SIGTERM, sigterm_handler)
        try:
            for runner in runners:
                if not runner.is_parallel:
                    await complete_all(futures)
                future = asyncio.ensure_future(run_test(runner))
                futures.append(future)
                running_tests[future] = runner.visible_name
                future.add_done_callback(test_done)
                if not runner.is_parallel:
                    await complete(future)
                if self.options.repeat > 1 and self.fail_count:
                    break

            await complete_all(futures)
        finally:
            if sys.platform != 'win32':
                loop.remove_signal_handler(signal.SIGINT)
                loop.remove_signal_handler(signal.SIGTERM)
            for l in self.loggers:
                await l.finish(self)

def list_tests(th: TestHarness) -> bool:
    tests = th.get_tests(errorfile=sys.stderr)
    for t in tests:
        print(th.get_pretty_suite(t))
    return not tests

def rebuild_deps(ninja: T.List[str], wd: str, tests: T.List[TestSerialisation]) -> bool:
    def convert_path_to_target(path: str) -> str:
        path = os.path.relpath(path, wd)
        if os.sep != '/':
            path = path.replace(os.sep, '/')
        return path

    assert len(ninja) > 0

    depends: T.Set[str] = set()
    targets: T.Set[str] = set()
    intro_targets: T.Dict[str, T.List[str]] = {}
    for target in load_info_file(get_infodir(wd), kind='targets'):
        intro_targets[target['id']] = [
            convert_path_to_target(f)
            for f in target['filename']]
    for t in tests:
        for d in t.depends:
            if d in depends:
                continue
            depends.update(d)
            targets.update(intro_targets[d])

    ret = subprocess.run(ninja + ['-C', wd] + sorted(targets)).returncode
    if ret != 0:
        print(f'Could not rebuild {wd}')
        return False

    return True

def run(options: argparse.Namespace) -> int:
    if options.benchmark:
        options.num_processes = 1

    if options.verbose and options.quiet:
        print('Can not be both quiet and verbose at the same time.')
        return 1

    check_bin = None
    if options.gdb:
        options.verbose = True
        if options.wrapper:
            print('Must not specify both a wrapper and gdb at the same time.')
            return 1
        check_bin = 'gdb'

    if options.wrapper:
        check_bin = options.wrapper[0]

    if check_bin is not None:
        exe = ExternalProgram(check_bin, silent=True)
        if not exe.found():
            print(f'Could not find requested program: {check_bin!r}')
            return 1

    b = build.load(options.wd)
    need_vsenv = T.cast('bool', b.environment.coredata.get_option(OptionKey('vsenv')))
    setup_vsenv(need_vsenv)

    if not options.no_rebuild:
        backend = b.environment.coredata.get_option(OptionKey('backend'))
        if backend == 'none':
            # nothing to build...
            options.no_rebuild = True
        elif backend != 'ninja':
            print('Only ninja backend is supported to rebuild tests before running them.')
            # Disable, no point in trying to build anything later
            options.no_rebuild = True

    with TestHarness(options) as th:
        try:
            if options.list:
                return list_tests(th)
            return th.doit()
        except TestException as e:
            print('Meson test encountered an error:\n')
            if os.environ.get('MESON_FORCE_BACKTRACE'):
                raise e
            else:
                print(e)
            return 1

def run_with_args(args: T.List[str]) -> int:
    parser = argparse.ArgumentParser(prog='meson test')
    add_arguments(parser)
    options = parser.parse_args(args)
    return run(options)

"""


```