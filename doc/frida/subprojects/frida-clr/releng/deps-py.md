Response:
Let's break down the thought process for analyzing this Python script.

**1. Understanding the Goal:** The request asks for a functional summary of the `deps.py` script, highlighting connections to reverse engineering, low-level details, logical reasoning, user errors, and debugging context. The "Part 1" indication suggests this is a high-level overview.

**2. Initial Scan and Keyword Recognition:**  A quick read-through reveals important keywords and structures:

*   `argparse`:  Indicates command-line interface functionality.
*   `sync`, `roll`, `build`, `wait`, `bump`: These look like the main commands the script performs.
*   `Bundle`, `MachineSpec`, `PackageSpec`: Data classes likely representing different types of dependencies, target architectures, and individual packages.
*   `urllib.request`, `tarfile`, `subprocess`:  Indicates interaction with the network, archive handling, and execution of external commands.
*   Paths and file operations:  The script clearly manages files and directories.
*   Environment variables:  References to `FRIDA_DEPS`.
*   Conditional logic (`if`, `else`, `try`, `except`):  The script makes decisions based on various factors.

**3. Deconstructing the Main Functionality (the `main()` function and its subparsers):**

*   **`sync`:** This immediately suggests synchronizing dependencies. The arguments (`bundle`, `host`, `location`) point to *what* dependency, *for what target*, and *where to put it*.
*   **`roll`:**  The name "roll" combined with `--build` and `--activate` suggests a build-and-deploy process. It likely builds dependencies if they're not already available and then potentially "activates" them (updates a configuration).
*   **`build`:** This is the core compilation step. The options `--only` and `--exclude` indicate fine-grained control over which packages are built.
*   **`wait`:** This likely involves waiting for some kind of build process to complete, possibly on a remote server.
*   **`bump`:**  A version bumping mechanism.

**4. Identifying Key Classes and Data Structures:**

*   **`Bundle`:** An `Enum` suggests a limited set of dependency types (likely SDK, Toolchain, etc.).
*   **`MachineSpec`:**  This class likely encapsulates information about target operating systems and architectures. The `make_from_local_system()` and `parse()` methods are clues.
*   **`PackageSpec`:** This likely defines the attributes of an individual dependency package (name, URL, version, dependencies, build options).

**5. Connecting to Reverse Engineering Concepts:**

*   **Prebuilt Dependencies:** The core function of the script is managing prebuilt dependencies. This is highly relevant to reverse engineering because tools like Frida often rely on specific libraries and components for different target platforms.
*   **Target Architectures:** The `MachineSpec` and the handling of different OS/arch combinations are crucial for reverse engineering, as tools need to be built for the specific system they'll be interacting with.
*   **Toolchains:** The concept of a "toolchain" (compilers, linkers, etc.) is fundamental in software development and reverse engineering. Having a way to manage different toolchains for different targets is essential.

**6. Identifying Low-Level and Kernel/Framework Connections:**

*   **Binary Artifacts:** The script downloads, extracts, and packages binary files. This is inherently low-level.
*   **Operating System Specifics:** The conditional logic based on `machine.os` (Windows, Apple, Linux) shows awareness of OS differences in build processes and file handling.
*   **`pkgconfig`:**  Mentioning `pkg_config_path` indicates interaction with a common mechanism for finding library dependencies, often used in Linux environments.

**7. Recognizing Logical Reasoning:**

*   **Dependency Resolution:** The script has logic to figure out the order in which packages need to be built based on their dependencies. The `graphlib` import confirms this.
*   **Conditional Building:** The `when` attributes in `PackageSpec` and `PackageOption` show that the script conditionally includes or excludes packages and build options.
*   **Caching:** The script checks for existing versions and avoids redownloading or rebuilding if possible.

**8. Anticipating User Errors:**

*   **Incorrect Command-line Arguments:**  The `argparse` library is used to validate input, but users could still provide incorrect bundle names or machine specifications. The `parse_bundle_option_value` function handles this specifically.
*   **Missing Dependencies/Network Issues:** The script handles `urllib.error.HTTPError`, suggesting potential network problems or missing files.
*   **Incorrect Environment Configuration:** While not explicitly shown in this snippet, misconfigured environment variables (like `FRIDA_DEPS`) could lead to problems.

**9. Tracing User Operations (Debugging Clues):**

*   The script is invoked from the command line. The `argparse` setup clearly defines the entry points and the expected arguments.
*   The file paths and directory names (e.g., `frida/subprojects/frida-clr/releng/deps.py`) give context about where this script lives within the Frida project.
*   The logging/printing statements (e.g., "Downloading SDK...", "Building...") provide feedback on the script's progress.

**10. Structuring the Summary (Iterative Process):**

The initial thoughts are often scattered. The next step is to organize them logically:

*   Start with the high-level purpose.
*   Describe the main commands and their functions.
*   Detail the key data structures and their roles.
*   Address each specific requirement of the prompt (reverse engineering, low-level details, etc.) with examples from the code.
*   Provide examples for logical reasoning, user errors, and debugging.
*   Finally, summarize the overall functionality.

**Self-Correction/Refinement:**

*   Initially, I might have just listed the commands. But the prompt asks for *functionality*. So, elaborating on what each command *does* is important.
*   I might have overlooked the significance of `graphlib`. Realizing it's for dependency resolution is a key insight.
*   The connection to reverse engineering might not be immediately obvious. Thinking about *why* Frida needs these dependencies is the crucial link.

By following these steps, breaking down the code, and actively looking for connections to the prompt's requirements, a comprehensive and accurate summary can be generated.
è¿™æ˜¯FridaåŠ¨æ€ instrumentationå·¥å…·çš„ä¸€ä¸ªPythonè„šæœ¬æ–‡ä»¶ï¼Œä½äº`frida/subprojects/frida-clr/releng/`ç›®å½•ä¸‹ï¼Œä¸»è¦ç”¨äºç®¡ç†Frida CLRæ¡¥æ¥ç»„ä»¶çš„é¢„æ„å»ºä¾èµ–ã€‚ä»¥ä¸‹æ˜¯å…¶åŠŸèƒ½çš„å½’çº³ï¼š

**æ ¸å¿ƒåŠŸèƒ½ï¼šç®¡ç†å’Œæ„å»ºé¢„æ„å»ºä¾èµ–**

`deps.py` è„šæœ¬çš„ä¸»è¦èŒè´£æ˜¯ç¡®ä¿ Frida CLR ç»„ä»¶æ‰€éœ€çš„å„ç§ä¾èµ–é¡¹ï¼ˆä¾‹å¦‚ï¼Œç¼–è¯‘å·¥å…·é“¾ã€SDKï¼‰èƒ½å¤Ÿè¢«æ­£ç¡®åœ°ä¸‹è½½ã€æ„å»ºå’Œç®¡ç†ã€‚å®ƒæä¾›äº†ä¸€ç»„å‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºï¼š

1. **åŒæ­¥ (sync):**  ä¸‹è½½æˆ–æ›´æ–°ç‰¹å®šç›®æ ‡å¹³å°ï¼ˆ`host`ï¼‰çš„é¢„æ„å»ºä¾èµ–åŒ…ï¼ˆ`bundle`ï¼Œå¦‚ SDK æˆ–å·¥å…·é“¾ï¼‰åˆ°æŒ‡å®šçš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä½ç½®ï¼ˆ`location`ï¼‰ã€‚å¦‚æœæœ¬åœ°å·²å­˜åœ¨æ—§ç‰ˆæœ¬ï¼Œåˆ™ä¼šåˆ é™¤å¹¶é‡æ–°ä¸‹è½½ã€‚

2. **æ»šåŠ¨ (roll):**  è´Ÿè´£æ„å»ºå¹¶ä¸Šä¼ é¢„æ„å»ºçš„ä¾èµ–é¡¹ã€‚å®ƒä¼šæ£€æŸ¥æŒ‡å®šç›®æ ‡å¹³å°ï¼ˆ`host`ï¼‰çš„ä¾èµ–åŒ…æ˜¯å¦å·²å­˜åœ¨äºè¿œç¨‹ä»“åº“ï¼ˆS3ï¼‰ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä¼šè§¦å‘æ„å»ºï¼ˆåœ¨`build`å‚æ•°æŒ‡å®šçš„å¹³å°ï¼‰ï¼Œç„¶åä¸Šä¼ åˆ°è¿œç¨‹ä»“åº“ï¼Œå¹¶å¯èƒ½æ‰§è¡Œä¸€äº›åå¤„ç†è„šæœ¬ã€‚

3. **æ„å»º (build):**  å®é™…æ‰§è¡Œé¢„æ„å»ºä¾èµ–çš„ç¼–è¯‘è¿‡ç¨‹ã€‚å¯ä»¥æŒ‡å®šè¦æ„å»ºçš„ä¾èµ–åŒ…ç±»å‹ (`--bundle`)ï¼Œæ„å»ºå¹³å° (`--build`) å’Œç›®æ ‡å¹³å° (`--host`)ã€‚ è¿˜å¯ä»¥é€‰æ‹©åªæ„å»ºæˆ–æ’é™¤ç‰¹å®šçš„è½¯ä»¶åŒ…ã€‚

4. **ç­‰å¾… (wait):**  å¯èƒ½ç”¨äºç­‰å¾…æŸä¸ªé¢„æ„å»ºä¾èµ–é¡¹å¯ç”¨ã€‚

5. **ç‰ˆæœ¬é€’å¢ (bump):**  ç”¨äºæ›´æ–°ä¾èµ–é¡¹çš„ç‰ˆæœ¬å·ã€‚

**ä¸é€†å‘æ–¹æ³•çš„å…³ç³»åŠä¸¾ä¾‹è¯´æ˜ï¼š**

*   **ä¾èµ–ç®¡ç†ï¼š** Frida ä½œä¸ºä¸€æ¬¾åŠ¨æ€æ’æ¡©å·¥å…·ï¼Œéœ€è¦ä¸ç›®æ ‡è¿›ç¨‹çš„è¿è¡Œæ—¶ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚å¯¹äº .NET åº”ç”¨ç¨‹åºï¼ŒFrida CLR éœ€è¦ä¸ CLR è¿è¡Œæ—¶ç¯å¢ƒäº¤äº’ã€‚è¿™äº›äº¤äº’é€šå¸¸ä¾èµ–äºç‰¹å®šçš„åº“å’Œå¤´æ–‡ä»¶ã€‚`deps.py` ç¡®ä¿äº†æ„å»º Frida CLR æ‰€éœ€çš„è¿™äº›ä¾èµ–é¡¹æ˜¯å¯ç”¨çš„ä¸”ç‰ˆæœ¬æ­£ç¡®ã€‚
    *   **ä¸¾ä¾‹ï¼š** åœ¨é€†å‘ä¸€ä¸ªä½¿ç”¨ç‰¹å®š .NET Framework ç‰ˆæœ¬çš„åº”ç”¨ç¨‹åºæ—¶ï¼ŒFrida CLR å¯èƒ½éœ€è¦é’ˆå¯¹è¯¥ç‰ˆæœ¬ç¼–è¯‘çš„ä¾èµ–é¡¹æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚`deps.py` å¯ä»¥ç”¨æ¥ä¸‹è½½æˆ–æ„å»ºä¸è¯¥ .NET Framework ç‰ˆæœ¬ç›¸åŒ¹é…çš„ä¾èµ–ã€‚

*   **ç›®æ ‡å¹³å°æ”¯æŒï¼š** é€†å‘å·¥ä½œç»å¸¸éœ€è¦åœ¨ä¸åŒçš„æ“ä½œç³»ç»Ÿå’Œæ¶æ„ä¸Šè¿›è¡Œã€‚`deps.py` å…è®¸æŒ‡å®šç›®æ ‡å¹³å° (`host`) å’Œæ„å»ºå¹³å° (`build`)ï¼Œè¿™ä½¿å¾— Frida CLR èƒ½å¤Ÿè¢«æ„å»ºæˆé€‚åº”ä¸åŒçš„ç›®æ ‡ç¯å¢ƒã€‚
    *   **ä¸¾ä¾‹ï¼š**  å¦‚æœéœ€è¦åœ¨ Android è®¾å¤‡ä¸Šé€†å‘ä¸€ä¸ª Unity æ¸¸æˆï¼ˆä½¿ç”¨ IL2CPPï¼‰ï¼Œåˆ™éœ€è¦ä¸º Android æ¶æ„æ„å»º Frida CLR çš„ä¾èµ–é¡¹ã€‚å¯ä»¥é€šè¿‡ `deps.py` æŒ‡å®š Android å¹³å°æ¥ä¸‹è½½æˆ–æ„å»ºç›¸åº”çš„ä¾èµ–ã€‚

**æ¶‰åŠäºŒè¿›åˆ¶åº•å±‚ã€Linuxã€Androidå†…æ ¸åŠæ¡†æ¶çš„çŸ¥è¯†åŠä¸¾ä¾‹è¯´æ˜ï¼š**

*   **å·¥å…·é“¾ç®¡ç†ï¼š** è„šæœ¬ä¸­æ¶‰åŠ "toolchain" çš„æ¦‚å¿µï¼Œè¿™æŒ‡çš„æ˜¯ç¼–è¯‘å’Œé“¾æ¥ä»£ç æ‰€éœ€è¦çš„å·¥å…·é›†åˆï¼Œä¾‹å¦‚ç¼–è¯‘å™¨ (gcc, clang)ã€é“¾æ¥å™¨ (ld) ç­‰ã€‚ä¸åŒå¹³å°éœ€è¦ä¸åŒçš„å·¥å…·é“¾ã€‚
    *   **ä¸¾ä¾‹ï¼š**  ä¸º Android æ„å»ºä¾èµ–é¡¹å¯èƒ½éœ€è¦ Android NDK (Native Development Kit) ä¸­çš„å·¥å…·é“¾ã€‚`deps.py` è´Ÿè´£ä¸‹è½½å’Œç®¡ç†è¿™äº›å·¥å…·é“¾ã€‚

*   **SDK ç®¡ç†ï¼š**  è„šæœ¬ä¸­ä¹Ÿæ¶‰åŠ "SDK" (Software Development Kit) çš„æ¦‚å¿µã€‚å¯¹äºä¸åŒçš„å¹³å°ï¼ŒSDK åŒ…å«äº†å¼€å‘æ‰€éœ€çš„åº“ã€å¤´æ–‡ä»¶ç­‰ã€‚
    *   **ä¸¾ä¾‹ï¼š**  ä¸º Android æ„å»ºå¯èƒ½éœ€è¦ Android SDK ä¸­çš„ç‰¹å®šåº“æ–‡ä»¶ã€‚`deps.py` è´Ÿè´£ä¸‹è½½å’Œç®¡ç†è¿™äº› SDKã€‚

*   **å¹³å°ç‰¹å®šçš„æ„å»ºé€‰é¡¹ï¼š**  è„šæœ¬ä¸­å¯èƒ½ä¼šæ ¹æ®ç›®æ ‡å¹³å°çš„ä¸åŒè®¾ç½®ä¸åŒçš„ç¼–è¯‘é€‰é¡¹ã€‚ä¾‹å¦‚ï¼ŒWindows å’Œ Linux çš„ç¼–è¯‘è¿‡ç¨‹å’Œåº“æ–‡ä»¶æ ¼å¼æœ‰æ‰€ä¸åŒã€‚
    *   **ä¸¾ä¾‹ï¼š**  åœ¨ä¸º Windows æ„å»ºæ—¶ï¼Œå¯èƒ½éœ€è¦å¤„ç† DLL æ–‡ä»¶çš„ç”Ÿæˆå’Œé“¾æ¥ï¼›åœ¨ Linux ä¸Šï¼Œå¯èƒ½éœ€è¦å¤„ç†å…±äº«åº“ (.so) çš„ç”Ÿæˆã€‚

*   **ä¸æ„å»ºç³»ç»Ÿçš„äº¤äº’ (Meson):**  è„šæœ¬ä¸­è°ƒç”¨äº† `env.call_meson`ï¼Œè¿™è¡¨æ˜ Frida CLR çš„æ„å»ºç³»ç»Ÿä½¿ç”¨äº† Mesonã€‚Meson æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„æ„å»ºç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„å¹³å°ç”Ÿæˆç›¸åº”çš„æ„å»ºæ–‡ä»¶ï¼ˆå¦‚ Ninja æ„å»ºæ–‡ä»¶ï¼‰ã€‚
    *   **ä¸¾ä¾‹ï¼š**  `deps.py` ä½¿ç”¨ Meson æ¥é…ç½®æ„å»ºè¿‡ç¨‹ï¼ŒæŒ‡å®šç¼–è¯‘é€‰é¡¹ã€ä¾èµ–é¡¹è·¯å¾„ç­‰ã€‚Meson ä¼šæ ¹æ®ç›®æ ‡å¹³å°ç”Ÿæˆå¯¹åº”çš„æ„å»ºæŒ‡ä»¤ã€‚

**é€»è¾‘æ¨ç†çš„å‡è®¾è¾“å…¥ä¸è¾“å‡ºï¼š**

*   **å‡è®¾è¾“å…¥:** ç”¨æˆ·æ‰§è¡Œå‘½ä»¤ `python deps.py sync sdk windows-x86_64 ./my_deps`
*   **é€»è¾‘æ¨ç†:** è„šæœ¬ä¼šè§£æå‘½ä»¤ï¼Œè¯†åˆ«å‡ºè¦åŒæ­¥çš„æ˜¯ SDK (`bundle=Bundle.SDK`)ï¼Œç›®æ ‡å¹³å°æ˜¯ Windows 64 ä½ (`host=MachineSpec(os='windows', arch='x86_64', ...)`ï¼‰ï¼Œæœ¬åœ°è·¯å¾„æ˜¯ `./my_deps`ã€‚ç„¶åï¼Œå®ƒä¼šæŸ¥æ‰¾ä¸ Windows 64 ä½ SDK å¯¹åº”çš„è¿œç¨‹åŒ… URL å’Œæ–‡ä»¶åï¼Œå¹¶å°è¯•ä¸‹è½½åˆ° `./my_deps` ç›®å½•ã€‚
*   **å‡è®¾è¾“å‡º:** å¦‚æœè¿œç¨‹ä»“åº“å­˜åœ¨å¯¹åº”çš„ SDK åŒ…ï¼Œåˆ™ä¼šä¸‹è½½å¹¶è§£å‹åˆ° `./my_deps` ç›®å½•ã€‚å¦‚æœæœ¬åœ°å·²å­˜åœ¨æ—§ç‰ˆæœ¬ï¼Œåˆ™å…ˆåˆ é™¤æ—§ç‰ˆæœ¬ã€‚å¦‚æœä¸‹è½½å¤±è´¥ï¼ˆä¾‹å¦‚ 404 é”™è¯¯ï¼‰ï¼Œåˆ™ä¼šæŠ›å‡º `BundleNotFoundError` å¼‚å¸¸ã€‚

**ç”¨æˆ·æˆ–ç¼–ç¨‹å¸¸è§çš„ä½¿ç”¨é”™è¯¯åŠä¸¾ä¾‹è¯´æ˜ï¼š**

*   **é”™è¯¯çš„ bundle åç§°:** ç”¨æˆ·å¯èƒ½ä¼šè¾“å…¥é”™è¯¯çš„ bundle åç§°ï¼Œä¾‹å¦‚ `python deps.py sync ksd ...`ï¼Œç”±äº `ksd` ä¸æ˜¯æœ‰æ•ˆçš„ `Bundle` æšä¸¾å€¼ï¼Œè„šæœ¬ä¼šæŠ›å‡º `argparse.ArgumentTypeError`ã€‚
*   **é”™è¯¯çš„ host å¹³å°æ ‡è¯†:** ç”¨æˆ·å¯èƒ½ä¼šè¾“å…¥æ— æ³•è§£æçš„å¹³å°æ ‡è¯†ï¼Œä¾‹å¦‚ `python deps.py sync sdk wimdows-x86 ...`ï¼Œ`MachineSpec.parse` å‡½æ•°ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚
*   **ç½‘ç»œé—®é¢˜:** åœ¨ `sync` æˆ– `roll` å‘½ä»¤ä¸­ï¼Œå¦‚æœæ— æ³•è¿æ¥åˆ°è¿œç¨‹ä»“åº“æˆ–ä¸‹è½½æ–‡ä»¶æ—¶ï¼Œä¼šæŠ›å‡º `urllib.error.HTTPError` å¼‚å¸¸ã€‚
*   **æƒé™é—®é¢˜:** å¦‚æœæŒ‡å®šçš„æœ¬åœ°è·¯å¾„æ²¡æœ‰å†™å…¥æƒé™ï¼Œè„šæœ¬åœ¨å°è¯•åˆ›å»ºæˆ–å†™å…¥æ–‡ä»¶æ—¶ä¼šé‡åˆ° `PermissionError`ã€‚

**ç”¨æˆ·æ“ä½œå¦‚ä½•ä¸€æ­¥æ­¥åˆ°è¾¾è¿™é‡Œï¼Œä½œä¸ºè°ƒè¯•çº¿ç´¢ï¼š**

1. **å¼€å‘æˆ–è´¡çŒ® Frida CLR:**  ç”¨æˆ·å¯èƒ½æ­£åœ¨å°è¯•æ„å»ºæˆ–ä¿®æ”¹ Frida CLR çš„ä»£ç ã€‚
2. **æŸ¥é˜…æ„å»ºæ–‡æ¡£:**  Frida CLR çš„æ„å»ºæ–‡æ¡£å¯èƒ½ä¼šæŒ‡ç¤ºç”¨æˆ·è¿è¡Œ `deps.py` è„šæœ¬æ¥å‡†å¤‡æ„å»ºç¯å¢ƒã€‚
3. **æ‰§è¡Œæ„å»ºå‘½ä»¤:** ç”¨æˆ·æ ¹æ®æ–‡æ¡£æŒ‡ç¤ºï¼Œæ‰§è¡Œç±»ä¼¼äº `python deps.py sync sdk linux-x86_64 ./deps` è¿™æ ·çš„å‘½ä»¤ã€‚
4. **é‡åˆ°æ„å»ºé”™è¯¯:** å¦‚æœåœ¨æ„å»ºè¿‡ç¨‹ä¸­é‡åˆ°ä¾èµ–é¡¹ç¼ºå¤±æˆ–ç‰ˆæœ¬ä¸åŒ¹é…çš„é—®é¢˜ï¼Œç”¨æˆ·å¯èƒ½ä¼šæ£€æŸ¥ `deps.py` çš„æ‰§è¡Œæƒ…å†µï¼ŒæŸ¥çœ‹æ˜¯å¦æˆåŠŸä¸‹è½½å’Œå®‰è£…äº†æ‰€éœ€çš„ä¾èµ–ã€‚
5. **è°ƒè¯• `deps.py`:** ç”¨æˆ·å¯èƒ½ä¼šé˜…è¯» `deps.py` çš„æºä»£ç ï¼Œäº†è§£å…¶å·¥ä½œåŸç†ï¼Œæˆ–è€…åœ¨è„šæœ¬ä¸­æ·»åŠ æ‰“å°è¯­å¥æ¥è°ƒè¯•é—®é¢˜ï¼Œä¾‹å¦‚æ£€æŸ¥ä¸‹è½½çš„ URLã€è§£å‹çš„è·¯å¾„ç­‰ã€‚

**å½’çº³ä¸€ä¸‹å®ƒçš„åŠŸèƒ½ (Part 1):**

`deps.py` æ˜¯ Frida CLR é¡¹ç›®ä¸­ç”¨äºç®¡ç†é¢„æ„å»ºä¾èµ–é¡¹çš„å…³é”®è„šæœ¬ã€‚å®ƒæä¾›äº†ä¸€ç»„å‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºåŒæ­¥ã€æ„å»ºå’Œç®¡ç†ä¸åŒç›®æ ‡å¹³å°çš„ä¾èµ–åŒ…ï¼ˆå¦‚ SDK å’Œå·¥å…·é“¾ï¼‰ã€‚è¿™ç¡®ä¿äº† Frida CLR èƒ½å¤Ÿæ­£ç¡®åœ°æ„å»ºå’Œè¿è¡Œåœ¨å„ç§æ“ä½œç³»ç»Ÿå’Œæ¶æ„ä¸Šï¼Œè¿™å¯¹äºåŠ¨æ€æ’æ¡©å’Œé€†å‘å·¥ç¨‹è‡³å…³é‡è¦ã€‚è¯¥è„šæœ¬æ¶‰åŠå¯¹ä¸åŒå¹³å°æ„å»ºå·¥å…·é“¾å’Œ SDK çš„ç®¡ç†ï¼Œå¹¶åˆ©ç”¨ Meson æ„å»ºç³»ç»Ÿè¿›è¡Œé…ç½®å’Œç¼–è¯‘ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œæ“ä½œæ¥ç®¡ç†è¿™äº›ä¾èµ–ï¼Œä½†å¸¸è§çš„é”™è¯¯åŒ…æ‹¬è¾“å…¥é”™è¯¯çš„å‚æ•°æˆ–é‡åˆ°ç½‘ç»œé—®é¢˜ã€‚ä½œä¸ºè°ƒè¯•çº¿ç´¢ï¼Œç†è§£ `deps.py` çš„å·¥ä½œæµç¨‹æœ‰åŠ©äºè¯Šæ–­ Frida CLR æ„å»ºè¿‡ç¨‹ä¸­é‡åˆ°çš„ä¾èµ–é—®é¢˜ã€‚

### æç¤ºè¯
```
è¿™æ˜¯ç›®å½•ä¸ºfrida/subprojects/frida-clr/releng/deps.pyçš„fridaDynamic instrumentation toolçš„æºä»£ç æ–‡ä»¶ï¼Œ è¯·åˆ—ä¸¾ä¸€ä¸‹å®ƒçš„åŠŸèƒ½, 
å¦‚æœå®ƒä¸é€†å‘çš„æ–¹æ³•æœ‰å…³ç³»ï¼Œè¯·åšå‡ºå¯¹åº”çš„ä¸¾ä¾‹è¯´æ˜ï¼Œ
å¦‚æœæ¶‰åŠåˆ°äºŒè¿›åˆ¶åº•å±‚ï¼Œlinux, androidå†…æ ¸åŠæ¡†æ¶çš„çŸ¥è¯†ï¼Œè¯·åšå‡ºå¯¹åº”çš„ä¸¾ä¾‹è¯´æ˜ï¼Œ
å¦‚æœåšäº†é€»è¾‘æ¨ç†ï¼Œè¯·ç»™å‡ºå‡è®¾è¾“å…¥ä¸è¾“å‡º,
å¦‚æœæ¶‰åŠç”¨æˆ·æˆ–è€…ç¼–ç¨‹å¸¸è§çš„ä½¿ç”¨é”™è¯¯ï¼Œè¯·ä¸¾ä¾‹è¯´æ˜,
è¯´æ˜ç”¨æˆ·æ“ä½œæ˜¯å¦‚ä½•ä¸€æ­¥æ­¥çš„åˆ°è¾¾è¿™é‡Œï¼Œä½œä¸ºè°ƒè¯•çº¿ç´¢ã€‚
è¿™æ˜¯ç¬¬1éƒ¨åˆ†ï¼Œå…±2éƒ¨åˆ†ï¼Œè¯·å½’çº³ä¸€ä¸‹å®ƒçš„åŠŸèƒ½
```

### æºä»£ç 
```python
#!/usr/bin/env python3
from __future__ import annotations
import argparse
import base64
from configparser import ConfigParser
import dataclasses
from dataclasses import dataclass, field
from enum import Enum
import graphlib
import itertools
import json
import os
from pathlib import Path
import re
import shlex
import shutil
import subprocess
import sys
import tarfile
import tempfile
import time
from typing import Callable, Iterator, Optional, Mapping, Sequence, Union
import urllib.request

RELENG_DIR = Path(__file__).resolve().parent
ROOT_DIR = RELENG_DIR.parent

if __name__ == "__main__":
    # TODO: Refactor
    sys.path.insert(0, str(ROOT_DIR))
sys.path.insert(0, str(RELENG_DIR / "tomlkit"))

from tomlkit.toml_file import TOMLFile

from releng import env
from releng.progress import Progress, ProgressCallback, print_progress
from releng.machine_spec import MachineSpec


def main():
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers()

    default_machine = MachineSpec.make_from_local_system().identifier

    bundle_opt_kwargs = {
        "help": "bundle (default: sdk)",
        "type": parse_bundle_option_value,
    }
    machine_opt_kwargs = {
        "help": f"os/arch (default: {default_machine})",
        "type": MachineSpec.parse,
    }

    command = subparsers.add_parser("sync", help="ensure prebuilt dependencies are up-to-date")
    command.add_argument("bundle", **bundle_opt_kwargs)
    command.add_argument("host", **machine_opt_kwargs)
    command.add_argument("location", help="filesystem location", type=Path)
    command.set_defaults(func=lambda args: sync(args.bundle, args.host, args.location.resolve()))

    command = subparsers.add_parser("roll", help="build and upload prebuilt dependencies if needed")
    command.add_argument("bundle", **bundle_opt_kwargs)
    command.add_argument("host", **machine_opt_kwargs)
    command.add_argument("--build", default=default_machine, **machine_opt_kwargs)
    command.add_argument("--activate", default=False, action='store_true')
    command.add_argument("--post", help="post-processing script")
    command.set_defaults(func=lambda args: roll(args.bundle, args.build, args.host, args.activate,
                                                Path(args.post) if args.post is not None else None))

    command = subparsers.add_parser("build", help="build prebuilt dependencies")
    command.add_argument("--bundle", default=Bundle.SDK, **bundle_opt_kwargs)
    command.add_argument("--build", default=default_machine, **machine_opt_kwargs)
    command.add_argument("--host", default=default_machine, **machine_opt_kwargs)
    command.add_argument("--only", help="only build packages A, B, and C", metavar="A,B,C",
                         type=parse_set_option_value)
    command.add_argument("--exclude", help="exclude packages A, B, and C", metavar="A,B,C",
                         type=parse_set_option_value, default=set())
    command.add_argument("-v", "--verbose", help="be verbose", action="store_true")
    command.set_defaults(func=lambda args: build(args.bundle, args.build, args.host,
                                                 args.only, args.exclude, args.verbose))

    command = subparsers.add_parser("wait", help="wait for prebuilt dependencies if needed")
    command.add_argument("bundle", **bundle_opt_kwargs)
    command.add_argument("host", **machine_opt_kwargs)
    command.set_defaults(func=lambda args: wait(args.bundle, args.host))

    command = subparsers.add_parser("bump", help="bump dependency versions")
    command.set_defaults(func=lambda args: bump())

    args = parser.parse_args()
    if 'func' in args:
        try:
            args.func(args)
        except CommandError as e:
            print(e, file=sys.stderr)
            sys.exit(1)
    else:
        parser.print_usage(file=sys.stderr)
        sys.exit(1)


def parse_bundle_option_value(raw_bundle: str) -> Bundle:
    try:
        return Bundle[raw_bundle.upper()]
    except KeyError:
        choices = "', '".join([e.name.lower() for e in Bundle])
        raise argparse.ArgumentTypeError(f"invalid choice: {raw_bundle} (choose from '{choices}')")


def parse_set_option_value(v: str) -> set[str]:
    return set([v.strip() for v in v.split(",")])


def query_toolchain_prefix(machine: MachineSpec,
                           cache_dir: Path) -> Path:
    if machine.os == "windows":
        identifier = "windows-x86" if machine.arch in {"x86", "x86_64"} else machine.os_dash_arch
    else:
        identifier = machine.identifier
    return cache_dir / f"toolchain-{identifier}"


def ensure_toolchain(machine: MachineSpec,
                     cache_dir: Path,
                     version: Optional[str] = None,
                     on_progress: ProgressCallback = print_progress) -> tuple[Path, SourceState]:
    toolchain_prefix = query_toolchain_prefix(machine, cache_dir)
    state = sync(Bundle.TOOLCHAIN, machine, toolchain_prefix, version, on_progress)
    return (toolchain_prefix, state)


def query_sdk_prefix(machine: MachineSpec,
                     cache_dir: Path) -> Path:
    return cache_dir / f"sdk-{machine.identifier}"


def ensure_sdk(machine: MachineSpec,
               cache_dir: Path,
               version: Optional[str] = None,
               on_progress: ProgressCallback = print_progress) -> tuple[Path, SourceState]:
    sdk_prefix = query_sdk_prefix(machine, cache_dir)
    state = sync(Bundle.SDK, machine, sdk_prefix, version, on_progress)
    return (sdk_prefix, state)


def detect_cache_dir(sourcedir: Path) -> Path:
    raw_location = os.environ.get("FRIDA_DEPS", None)
    if raw_location is not None:
        location = Path(raw_location)
    else:
        location = sourcedir / "deps"
    return location


def sync(bundle: Bundle,
         machine: MachineSpec,
         location: Path,
         version: Optional[str] = None,
         on_progress: ProgressCallback = print_progress) -> SourceState:
    state = SourceState.PRISTINE

    if version is None:
        version = load_dependency_parameters().deps_version

    bundle_nick = bundle.name.lower() if bundle != Bundle.SDK else bundle.name

    if location.exists():
        try:
            cached_version = (location / "VERSION.txt").read_text(encoding="utf-8").strip()
            if cached_version == version:
                return state
        except:
            pass
        shutil.rmtree(location)
        state = SourceState.MODIFIED

    (url, filename) = compute_bundle_parameters(bundle, machine, version)

    local_bundle = location.parent / filename
    if local_bundle.exists():
        on_progress(Progress("Deploying local {}".format(bundle_nick)))
        archive_path = local_bundle
        archive_is_temporary = False
    else:
        if bundle == Bundle.SDK:
            on_progress(Progress(f"Downloading SDK {version} for {machine.identifier}"))
        else:
            on_progress(Progress(f"Downloading {bundle_nick} {version}"))
        try:
            with urllib.request.urlopen(url) as response, \
                    tempfile.NamedTemporaryFile(delete=False) as archive:
                shutil.copyfileobj(response, archive)
                archive_path = Path(archive.name)
                archive_is_temporary = True
            on_progress(Progress(f"Extracting {bundle_nick}"))
        except urllib.error.HTTPError as e:
            if e.code == 404:
                raise BundleNotFoundError(f"missing bundle at {url}") from e
            raise e

    try:
        staging_dir = location.parent / f"_{location.name}"
        if staging_dir.exists():
            shutil.rmtree(staging_dir)
        staging_dir.mkdir(parents=True)

        with tarfile.open(archive_path, "r:xz") as tar:
            tar.extractall(staging_dir)

        suffix_len = len(".frida.in")
        raw_location = location.as_posix()
        for f in staging_dir.rglob("*.frida.in"):
            target = f.parent / f.name[:-suffix_len]
            f.write_text(f.read_text(encoding="utf-8").replace("@FRIDA_TOOLROOT@", raw_location),
                         encoding="utf-8")
            f.rename(target)

        staging_dir.rename(location)
    finally:
        if archive_is_temporary:
            archive_path.unlink()

    return state


def roll(bundle: Bundle,
         build_machine: MachineSpec,
         host_machine: MachineSpec,
         activate: bool,
         post: Optional[Path]):
    params = load_dependency_parameters()
    version = params.deps_version

    if activate and bundle == Bundle.SDK:
        configure_bootstrap_version(version)

    (public_url, filename) = compute_bundle_parameters(bundle, host_machine, version)

    # First do a quick check to avoid hitting S3 in most cases.
    request = urllib.request.Request(public_url)
    request.get_method = lambda: "HEAD"
    try:
        with urllib.request.urlopen(request) as r:
            return
    except urllib.request.HTTPError as e:
        if e.code != 404:
            raise CommandError("network error") from e

    s3_url = "s3://build.frida.re/deps/{version}/{filename}".format(version=version, filename=filename)

    # We will most likely need to build, but let's check S3 to be certain.
    r = subprocess.run(["aws", "s3", "ls", s3_url], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, encoding="utf-8")
    if r.returncode == 0:
        return
    if r.returncode != 1:
        raise CommandError(f"unable to access S3: {r.stdout.strip()}")

    artifact = build(bundle, build_machine, host_machine)

    if post is not None:
        post_script = RELENG_DIR / post
        if not post_script.exists():
            raise CommandError("post-processing script not found")

        subprocess.run([
                           sys.executable, post_script,
                           "--bundle=" + bundle.name.lower(),
                           "--host=" + host_machine.identifier,
                           "--artifact=" + str(artifact),
                           "--version=" + version,
                       ],
                       check=True)

    subprocess.run(["aws", "s3", "cp", artifact, s3_url], check=True)

    # Use the shell for Windows compatibility, where npm generates a .bat script.
    subprocess.run("cfcli purge " + public_url, shell=True, check=True)

    if activate and bundle == Bundle.TOOLCHAIN:
        configure_bootstrap_version(version)


def build(bundle: Bundle,
          build_machine: MachineSpec,
          host_machine: MachineSpec,
          only_packages: Optional[set[str]] = None,
          excluded_packages: set[str] = set(),
          verbose: bool = False) -> Path:
    builder = Builder(bundle, build_machine, host_machine, verbose)
    try:
        return builder.build(only_packages, excluded_packages)
    except subprocess.CalledProcessError as e:
        print(e, file=sys.stderr)
        if e.stdout is not None:
            print("\n=== stdout ===\n" + e.stdout, file=sys.stderr)
        if e.stderr is not None:
            print("\n=== stderr ===\n" + e.stderr, file=sys.stderr)
        sys.exit(1)


class Builder:
    def __init__(self,
                 bundle: Bundle,
                 build_machine: MachineSpec,
                 host_machine: MachineSpec,
                 verbose: bool):
        self._bundle = bundle
        self._host_machine = host_machine.default_missing()
        self._build_machine = build_machine.default_missing().maybe_adapt_to_host(self._host_machine)
        self._verbose = verbose
        self._default_library = "static"

        self._params = load_dependency_parameters()
        self._cachedir = detect_cache_dir(ROOT_DIR)
        self._workdir = self._cachedir / "src"

        self._toolchain_prefix: Optional[Path] = None
        self._build_config: Optional[env.MachineConfig] = None
        self._host_config: Optional[env.MachineConfig] = None
        self._build_env: dict[str, str] = {}
        self._host_env: dict[str, str] = {}

        self._ansi_supported = os.environ.get("TERM") != "dumb" \
                    and (self._build_machine.os != "windows" or "WT_SESSION" in os.environ)

    def build(self,
              only_packages: Optional[list[str]],
              excluded_packages: set[str]) -> Path:
        started_at = time.time()
        prepare_ended_at = None
        clone_time_elapsed = None
        build_time_elapsed = None
        build_ended_at = None
        packaging_ended_at = None
        try:
            all_packages = {i: self._resolve_package(p) for i, p in self._params.packages.items() \
                    if self._can_build(p)}
            if only_packages is not None:
                toplevel_packages = [all_packages[identifier] for identifier in only_packages]
                selected_packages = self._resolve_dependencies(toplevel_packages, all_packages)
            elif self._bundle is Bundle.TOOLCHAIN:
                toplevel_packages = [p for p in all_packages.values() if p.scope == "toolchain"]
                selected_packages = self._resolve_dependencies(toplevel_packages, all_packages)
            else:
                selected_packages = {i: p for i, p, in all_packages.items() if p.scope is None}
            selected_packages = {i: p for i, p in selected_packages.items() if i not in excluded_packages}

            packages = [selected_packages[i] for i in iterate_package_ids_in_dependency_order(selected_packages.values())]
            all_deps = itertools.chain.from_iterable([pkg.dependencies for pkg in packages])
            deps_for_build_machine = {dep.identifier for dep in all_deps if dep.for_machine == "build"}

            self._prepare()
            prepare_ended_at = time.time()

            clone_time_elapsed = 0
            build_time_elapsed = 0
            for pkg in packages:
                self._print_package_banner(pkg)

                t1 = time.time()
                self._clone_repo_if_needed(pkg)
                t2 = time.time()
                clone_time_elapsed += t2 - t1

                machines = [self._host_machine]
                if pkg.identifier in deps_for_build_machine:
                    machines += [self._build_machine]
                self._build_package(pkg, machines)
                t3 = time.time()
                build_time_elapsed += t3 - t2
            build_ended_at = time.time()

            artifact_file = self._package()
            packaging_ended_at = time.time()
        finally:
            ended_at = time.time()

            if prepare_ended_at is not None:
                self._print_summary_banner()
                print("      Total: {}".format(format_duration(ended_at - started_at)))

            if prepare_ended_at is not None:
                print("    Prepare: {}".format(format_duration(prepare_ended_at - started_at)))

            if clone_time_elapsed is not None:
                print("      Clone: {}".format(format_duration(clone_time_elapsed)))

            if build_time_elapsed is not None:
                print("      Build: {}".format(format_duration(build_time_elapsed)))

            if packaging_ended_at is not None:
                print("  Packaging: {}".format(format_duration(packaging_ended_at - build_ended_at)))

            print("", flush=True)

        return artifact_file

    def _can_build(self, pkg: PackageSpec) -> bool:
        return self._evaluate_condition(pkg.when)

    def _resolve_package(self, pkg: PackageSpec) -> bool:
        resolved_opts = [opt for opt in pkg.options if self._evaluate_condition(opt.when)]
        resolved_deps = [dep for dep in pkg.dependencies if self._evaluate_condition(dep.when)]
        return dataclasses.replace(pkg,
                                   options=resolved_opts,
                                   dependencies=resolved_deps)

    def _resolve_dependencies(self,
                              packages: Sequence[PackageSpec],
                              all_packages: Mapping[str, PackageSpec]) -> dict[str, PackageSpec]:
        result = {p.identifier: p for p in packages}
        for p in packages:
            self._resolve_package_dependencies(p, all_packages, result)
        return result

    def _resolve_package_dependencies(self,
                                      package: PackageSpec,
                                      all_packages: Mapping[str, PackageSpec],
                                      resolved_packages: Mapping[str, PackageSpec]):
        for dep in package.dependencies:
            identifier = dep.identifier
            if identifier in resolved_packages:
                continue
            p = all_packages[identifier]
            resolved_packages[identifier] = p
            self._resolve_package_dependencies(p, all_packages, resolved_packages)

    def _evaluate_condition(self, cond: Optional[str]) -> bool:
        if cond is None:
            return True
        global_vars = {
            "Bundle": Bundle,
            "bundle": self._bundle,
            "machine": self._host_machine,
        }
        return eval(cond, global_vars)

    def _prepare(self):
        self._toolchain_prefix, toolchain_state = \
                ensure_toolchain(self._build_machine,
                                 self._cachedir,
                                 version=self._params.bootstrap_version)
        if toolchain_state == SourceState.MODIFIED:
            self._wipe_build_state()

        envdir = self._get_builddir_container()
        envdir.mkdir(parents=True, exist_ok=True)

        menv = {**os.environ}

        if self._bundle is Bundle.TOOLCHAIN:
            extra_ldflags = []
            if self._host_machine.is_apple:
                symfile = envdir / "toolchain-executable.symbols"
                symfile.write_text("# No exported symbols.\n", encoding="utf-8")
                extra_ldflags += [f"-Wl,-exported_symbols_list,{symfile}"]
            elif self._host_machine.os != "windows":
                verfile = envdir / "toolchain-executable.version"
                verfile.write_text("\n".join([
                                                 "{",
                                                 "  global:",
                                                 "    # FreeBSD needs these two:",
                                                 "    __progname;",
                                                 "    environ;",
                                                 "",
                                                 "  local:",
                                                 "    *;",
                                                 "};",
                                                 ""
                                             ]),
                                   encoding="utf-8")
                extra_ldflags += [f"-Wl,--version-script,{verfile}"]
            if extra_ldflags:
                menv["LDFLAGS"] = shlex.join(extra_ldflags + shlex.split(menv.get("LDFLAGS", "")))

        build_sdk_prefix = None
        host_sdk_prefix = None

        self._build_config, self._host_config = \
                env.generate_machine_configs(self._build_machine,
                                             self._host_machine,
                                             menv,
                                             self._toolchain_prefix,
                                             build_sdk_prefix,
                                             host_sdk_prefix,
                                             self._call_meson,
                                             self._default_library,
                                             envdir)
        self._build_env = self._build_config.make_merged_environment(os.environ)
        self._host_env = self._host_config.make_merged_environment(os.environ)

    def _clone_repo_if_needed(self, pkg: PackageSpec):
        sourcedir = self._get_sourcedir(pkg)

        git = lambda *args, **kwargs: subprocess.run(["git", *args],
                                                     **kwargs,
                                                     capture_output=True,
                                                     encoding="utf-8")

        if sourcedir.exists():
            self._print_status(pkg.name, "Reusing existing checkout")
            current_rev = git("rev-parse", "FETCH_HEAD", cwd=sourcedir, check=True).stdout.strip()
            if current_rev != pkg.version:
                self._print_status(pkg.name, "WARNING: Checkout does not match version in deps.toml")
        else:
            self._print_status(pkg.name, "Cloning")
            clone_shallow(pkg, sourcedir, git)

    def _wipe_build_state(self):
        for path in (self._get_outdir(), self._get_builddir_container()):
            if path.exists():
                self._print_status(path.relative_to(self._workdir).as_posix(), "Wiping")
                shutil.rmtree(path)

    def _build_package(self, pkg: PackageSpec, machines: Sequence[MachineSpec]):
        for machine in machines:
            manifest_path = self._get_manifest_path(pkg, machine)
            action = "skip" if manifest_path.exists() else "build"

            message = "Building" if action == "build" else "Already built"
            message += f" for {machine.identifier}"
            self._print_status(pkg.name, message)

            if action == "build":
                self._build_package_for_machine(pkg, machine)
                assert manifest_path.exists()

    def _build_package_for_machine(self, pkg: PackageSpec, machine: MachineSpec):
        sourcedir = self._get_sourcedir(pkg)
        builddir = self._get_builddir(pkg, machine)

        prefix = self._get_prefix(machine)
        libdir = prefix / "lib"

        strip = "true" if machine.toolchain_can_strip else "false"

        if builddir.exists():
            shutil.rmtree(builddir)

        machine_file_opts = [f"--native-file={self._build_config.machine_file}"]
        pc_opts = [f"-Dpkg_config_path={prefix / machine.libdatadir / 'pkgconfig'}"]
        if self._host_config is not self._build_config and machine is self._host_machine:
            machine_file_opts += [f"--cross-file={self._host_config.machine_file}"]
            pc_path_for_build = self._get_prefix(self._build_machine) / self._build_machine.libdatadir / "pkgconfig"
            pc_opts += [f"-Dbuild.pkg_config_path={pc_path_for_build}"]

        menv = self._host_env if machine is self._host_machine else self._build_env

        meson_kwargs = {
            "env": menv,
            "check": True,
        }
        if not self._verbose:
            meson_kwargs["capture_output"] = True
            meson_kwargs["encoding"] = "utf-8"

        self._call_meson([
                             "setup",
                             builddir,
                             *machine_file_opts,
                             f"-Dprefix={prefix}",
                             f"-Dlibdir={libdir}",
                             *pc_opts,
                             f"-Ddefault_library={self._default_library}",
                             f"-Dbackend=ninja",
                             *machine.meson_optimization_options,
                             f"-Dstrip={strip}",
                             *[opt.value for opt in pkg.options],
                         ],
                         cwd=sourcedir,
                         **meson_kwargs)

        self._call_meson(["install"],
                         cwd=builddir,
                         **meson_kwargs)

        manifest_lines = []
        install_locations = json.loads(self._call_meson(["introspect", "--installed"],
                                                        cwd=builddir,
                                                        capture_output=True,
                                                        encoding="utf-8",
                                                        env=menv).stdout)
        for installed_path in install_locations.values():
            manifest_lines.append(Path(installed_path).relative_to(prefix).as_posix())
        manifest_lines.sort()
        manifest_path = self._get_manifest_path(pkg, machine)
        manifest_path.parent.mkdir(parents=True, exist_ok=True)
        manifest_path.write_text("\n".join(manifest_lines) + "\n", encoding="utf-8")

    def _call_meson(self, argv, *args, **kwargs):
        if self._verbose and argv[0] in {"setup", "install"}:
            vanilla_env = os.environ
            meson_env = kwargs["env"]
            changed_env = {k: v for k, v in meson_env.items() if k not in vanilla_env or v != vanilla_env[k]}

            indent = "  "
            env_summary = f" \\\n{indent}".join([f"{k}={shlex.quote(v)}" for k, v in changed_env.items()])
            argv_summary = f" \\\n{3 * indent}".join([str(arg) for arg in argv])

            print(f"> {env_summary} \\\n{indent}meson {argv_summary}", flush=True)

        return env.call_meson(argv, use_submodule=True, *args, **kwargs)

    def _package(self):
        outfile = self._cachedir / f"{self._bundle.name.lower()}-{self._host_machine.identifier}.tar.xz"

        self._print_packaging_banner()
        with tempfile.TemporaryDirectory(prefix="frida-deps") as raw_tempdir:
            tempdir = Path(raw_tempdir)

            self._print_status(outfile.name, "Staging files")
            if self._bundle is Bundle.TOOLCHAIN:
                self._stage_toolchain_files(tempdir)
            else:
                self._stage_sdk_files(tempdir)

            self._adjust_manifests(tempdir)
            self._adjust_files_containing_hardcoded_paths(tempdir)

            (tempdir / "VERSION.txt").write_text(self._params.deps_version + "\n", encoding="utf-8")

            self._print_status(outfile.name, "Assembling")
            with tarfile.open(outfile, "w:xz") as tar:
                tar.add(tempdir, ".")

            self._print_status(outfile.name, "All done")

        return outfile

    def _stage_toolchain_files(self, location: Path) -> list[Path]:
        if self._host_machine.os == "windows":
            toolchain_prefix = self._toolchain_prefix
            mixin_files = [f for f in self._walk_plain_files(toolchain_prefix)
                           if self._file_should_be_mixed_into_toolchain(f)]
            copy_files(toolchain_prefix, mixin_files, location)

        prefix = self._get_prefix(self._host_machine)
        files = [f for f in self._walk_plain_files(prefix)
                 if self._file_is_toolchain_related(f)]
        copy_files(prefix, files, location)

    def _stage_sdk_files(self, location: Path) -> list[Path]:
        prefix = self._get_prefix(self._host_machine)
        files = [f for f in self._walk_plain_files(prefix)
                 if self._file_is_sdk_related(f)]
        copy_files(prefix, files, location)

    def _adjust_files_containing_hardcoded_paths(self, bundledir: Path):
        prefix = self._get_prefix(self._host_machine)

        raw_prefixes = [str(prefix)]
        if self._host_machine.os == "windows":
            raw_prefixes.append(prefix.as_posix())

        for f in self._walk_plain_files(bundledir):
            filepath = bundledir / f
            try:
                text = filepath.read_text(encoding="utf-8")

                new_text = text
                is_pcfile = filepath.suffix == ".pc"
                replacement = "${frida_sdk_prefix}" if is_pcfile else "@FRIDA_TOOLROOT@"
                for p in raw_prefixes:
                    new_text = new_text.replace(p, replacement)

                if new_text != text:
                    filepath.write_text(new_text, encoding="utf-8")
                    if not is_pcfile:
                        filepath.rename(filepath.parent / f"{f.name}.frida.in")
            except UnicodeDecodeError:
                pass

    @staticmethod
    def _walk_plain_files(rootdir: Path) -> Iterator[Path]:
        for dirpath, dirnames, filenames in os.walk(rootdir):
            for filename in filenames:
                f = Path(dirpath) / filename
                if f.is_symlink():
                    continue
                yield f.relative_to(rootdir)

    @staticmethod
    def _adjust_manifests(bundledir: Path):
        for manifest_path in (bundledir / "manifest").glob("*.pkg"):
            lines = []

            prefix = manifest_path.parent.parent
            for entry in manifest_path.read_text(encoding="utf-8").strip().split("\n"):
                if prefix.joinpath(entry).exists():
                    lines.append(entry)

            if lines:
                lines.sort()
                manifest_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
            else:
                manifest_path.unlink()

    def _file_should_be_mixed_into_toolchain(self, f: Path) -> bool:
        parts = f.parts
        if parts[0] == "VERSION.txt":
            return False
        if parts[0] == "bin":
            stem = f.stem
            return stem in {"bison", "flex", "m4", "nasm", "vswhere"} or stem.startswith("msys-")
        if parts[0] == "manifest":
            return False

        if self._file_is_vala_toolchain_related(f):
            return False

        return True

    def _file_is_toolchain_related(self, f: Path) -> bool:
        if self._file_is_vala_toolchain_related(f):
            return True

        parts = f.parts
        if parts[0] == "bin":
            if f.suffix == ".pdb":
                return False
            stem = f.stem
            if stem in {"gdbus", "gio", "gobject-query", "gsettings"}:
                return False
            if stem.startswith("gspawn-"):
                return False
            return True
        if parts[0] == "manifest":
            return True

        return False

    def _file_is_vala_toolchain_related(self, f: Path) -> bool:
        if f.suffix in {".vapi", ".deps"}:
            return True

        name = f.name
        if f.suffix == self._host_machine.executable_suffix:
            return name.startswith("vala") or name.startswith("vapi") or name.startswith("gen-introspect")
        if f.parts[0] == "bin" and name.startswith("vala-gen-introspect"):
            return True

        return False

    def _file_is_sdk_related(self, f: Path) -> bool:
        suffix = f.suffix
        if suffix == ".pdb":
            return False
        if suffix in [".vapi", ".deps"]:
            return True

        parts = f.parts
        if parts[0] == "bin":
            return f.name.startswith("v8-mksnapshot-")

        return "share" not in parts

    def _get_outdir(self) -> Path:
        return self._workdir / f"_{self._bundle.name.lower()}.out"

    def _get_sourcedir(self, pkg: PackageSpec) -> Path:
        return self._workdir / pkg.identifier

    def _get_builddir(self, pkg: PackageSpec, machine: MachineSpec) -> Path:
        return self._get_builddir_container() / machine.identifier / pkg.identifier

    def _get_builddir_container(self) -> Path:
        return self._workdir / f"_{self._bundle.name.lower()}.tmp"

    def _get_prefix(self, machine: MachineSpec) -> Path:
        return self._get_outdir() / machine.identifier

    def _get_manifest_path(self, pkg: PackageSpec, machine: MachineSpec) -> Path:
        return self._get_prefix(machine) / "manifest" / f"{pkg.identifier}.pkg"

    def _print_package_banner(self, pkg: PackageSpec):
        if self._ansi_supported:
            print("\n".join([
                "",
                "â•­â”€â”€â”€â”€",
                f"â”‚ ğŸ“¦ \033[1m{pkg.name}\033[0m",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®",
                f"â”‚ URL: {pkg.url}",
                f"â”‚ CID: {pkg.version}",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯",
            ]), flush=True)
        else:
            print("\n".join([
                "",
                f"# {pkg.name}",
                f"- URL: {pkg.url}",
                f"- CID: {pkg.version}",
            ]), flush=True)

    def _print_packaging_banner(self):
        if self._ansi_supported:
            print("\n".join([
                "",
                "â•­â”€â”€â”€â”€",
                f"â”‚ ğŸ—ï¸  \033[1mPackaging\033[0m",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®",
            ]), flush=True)
        else:
            print("\n".join([
                "",
                f"# Packaging",
            ]), flush=True)

    def _print_summary_banner(self):
        if self._ansi_supported:
            print("\n".join([
                "",
                "â•­â”€â”€â”€â”€",
                f"â”‚ ğŸ‰ \033[1mDone\033[0m",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®",
            ]), flush=True)
        else:
            print("\n".join([
                "",
                f"# Done",
```